#!/usr/bin/env python3
"""
V3.0 å‘é‡æ•°æ®åº“æ„å»ºè„šæœ¬ï¼ˆå®Œæ•´ç‰ˆï¼‰
åŸºäº VECTOR_DB_BUILD_SPEC_V3.md è§„èŒƒ

æ ¸å¿ƒç‰¹æ€§ï¼š
1. 18ä¸ªå®Œæ•´å…ƒæ•°æ®å­—æ®µ
2. é¡µå†…æ®µè½å®šä½ï¼ˆchunk_index_in_page, total_chunks_in_pageï¼‰
3. æ–‡çŒ®å…ƒæ•°æ®æå–ï¼ˆtitle, authors, year, journalï¼‰
4. ä¸Šä¸‹æ–‡é“¾æ¥ï¼ˆprev_chunk_id, next_chunk_idï¼‰
5. ä¿®å¤DOIæ˜ å°„åè½¬é—®é¢˜

æ„å»ºæ—¶é—´ï¼šçº¦2-3å°æ—¶ï¼ˆå–å†³äºPDFæ•°é‡å’ŒBGEæœåŠ¡æ€§èƒ½ï¼‰
"""
import os
import re
import json
import time
import hashlib
import requests
import fitz  # PyMuPDF
import chromadb
from tqdm import tqdm
from uuid import uuid4
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# é…ç½®åŒº
# ============================================================================
PROJECT_ROOT = os.path.dirname(__file__)
PDF_DIR = os.path.join(PROJECT_ROOT, "papers")
DOI_MAPPING_FILE = os.path.join(PROJECT_ROOT, "doi_to_pdf_mapping.json")
CHROMA_DB_PATH = os.path.join(PROJECT_ROOT, "vector_database_v3")
COLLECTION_NAME = "lfp_papers_v3"
BGE_API_URL = "http://localhost:8001/v1/embeddings"

# åˆ‡ç‰‡å‚æ•°
CHUNK_SIZE = 600
CHUNK_OVERLAP = 100
BATCH_SIZE = 128

# æ„å»ºç‰ˆæœ¬
BUILD_VERSION = "3.0"


# ============================================================================
# æ–‡æœ¬åˆ‡åˆ†å™¨
# ============================================================================
class SimpleTextSplitter:
    """é€’å½’æ–‡æœ¬åˆ‡åˆ†å™¨"""
    def __init__(self, chunk_size=600, chunk_overlap=100, separators=None):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", ". ", " ", ""]
    
    def split_text(self, text: str) -> List[str]:
        """é€’å½’åˆ‡åˆ†æ–‡æœ¬"""
        chunks = []
        text = text.strip()
        
        if len(text) <= self.chunk_size:
            return [text] if text else []
        
        for sep in self.separators:
            if sep == "":
                # æœ€åæ‰‹æ®µï¼šæŒ‰å­—ç¬¦æ•°ç¡¬åˆ‡åˆ†
                for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
                    chunk = text[i:i + self.chunk_size]
                    if chunk:
                        chunks.append(chunk)
                break
            
            parts = text.split(sep)
            current_chunk = ""
            
            for part in parts:
                test_chunk = current_chunk + sep + part if current_chunk else part
                if len(test_chunk) > self.chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = part
                else:
                    current_chunk = test_chunk
            
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            if len(chunks) > 1:
                break
        
        # åˆå¹¶ç›¸é‚»å°å—
        merged = []
        for chunk in chunks:
            if not chunk:
                continue
            if merged and len(merged[-1]) + len(chunk) < self.chunk_size:
                merged[-1] += " " + chunk
            else:
                merged.append(chunk)
        
        return merged if merged else [text[:self.chunk_size]]


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================
def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡æœ¬"""
    # ä¿®å¤è·¨è¡Œæ–­è¯
    text = text.replace("-\n", "").replace("\n", " ")
    # å‹ç¼©å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def generate_document_id(filename: str, file_size: int) -> str:
    """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ID"""
    content = f"{filename}_{file_size}"
    return hashlib.sha256(content.encode()).hexdigest()


def get_embeddings(texts: List[str]) -> List[List[float]]:
    """è°ƒç”¨ BGE æœåŠ¡è·å–å‘é‡"""
    if not texts:
        return []
    
    try:
        response = requests.post(
            BGE_API_URL,
            json={"input": texts},
            timeout=120
        )
        response.raise_for_status()
        data = response.json()["data"]
        return [item["embedding"] for item in data]
    except Exception as e:
        logger.error(f"Embedding API é”™è¯¯: {e}")
        # é”™è¯¯æ—¶è¿”å›é›¶å‘é‡ï¼Œé¿å…ç¨‹åºå´©æºƒ
        return [[0.0] * 1024 for _ in texts]


def load_doi_mapping() -> Dict[str, str]:
    """
    åŠ è½½ DOI æ˜ å°„å¹¶åè½¬ä¸º filename -> DOI
    
    åŸå§‹æ ¼å¼: {"DOI": "filename.pdf"}
    è¿”å›æ ¼å¼: {"filename.pdf": "DOI"}
    """
    if os.path.exists(DOI_MAPPING_FILE):
        with open(DOI_MAPPING_FILE, 'r', encoding='utf-8') as f:
            doi_to_file = json.load(f)
        
        # åè½¬æ˜ å°„ï¼šDOI -> filename å˜ä¸º filename -> DOI
        file_to_doi = {filename: doi for doi, filename in doi_to_file.items()}
        
        logger.info(f"ğŸ“‹ åŠ è½½äº† {len(file_to_doi)} ä¸ª DOI æ˜ å°„ï¼ˆå·²åè½¬ï¼‰")
        return file_to_doi
    
    logger.warning("âš ï¸  DOI æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨")
    return {}


# ============================================================================
# æ–‡çŒ®å…ƒæ•°æ®æå–
# ============================================================================
def extract_title_from_page(page) -> str:
    """ä»é¡µé¢æå–æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€é¡µæœ€å¤§å­—å·çš„æ–‡æœ¬ï¼‰"""
    try:
        blocks = page.get_text("dict")["blocks"]
        title_candidates = []
        
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    for span in line["spans"]:
                        text = span["text"].strip()
                        size = span["size"]
                        if text and len(text) > 10 and size > 12:
                            title_candidates.append((text, size))
        
        # æŒ‰å­—å·æ’åºï¼Œå–æœ€å¤§çš„
        if title_candidates:
            title_candidates.sort(key=lambda x: x[1], reverse=True)
            return title_candidates[0][0]
    except:
        pass
    
    return ""


def extract_authors_from_text(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–ä½œè€…ï¼ˆç®€å•è§„åˆ™ï¼‰"""
    # æŸ¥æ‰¾å¸¸è§ä½œè€…æ¨¡å¼
    patterns = [
        r'([A-Z][a-z]+\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)',  # John Smith
        r'([A-Z]\.\s*[A-Z]\.\s*[A-Z][a-z]+)',  # J. K. Smith
    ]
    
    authors = []
    for pattern in patterns:
        matches = re.findall(pattern, text[:1000])  # åªæœç´¢å‰1000å­—ç¬¦
        authors.extend(matches[:5])  # æœ€å¤š5ä¸ªä½œè€…
        if authors:
            break
    
    return authors[:5] if authors else []


def extract_year_from_text(text: str) -> int:
    """ä»æ–‡æœ¬ä¸­æå–å¹´ä»½"""
    # æŸ¥æ‰¾4ä½æ•°å¹´ä»½ï¼ˆ1900-2099ï¼‰
    matches = re.findall(r'\b(19\d{2}|20\d{2})\b', text[:2000])
    if matches:
        # è¿”å›æœ€å¸¸è§çš„å¹´ä»½
        from collections import Counter
        year_counts = Counter(matches)
        return int(year_counts.most_common(1)[0][0])
    return 0


def extract_journal_from_text(text: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå–æœŸåˆŠåï¼ˆç®€å•è§„åˆ™ï¼‰"""
    # æŸ¥æ‰¾å¸¸è§æœŸåˆŠå…³é”®è¯
    journal_keywords = [
        r'Journal of ([A-Z][a-z\s]+)',
        r'([A-Z][a-z\s]+) Journal',
        r'Proceedings of ([A-Z][a-z\s]+)',
    ]
    
    for pattern in journal_keywords:
        match = re.search(pattern, text[:2000])
        if match:
            return match.group(0)
    
    return ""


def extract_paper_metadata(pdf_path: str, doi: str) -> Dict[str, any]:
    """
    æå–æ–‡çŒ®å…ƒæ•°æ®
    
    Returns:
        {
            "title": str,
            "authors": List[str],
            "year": int,
            "journal": str
        }
    """
    try:
        doc = fitz.open(pdf_path)
        
        # 1. å°è¯•ä»PDFå…ƒæ•°æ®æå–
        metadata = doc.metadata
        title = metadata.get("title", "").strip()
        
        # 2. ä»ç¬¬ä¸€é¡µæå–
        first_page = doc[0]
        first_page_text = first_page.get_text()
        
        # æ ‡é¢˜
        if not title or len(title) < 10:
            title = extract_title_from_page(first_page)
        
        # ä½œè€…
        authors = extract_authors_from_text(first_page_text)
        
        # å¹´ä»½
        year = extract_year_from_text(first_page_text)
        
        # æœŸåˆŠ
        journal = extract_journal_from_text(first_page_text)
        
        doc.close()
        
        return {
            "title": title if title else "æœªçŸ¥æ ‡é¢˜",
            "authors": authors if authors else ["æœªçŸ¥ä½œè€…"],
            "year": year if year else 0,
            "journal": journal if journal else "æœªçŸ¥æœŸåˆŠ"
        }
        
    except Exception as e:
        logger.warning(f"æå–å…ƒæ•°æ®å¤±è´¥: {e}")
        return {
            "title": "æœªçŸ¥æ ‡é¢˜",
            "authors": ["æœªçŸ¥ä½œè€…"],
            "year": 0,
            "journal": "æœªçŸ¥æœŸåˆŠ"
        }


# ============================================================================
# PDFå¤„ç†
# ============================================================================
def process_single_pdf(
    filepath: str,
    filename: str,
    doi: str,
    text_splitter: SimpleTextSplitter
) -> List[Dict]:
    """
    å¤„ç†å•ä¸ª PDF æ–‡ä»¶ï¼ˆV3.0å®Œæ•´ç‰ˆï¼Œå¸¦OOMä¿æŠ¤ï¼‰
    
    Returns:
        åˆ‡ç‰‡åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ‡ç‰‡åŒ…å«å®Œæ•´çš„V3.0å…ƒæ•°æ®
    """
    chunks = []
    doc = None
    
    try:
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filepath)
        
        # ç”Ÿæˆdocument_id
        document_id = generate_document_id(filename, file_size)
        
        # æå–æ–‡çŒ®å…ƒæ•°æ®
        paper_meta = extract_paper_metadata(filepath, doi)
        
        # æ‰“å¼€PDF
        doc = fitz.open(filepath)
        
        # OOMä¿æŠ¤ï¼šé™åˆ¶æœ€å¤§é¡µæ•°ï¼ˆé¿å…è¶…å¤§PDFï¼‰
        max_pages = min(len(doc), 500)  # æœ€å¤šå¤„ç†500é¡µ
        if len(doc) > max_pages:
            logger.warning(f"   âš ï¸  {filename} æœ‰ {len(doc)} é¡µï¼Œåªå¤„ç†å‰ {max_pages} é¡µ")
        
        # å…¨å±€è®¡æ•°å™¨
        global_counter = 0
        all_chunks_data = []  # å­˜å‚¨æ‰€æœ‰æ®µè½æ•°æ®ï¼Œç”¨äºåç»­å»ºç«‹é“¾æ¥
        
        # é€é¡µå¤„ç†
        for page_index in range(max_pages):
            page = doc[page_index]
            page_num = page_index + 1
            
            # æå–æ–‡æœ¬
            raw_text = page.get_text("text", sort=True)
            clean_text_str = clean_text(raw_text)
            
            # è·³è¿‡ç©ºç™½é¡µ
            if len(clean_text_str) < 50:
                continue
            
            # åˆ‡åˆ†æ®µè½
            page_chunks = text_splitter.split_text(clean_text_str)
            
            # ç»Ÿè®¡è¯¥é¡µæ®µè½æ•° â­ æ ¸å¿ƒåˆ›æ–°
            total_chunks_in_page = len(page_chunks)
            
            # å¤„ç†æ¯ä¸ªæ®µè½
            for chunk_index_in_page, chunk_text in enumerate(page_chunks):
                if len(chunk_text) < 30:  # è·³è¿‡å¤ªçŸ­çš„ç¢ç‰‡
                    continue
                
                # ç”Ÿæˆchunk_id
                chunk_id = str(uuid4())
                
                # æ„å»ºå®Œæ•´çš„V3.0å…ƒæ•°æ®
                chunk_data = {
                    "chunk_id": chunk_id,
                    "text": chunk_text,
                    "metadata": {
                        # === æ ¸å¿ƒæ ‡è¯† ===
                        "document_id": document_id,
                        "doi": doi,
                        "filename": filename,
                        
                        # === å±‚çº§å®šä½ â­ ===
                        "chunk_id": chunk_id,
                        "chunk_index_global": global_counter,
                        "page": page_num,
                        "chunk_index_in_page": chunk_index_in_page,  # â­ å…³é”®å­—æ®µ
                        "total_chunks_in_page": total_chunks_in_page,  # â­ å…³é”®å­—æ®µ
                        
                        # === å†…å®¹ä¿¡æ¯ ===
                        "source_text": chunk_text,
                        "text_hash": hashlib.md5(chunk_text.encode()).hexdigest(),
                        "char_count": len(chunk_text),
                        
                        # === ä¸Šä¸‹æ–‡é“¾æ¥ï¼ˆç¨åå¡«å……ï¼‰===
                        "prev_chunk_id": "",
                        "next_chunk_id": "",
                        
                        # === æ–‡çŒ®ä¿¡æ¯ â­ ===
                        "title": paper_meta["title"] or "Unknown",
                        "authors": ", ".join(paper_meta["authors"]) if isinstance(paper_meta["authors"], list) else (paper_meta["authors"] or "Unknown"),
                        "year": paper_meta["year"] or 0,
                        "journal": paper_meta["journal"] or "Unknown",
                        
                        # === æ„å»ºä¿¡æ¯ ===
                        "build_version": BUILD_VERSION,
                        "build_timestamp": datetime.now().isoformat(),
                    }
                }
                
                all_chunks_data.append(chunk_data)
                global_counter += 1
            
            # OOMä¿æŠ¤ï¼šæ¯å¤„ç†50é¡µæ¸…ç†ä¸€æ¬¡å†…å­˜
            if page_index % 50 == 0 and page_index > 0:
                import gc
                gc.collect()
        
        # å…³é—­æ–‡æ¡£
        if doc:
            doc.close()
            doc = None
        
        # å»ºç«‹ä¸Šä¸‹æ–‡é“¾æ¥ â­
        for i, chunk_data in enumerate(all_chunks_data):
            if i > 0:
                chunk_data["metadata"]["prev_chunk_id"] = all_chunks_data[i-1]["chunk_id"]
            if i < len(all_chunks_data) - 1:
                chunk_data["metadata"]["next_chunk_id"] = all_chunks_data[i+1]["chunk_id"]
        
        # è¿”å›ç»“æœï¼ˆOOMä¿æŠ¤ï¼šä¸ä¿ç•™ä¸­é—´å˜é‡ï¼‰
        result = all_chunks_data
        all_chunks_data = None  # é‡Šæ”¾å¼•ç”¨
        return result
        
    except Exception as e:
        logger.error(f"å¤„ç† PDF å¤±è´¥ {filename}: {e}")
        # OOMä¿æŠ¤ï¼šå¼‚å¸¸æ—¶ç¡®ä¿é‡Šæ”¾èµ„æº
        if doc:
            try:
                doc.close()
            except:
                pass
        import gc
        gc.collect()
        return []


# ============================================================================
# ä¸»æµç¨‹
# ============================================================================
def main():
    """ä¸»æµç¨‹"""
    logger.info("=" * 80)
    logger.info("ğŸš€ V3.0 å‘é‡æ•°æ®åº“æ„å»ºç¨‹åºå¯åŠ¨")
    logger.info("   åŸºäº VECTOR_DB_BUILD_SPEC_V3.md è§„èŒƒ")
    logger.info("=" * 80)
    
    # ========== 1. æ„å»ºå‰éªŒè¯ ==========
    logger.info("\nğŸ“‹ [æ­¥éª¤1] æ„å»ºå‰éªŒè¯")
    
    # æ£€æŸ¥ PDF ç›®å½•
    if not os.path.exists(PDF_DIR):
        logger.error(f"âŒ PDF ç›®å½•ä¸å­˜åœ¨: {PDF_DIR}")
        return
    
    pdf_files = [f for f in os.listdir(PDF_DIR) if f.endswith('.pdf')]
    logger.info(f"âœ… PDFæ–‡ä»¶: {len(pdf_files)} ä¸ª")
    
    if not pdf_files:
        logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        return
    
    # åŠ è½½ DOI æ˜ å°„
    file_to_doi = load_doi_mapping()
    valid_dois = sum(1 for doi in file_to_doi.values() if doi.startswith("10."))
    logger.info(f"âœ… DOIæ˜ å°„: {len(file_to_doi)} ä¸ªï¼ˆæœ‰æ•ˆ: {valid_dois}ï¼‰")
    
    # æ£€æŸ¥ BGE æœåŠ¡
    try:
        response = requests.get(BGE_API_URL.replace("/v1/embeddings", "/health"), timeout=5)
        logger.info(f"âœ… BGEæœåŠ¡æ­£å¸¸")
    except:
        logger.warning(f"âš ï¸  BGEæœåŠ¡å¯èƒ½æœªå¯åŠ¨: {BGE_API_URL}")
    
    # ========== 2. åˆå§‹åŒ– ==========
    logger.info("\nğŸ”§ [æ­¥éª¤2] åˆå§‹åŒ–")
    
    # åˆå§‹åŒ–åˆ‡åˆ†å™¨
    text_splitter = SimpleTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    logger.info(f"âœ… æ–‡æœ¬åˆ‡åˆ†å™¨: chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}")
    
    # åˆå§‹åŒ– ChromaDB
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # åˆ é™¤æ—§é›†åˆ
    try:
        client.delete_collection(COLLECTION_NAME)
        logger.info(f"ğŸ—‘ï¸  å·²åˆ é™¤æ—§é›†åˆ: {COLLECTION_NAME}")
    except:
        pass
    
    collection = client.create_collection(name=COLLECTION_NAME)
    logger.info(f"âœ… åˆ›å»ºæ–°é›†åˆ: {COLLECTION_NAME}")
    
    # ========== 3. æ‰¹é‡å¤„ç† ==========
    logger.info(f"\nğŸ“„ [æ­¥éª¤3] å¤„ç†PDFæ–‡ä»¶ï¼ˆå…± {len(pdf_files)} ä¸ªï¼‰")
    
    batch_documents = []
    batch_metadatas = []
    batch_ids = []
    
    total_chunks = 0
    total_pdfs = 0
    failed_pdfs = []
    
    stats = {
        "valid_doi": 0,
        "unknown_doi": 0,
        "with_title": 0,
        "with_authors": 0,
        "with_year": 0,
    }
    
    start_time = time.time()
    
    for idx, filename in enumerate(tqdm(pdf_files, desc="å¤„ç†PDF"), 1):
        filepath = os.path.join(PDF_DIR, filename)
        doi = file_to_doi.get(filename, "unknown_doi")
        
        # ç»Ÿè®¡DOI
        if doi.startswith("10."):
            stats["valid_doi"] += 1
        else:
            stats["unknown_doi"] += 1
        
        try:
            # å¤„ç†PDF
            pdf_chunks = process_single_pdf(filepath, filename, doi, text_splitter)
            
            if pdf_chunks:
                # ç»Ÿè®¡å…ƒæ•°æ®è´¨é‡
                first_meta = pdf_chunks[0]["metadata"]
                if first_meta["title"] != "æœªçŸ¥æ ‡é¢˜":
                    stats["with_title"] += 1
                if first_meta["authors"] != ["æœªçŸ¥ä½œè€…"]:
                    stats["with_authors"] += 1
                if first_meta["year"] > 0:
                    stats["with_year"] += 1
                
                # æ·»åŠ åˆ°æ‰¹æ¬¡
                for chunk_data in pdf_chunks:
                    batch_documents.append(chunk_data["text"])
                    batch_metadatas.append(chunk_data["metadata"])
                    batch_ids.append(chunk_data["chunk_id"])
                
                total_chunks += len(pdf_chunks)
                total_pdfs += 1
            else:
                failed_pdfs.append(filename)
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥ {filename}: {e}")
            failed_pdfs.append(filename)
            continue
        
        # æ‰¹æ¬¡å†™å…¥ï¼ˆOOMä¿æŠ¤ï¼šç«‹å³å†™å…¥å¹¶æ¸…ç©ºç¼“å†²åŒºï¼‰
        if len(batch_documents) >= BATCH_SIZE:
            logger.info(f"   ğŸ’¾ å†™å…¥æ‰¹æ¬¡: {len(batch_documents)} ä¸ªæ®µè½")
            
            # è·å–å‘é‡
            embeddings = get_embeddings(batch_documents)
            
            # å†™å…¥æ•°æ®åº“
            collection.add(
                embeddings=embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas,
                ids=batch_ids
            )
            
            # ç«‹å³æ¸…ç©ºç¼“å†²åŒºï¼ˆOOMä¿æŠ¤ï¼‰â­
            batch_documents.clear()
            batch_metadatas.clear()
            batch_ids.clear()
            embeddings = None  # é‡Šæ”¾embeddingå†…å­˜
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¯é€‰ï¼Œä½†æœ‰åŠ©äºé‡Šæ”¾å†…å­˜ï¼‰
            import gc
            gc.collect()
        
        # æ¯å¤„ç†100ä¸ªPDFæ˜¾ç¤ºè¿›åº¦
        if idx % 100 == 0:
            elapsed = time.time() - start_time
            avg_time = elapsed / idx
            remaining = (len(pdf_files) - idx) * avg_time
            logger.info(f"   è¿›åº¦: {idx}/{len(pdf_files)}, "
                       f"å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ, "
                       f"é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
    
    # å¤„ç†å‰©ä½™æ•°æ®
    if batch_documents:
        logger.info(f"   ğŸ’¾ å†™å…¥æœ€åæ‰¹æ¬¡: {len(batch_documents)} ä¸ªæ®µè½")
        embeddings = get_embeddings(batch_documents)
        collection.add(
            embeddings=embeddings,
            documents=batch_documents,
            metadatas=batch_metadatas,
            ids=batch_ids
        )
        
        # æ¸…ç©ºç¼“å†²åŒº
        batch_documents.clear()
        batch_metadatas.clear()
        batch_ids.clear()
        embeddings = None
        
        import gc
        gc.collect()
    
    # ========== 4. æ„å»ºåéªŒè¯ ==========
    logger.info("\nğŸ” [æ­¥éª¤4] æ„å»ºåéªŒè¯")
    
    final_count = collection.count()
    logger.info(f"âœ… æ•°æ®åº“æ€»æ®µè½æ•°: {final_count}")
    
    # æŠ½æ ·éªŒè¯
    sample = collection.get(limit=100)
    sample_metas = sample["metadatas"]
    
    # éªŒè¯DOI
    valid_doi_count = sum(1 for m in sample_metas if m["doi"].startswith("10."))
    logger.info(f"âœ… æœ‰æ•ˆDOIæ¯”ä¾‹: {valid_doi_count}/100")
    
    # éªŒè¯é¡µå†…åºå·
    try:
        for meta in sample_metas[:10]:
            assert 0 <= meta["chunk_index_in_page"] < meta["total_chunks_in_page"]
        logger.info(f"âœ… é¡µå†…åºå·éªŒè¯é€šè¿‡")
    except AssertionError:
        logger.warning(f"âš ï¸  é¡µå†…åºå·éªŒè¯å¤±è´¥")
    
    # éªŒè¯ä¸Šä¸‹æ–‡é“¾æ¥
    linked_count = sum(1 for m in sample_metas if m.get("prev_chunk_id") or m.get("next_chunk_id"))
    logger.info(f"âœ… ä¸Šä¸‹æ–‡é“¾æ¥: {linked_count}/100 ä¸ªæ®µè½æœ‰é“¾æ¥")
    
    # ========== 5. ç»Ÿè®¡æŠ¥å‘Š ==========
    total_time = time.time() - start_time
    
    logger.info("\n" + "=" * 80)
    logger.info("âœ… V3.0 å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ!")
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    logger.info(f"   - å¤„ç†PDF: {total_pdfs}/{len(pdf_files)}")
    logger.info(f"   - å¤±è´¥PDF: {len(failed_pdfs)}")
    logger.info(f"   - ç”Ÿæˆæ®µè½: {total_chunks}")
    logger.info(f"   - æ•°æ®åº“æ€»é‡: {final_count}")
    logger.info(f"")
    logger.info(f"ğŸ“‹ DOIç»Ÿè®¡:")
    logger.info(f"   - æœ‰æ•ˆDOI: {stats['valid_doi']}")
    logger.info(f"   - æœªçŸ¥DOI: {stats['unknown_doi']}")
    logger.info(f"")
    logger.info(f"ğŸ“š å…ƒæ•°æ®è´¨é‡:")
    logger.info(f"   - æœ‰æ ‡é¢˜: {stats['with_title']}/{total_pdfs}")
    logger.info(f"   - æœ‰ä½œè€…: {stats['with_authors']}/{total_pdfs}")
    logger.info(f"   - æœ‰å¹´ä»½: {stats['with_year']}/{total_pdfs}")
    logger.info(f"")
    logger.info(f"â±ï¸  è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    logger.info(f"ğŸ’¾ å­˜å‚¨è·¯å¾„: {CHROMA_DB_PATH}")
    logger.info(f"ğŸ“¦ é›†åˆåç§°: {COLLECTION_NAME}")
    logger.info("=" * 80)
    
    if failed_pdfs:
        logger.info(f"\nâš ï¸  å¤±è´¥æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰:")
        for f in failed_pdfs[:10]:
            logger.info(f"   - {f}")


if __name__ == "__main__":
    main()
