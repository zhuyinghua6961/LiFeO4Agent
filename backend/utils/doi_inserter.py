"""
DOIæ’å…¥å·¥å…· - ç¨‹åºåŒ–å°†DOIæ’å…¥åˆ°ç­”æ¡ˆä¸­
é¿å…LLMç¼–é€ DOIï¼Œç¡®ä¿æ‰€æœ‰DOIæ¥è‡ªæ£€ç´¢ç»“æœ
"""
import re
import logging
from typing import Dict, List, Any, Optional
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # è®¾ç½®ä¸ºDEBUGçº§åˆ«ä»¥æŸ¥çœ‹è¯¦ç»†æ—¥å¿—


def validate_doi(doi: str) -> Optional[str]:
    """
    éªŒè¯å¹¶æ¸…ç†DOIæ ¼å¼
    
    Args:
        doi: åŸå§‹DOIå­—ç¬¦ä¸²
        
    Returns:
        æ¸…ç†åçš„DOIæˆ–Noneï¼ˆæ— æ•ˆï¼‰
    """
    if not doi:
        return None
    
    doi = doi.strip()
    
    # ç§»é™¤å¯èƒ½æ··å…¥çš„URLéƒ¨åˆ†
    url_patterns = [r'www\.', r'http://', r'https://', r'\.com', r'\.org', r'\.net']
    for pattern in url_patterns:
        match = re.search(pattern, doi, re.IGNORECASE)
        if match:
            doi = doi[:match.start()].strip()
            break
    
    # ç§»é™¤æœ«å°¾æ ‡ç‚¹
    doi = re.sub(r'[.,;:]+$', '', doi)
    
    # éªŒè¯åŸºæœ¬æ ¼å¼ï¼š10.xxxx/xxxxx
    if not re.match(r'^10\.\d+/[A-Za-z0-9._\-/]{2,}$', doi):
        return None
    
    return doi


class ProgrammaticDOIInserter:
    """ç¨‹åºåŒ–DOIæ’å…¥å™¨ - åŸºäºç›¸ä¼¼åº¦åŒ¹é…è‡ªåŠ¨æ’å…¥DOI"""
    
    def __init__(
        self,
        similarity_threshold: float = 0.22,  # é™ä½é˜ˆå€¼åˆ°0.22,åŸºäºå®é™…æµ‹è¯•ä¼˜åŒ–
        seq_weight: float = 0.4,  # é™ä½æ–‡æœ¬æƒé‡,LLMä¼šé‡ç»„è¡¨è¾¾
        vector_weight: float = 0.6,  # æé«˜å‘é‡æƒé‡,æ›´å¯é çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        max_compare_chars: int = 1000
    ):
        """
        åˆå§‹åŒ–DOIæ’å…¥å™¨
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼æ‰æ’å…¥DOIï¼ˆé»˜è®¤0.30ï¼‰
            seq_weight: æ–‡æœ¬åºåˆ—ç›¸ä¼¼åº¦æƒé‡
            vector_weight: å‘é‡ç›¸ä¼¼åº¦æƒé‡
            max_compare_chars: æœ€å¤§æ¯”è¾ƒå­—ç¬¦æ•°
        """
        self.similarity_threshold = similarity_threshold
        self.seq_weight = seq_weight
        self.vector_weight = vector_weight
        self.max_compare_chars = max_compare_chars
        
        logger.info(f"   DOIæ’å…¥å™¨åˆå§‹åŒ–: é˜ˆå€¼={similarity_threshold}, æ–‡æœ¬æƒé‡={seq_weight}, å‘é‡æƒé‡={vector_weight}")
    
    def insert_dois(
        self,
        answer: str,
        search_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å°†DOIç¨‹åºåŒ–æ’å…¥åˆ°ç­”æ¡ˆä¸­ï¼Œå¹¶è¿”å›ä½ç½®ä¿¡æ¯
        
        å·¥ä½œåŸç†ï¼š
        1. å°†ç­”æ¡ˆæ‹†åˆ†ä¸ºå¥å­
        2. å¯¹æ¯ä¸ªå¥å­ï¼Œè®¡ç®—ä¸æ£€ç´¢æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        3. å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ’å…¥å¯¹åº”æ–‡æ¡£çš„DOI
        4. è®°å½•æ¯ä¸ªDOIçš„ä½ç½®ä¿¡æ¯ï¼ˆé¡µç ã€æ®µè½å·ç­‰ï¼‰
        
        Args:
            answer: LLMç”Ÿæˆçš„çº¯å‡€ç­”æ¡ˆï¼ˆä¸å«DOIï¼‰
            search_results: æ£€ç´¢ç»“æœï¼ŒåŒ…å«documents, metadatas, distances
            
        Returns:
            {
                'answer': æ’å…¥DOIåçš„ç­”æ¡ˆ,
                'doi_locations': æ¯ä¸ªDOIçš„ä½ç½®ä¿¡æ¯
            }
        """
        if not answer or not search_results:
            return {
                'answer': answer,
                'doi_locations': {}
            }
        
        # æå–æ£€ç´¢ç»“æœä¸­çš„å€™é€‰æ–‡æ¡£
        candidate_docs = self._extract_candidate_docs(search_results)
        
        if not candidate_docs:
            logger.info("   âš ï¸ æ— å¯ç”¨çš„å¸¦DOIæ–‡æ¡£ï¼Œè·³è¿‡DOIæ’å…¥")
            return {
                'answer': answer,
                'doi_locations': {}
            }
        
        # å°†ç­”æ¡ˆæ‹†åˆ†ä¸ºå¥å­
        sentences = self._split_sentences(answer)
        
        # å¯¹æ¯ä¸ªå¥å­åŒ¹é…æœ€ä½³DOI
        output_sentences = []
        inserted_dois = set()
        matched_count = 0
        total_sentences = 0
        doi_locations = {}  # è®°å½•æ¯ä¸ªDOIçš„ä½ç½®ä¿¡æ¯
        
        for sent in sentences:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¢è¡Œç¬¦ã€ç©ºè¡Œã€æ ‡é¢˜è¡Œã€è¡¨æ ¼è¡Œ
            sent_strip = sent.strip()
            
            # ç©ºè¡Œã€æ¢è¡Œç¬¦ç›´æ¥ä¿ç•™
            if sent == '\n' or not sent_strip:
                output_sentences.append(sent)
                continue
            
            # æ ‡é¢˜è¡Œå’Œè¡¨æ ¼è¡Œç›´æ¥ä¿ç•™ï¼ˆä¸æ’å…¥DOIï¼‰
            if sent_strip.startswith('#') or '|' in sent_strip:
                output_sentences.append(sent)
                continue
            
            # å·²åŒ…å«DOIçš„å¥å­ç›´æ¥ä¿ç•™
            if self._has_doi(sent_strip):
                output_sentences.append(sent)
                continue
            
            # æ£€æµ‹å¹¶ç§»é™¤å¥é¦–çš„åˆ—è¡¨åºå· (å¦‚ "1. ", "2) ", "a. ", "* ")
            list_marker_match = re.match(r'^(\s*)([0-9]+[.)]\s*|[a-zA-Z][.)]\s*|[*\-+]\s+)', sent_strip)
            if list_marker_match:
                prefix = list_marker_match.group(0)
                sent_content = sent_strip[len(prefix):]
            else:
                prefix = ""
                sent_content = sent_strip
            
            total_sentences += 1
            
            # ä½¿ç”¨å»é™¤åºå·åçš„å†…å®¹è¿›è¡ŒåŒ¹é…
            best_doc, best_score = self._find_best_match(sent_content if sent_content else sent_strip, candidate_docs)
            
            # è°ƒè¯•æ—¥å¿—
            if total_sentences <= 5:  # åªè®°å½•å‰5ä¸ªå¥å­çš„è¯¦ç»†ä¿¡æ¯
                logger.debug(f"   å¥å­ {total_sentences}: {sent_content[:50] if sent_content else sent_strip[:50]}...")
                logger.debug(f"   æœ€ä½³åŒ¹é…DOI: {best_doc['doi'] if best_doc else 'None'}")
                logger.debug(f"   ç›¸ä¼¼åº¦åˆ†æ•°: {best_score:.3f} (é˜ˆå€¼: {self.similarity_threshold})")
            
            # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ’å…¥DOIï¼ˆåœ¨å†…å®¹åï¼Œåºå·ä¿æŒåŸä½ï¼‰
            if best_doc and best_score >= self.similarity_threshold:
                doi = best_doc['doi']
                inserted_dois.add(doi)
                matched_count += 1
                
                # è®°å½•ä½ç½®ä¿¡æ¯
                metadata = best_doc.get('metadata', {})
                if doi not in doi_locations:
                    doi_locations[doi] = []
                
                doi_locations[doi].append({
                    'sentence': sent_content if sent_content else sent_strip,
                    'page': metadata.get('page', 0),
                    'chunk_index_in_page': metadata.get('chunk_index_in_page', 0),
                    'total_chunks_in_page': metadata.get('total_chunks_in_page', 1),
                    'similarity': best_score,
                    'source_preview': best_doc['text'][:300],
                    'confidence': 'high' if best_score >= 0.4 else 'medium' if best_score >= 0.3 else 'low'
                })
                
                # DOIæ’å…¥åˆ°å†…å®¹æœ«å°¾ï¼Œä¿ç•™åºå·å‰ç¼€å’Œæ¢è¡Œç¬¦
                if prefix:
                    output_sent = prefix + sent_content.rstrip() + f" (doi={doi})\n"
                else:
                    output_sent = sent_strip.rstrip() + f" (doi={doi})\n"
                output_sentences.append(output_sent)
                logger.debug(f"   âœ… æ’å…¥DOI: {doi} (ç›¸ä¼¼åº¦: {best_score:.3f})")
            else:
                # ä¿ç•™åŸå§‹å¥å­ï¼ˆåŒ…å«æ¢è¡Œï¼‰
                output_sentences.append(sent)
        
        result = "".join(output_sentences)
        
        logger.info(f"   âœ… ç¨‹åºåŒ–DOIæ’å…¥å®Œæˆ: åˆ†æäº† {total_sentences} ä¸ªå¥å­ï¼ŒåŒ¹é… {matched_count} ä¸ªï¼Œæ’å…¥äº† {len(inserted_dois)} ä¸ªä¸åŒçš„DOI")
        if inserted_dois:
            logger.info(f"   æ’å…¥çš„DOI: {', '.join(sorted(inserted_dois))}")
        else:
            logger.warning(f"   âš ï¸ æ²¡æœ‰æ’å…¥ä»»ä½•DOI - å¯èƒ½åŸå› ï¼š")
            logger.warning(f"      1. ç­”æ¡ˆå¥å­ä¸æ£€ç´¢æ–‡æ¡£ç›¸ä¼¼åº¦éƒ½ä½äºé˜ˆå€¼ {self.similarity_threshold}")
            logger.warning(f"      2. é˜ˆå€¼è®¾ç½®è¿‡é«˜ï¼Œå»ºè®®é™ä½åˆ° 0.3-0.35")
            logger.warning(f"      3. ç­”æ¡ˆå†…å®¹ä¸æ£€ç´¢ç»“æœä¸åŒ¹é…")
        
        # è®°å½•ä½ç½®ä¿¡æ¯ç»Ÿè®¡
        if doi_locations:
            logger.info(f"   ğŸ“ ä½ç½®ä¿¡æ¯ç»Ÿè®¡:")
            for doi, locations in doi_locations.items():
                pages = set(loc['page'] for loc in locations)
                logger.info(f"      {doi}: {len(locations)}ä¸ªå¼•ç”¨ï¼Œåˆ†å¸ƒåœ¨ç¬¬{sorted(pages)}é¡µ")
        
        return {
            'answer': result,
            'doi_locations': doi_locations
        }
    
    def _extract_candidate_docs(self, search_results: Dict[str, Any]) -> List[Dict]:
        """ä»æ£€ç´¢ç»“æœä¸­æå–å€™é€‰æ–‡æ¡£ï¼ˆå¸¦DOIå’Œå…ƒæ•°æ®ï¼‰"""
        metadatas = search_results.get('metadatas', []) or []
        documents = search_results.get('documents', []) or []
        distances = search_results.get('distances', []) or []
        
        candidates = []
        
        for i, (meta, doc) in enumerate(zip(metadatas, documents)):
            if not meta or not doc:
                continue
            
            # æå–DOI
            doi_raw = meta.get('DOI') or meta.get('doi') or ''
            doi_raw = doi_raw.strip()
            
            if not doi_raw or not doi_raw.startswith('10.'):
                continue
            
            # éªŒè¯DOI
            doi_clean = validate_doi(doi_raw)
            if not doi_clean:
                continue
            
            # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ (1 - distance)
            try:
                dist = distances[i] if i < len(distances) else 1.0
                vector_sim = max(0.0, 1.0 - float(dist))
            except:
                vector_sim = 0.0
            
            candidates.append({
                'doi': doi_clean,
                'text': doc,
                'vector_sim': vector_sim,
                'metadata': meta  # ä¿ç•™å®Œæ•´çš„å…ƒæ•°æ®
            })
        
        logger.info(f"   æå–åˆ° {len(candidates)} ä¸ªå€™é€‰æ–‡æ¡£ï¼ˆå¸¦DOIï¼‰")
        return candidates
    
    def _split_sentences(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­ï¼Œä¿ç•™æ ‡ç‚¹ç¬¦å·å’ŒåŸå§‹æ ¼å¼"""
        # ç­–ç•¥ï¼šåªåœ¨ä¸­æ–‡æ ‡ç‚¹å¤„åˆ†å‰²ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦å’Œè¡¨æ ¼ç»“æ„
        # ä¸ç›´æ¥åˆ†å‰²ï¼Œè€Œæ˜¯é€è¡Œå¤„ç†ï¼Œä¿æŒåŸå§‹æ ¼å¼
        
        lines = text.split('\n')
        sentences = []
        
        for line in lines:
            line_stripped = line.strip()
            
            # ç©ºè¡Œä¿ç•™
            if not line_stripped:
                sentences.append('\n')
                continue
            
            # Markdownæ ‡é¢˜è¡Œä¿ç•™å®Œæ•´
            if line_stripped.startswith('#'):
                sentences.append(line + '\n')
                continue
            
            # è¡¨æ ¼è¡Œä¿ç•™å®Œæ•´
            if '|' in line_stripped:
                sentences.append(line + '\n')
                continue
            
            # æ™®é€šæ–‡æœ¬è¡ŒæŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å‰²
            # å…ˆæ›¿æ¢ä¸­æ–‡æ ‡ç‚¹ä¸ºç‰¹æ®Šæ ‡è®°
            processed_line = line_stripped
            processed_line = processed_line.replace('ã€‚', '<PERIOD>')
            processed_line = processed_line.replace('ï¼', '<EXCLAIM>')
            processed_line = processed_line.replace('ï¼Ÿ', '<QUESTION>')
            processed_line = processed_line.replace('ï¼›', '<SEMICOLON>')
            
            # åˆ†å‰²å¥å­
            parts = re.split(r'<PERIOD>|<EXCLAIM>|<QUESTION>|<SEMICOLON>', processed_line)
            parts = [p.strip() for p in parts if p.strip()]
            
            # æ·»åŠ å¥å­ï¼Œæ¯ä¸ªå¥å­ååŠ æ ‡ç‚¹æ¢å¤æ ‡è®°
            for i, part in enumerate(parts):
                # æ¢å¤åŸå§‹æ ‡ç‚¹
                original_sent = part
                if i == len(parts) - 1:
                    # æœ€åä¸€ä¸ªå¥å­ååŠ æ¢è¡Œ
                    sentences.append(original_sent + '\n')
                else:
                    sentences.append(original_sent)
        
        return sentences
    
    def _has_doi(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦å·²åŒ…å«DOI"""
        return bool(re.search(r'\(doi\s*=\s*10\.\d+/', text, re.IGNORECASE))
    
    def _find_best_match(
        self,
        sentence: str,
        candidates: List[Dict]
    ) -> tuple:
        """
        ä¸ºå¥å­æŸ¥æ‰¾æœ€ä½³åŒ¹é…çš„æ–‡æ¡£
        
        Returns:
            (best_doc, best_score) å…ƒç»„
        """
        best_doc = None
        best_score = 0.0
        all_scores = []
        
        for doc in candidates:
            # æˆªå–æ–‡æ¡£å†…å®¹è¿›è¡Œæ¯”è¾ƒ
            doc_text = doc['text'][:self.max_compare_chars]
            
            # è®¡ç®—æ–‡æœ¬åºåˆ—ç›¸ä¼¼åº¦
            try:
                seq_sim = SequenceMatcher(None, sentence, doc_text).ratio()
            except:
                seq_sim = 0.0
            
            # ç»„åˆç›¸ä¼¼åº¦ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦ + å‘é‡ç›¸ä¼¼åº¦
            combined_score = (
                self.seq_weight * seq_sim +
                self.vector_weight * doc['vector_sim']
            )
            
            all_scores.append({
                'doi': doc['doi'],
                'seq_sim': seq_sim,
                'vec_sim': doc['vector_sim'],
                'combined': combined_score
            })
            
            if combined_score > best_score:
                best_score = combined_score
                best_doc = doc
        
        # è®°å½•å‰3ä¸ªæœ€é«˜åˆ†æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if all_scores:
            top3 = sorted(all_scores, key=lambda x: x['combined'], reverse=True)[:3]
            logger.debug(f"   Top 3 åŒ¹é…:")
            for i, score_info in enumerate(top3, 1):
                logger.debug(f"      {i}. DOI={score_info['doi'][:30]}... "
                           f"æ–‡æœ¬ç›¸ä¼¼åº¦={score_info['seq_sim']:.3f} "
                           f"å‘é‡ç›¸ä¼¼åº¦={score_info['vec_sim']:.3f} "
                           f"ç»¼åˆ={score_info['combined']:.3f}")
        
        return best_doc, best_score
