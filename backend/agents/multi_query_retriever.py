"""
å¤šæŸ¥è¯¢æ£€ç´¢å™¨
è´Ÿè´£æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢å¹¶åˆå¹¶ç»“æœ
"""
import logging
import requests
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

from backend.repositories.vector_repository import VectorRepository
from backend.config.settings import settings

logger = logging.getLogger(__name__)


@dataclass
class MultiQueryResult:
    """å¤šæŸ¥è¯¢æ£€ç´¢ç»“æœ"""
    documents: List[Dict]                # åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
    query_contributions: Dict[str, int]  # æ¯ä¸ªæŸ¥è¯¢çš„è´¡çŒ®åº¦
    total_before_dedup: int              # å»é‡å‰çš„æ€»æ•°
    total_after_dedup: int               # å»é‡åçš„æ€»æ•°
    retrieval_time: float                # æ£€ç´¢è€—æ—¶ï¼ˆç§’ï¼‰


class MultiQueryRetriever:
    """å¤šæŸ¥è¯¢æ£€ç´¢å™¨ç±»"""
    
    def __init__(self, vector_repo: VectorRepository, bge_api_url: str):
        """
        åˆå§‹åŒ–å¤šæŸ¥è¯¢æ£€ç´¢å™¨
        
        Args:
            vector_repo: å‘é‡æ•°æ®åº“ä»“å‚¨å®ä¾‹
            bge_api_url: BGE APIåœ°å€
        """
        self.vector_repo = vector_repo
        self.bge_api_url = bge_api_url
        logger.info("âœ… MultiQueryRetriever åˆå§‹åŒ–æˆåŠŸ")
    
    def _generate_embeddings_batch(self, queries: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡ç”ŸæˆæŸ¥è¯¢embedding
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            embeddingåˆ—è¡¨
            
        Raises:
            Exception: å¦‚æœAPIè°ƒç”¨å¤±è´¥
        """
        if not queries:
            return []
        
        try:
            logger.info(f"ğŸ”¢ æ­£åœ¨ä¸º {len(queries)} ä¸ªæŸ¥è¯¢ç”Ÿæˆembedding...")
            response = requests.post(
                self.bge_api_url,
                json={"input": queries},
                timeout=30
            )
            response.raise_for_status()
            embeddings = [item["embedding"] for item in response.json()["data"]]
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªembedding")
            return embeddings
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ç”Ÿæˆembeddingå¤±è´¥: {e}")
            raise
    
    def _retrieve_single(
        self, 
        query: str, 
        query_embedding: List[float], 
        top_k: int = 20
    ) -> List[Dict]:
        """
        å•ä¸ªæŸ¥è¯¢æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            query_embedding: æŸ¥è¯¢embedding
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            result = self.vector_repo.search(
                query=query,
                query_embedding=query_embedding,
                n_results=top_k
            )
            
            if not result.get("success"):
                logger.warning(f"âš ï¸ æŸ¥è¯¢å¤±è´¥: {query}")
                return []
            
            # æ„å»ºæ–‡æ¡£åˆ—è¡¨
            documents = []
            for i in range(len(result.get("documents", []))):
                doc = {
                    "text": result["documents"][i],
                    "metadata": result["metadatas"][i],
                    "distance": result["distances"][i],
                    "id": result["ids"][i],
                    "score": 1 - result["distances"][i],  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    "source_query": query  # è®°å½•æ¥æºæŸ¥è¯¢
                }
                documents.append(doc)
            
            logger.info(f"âœ… æŸ¥è¯¢ '{query}' è¿”å› {len(documents)} ä¸ªç»“æœ")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ å•ä¸ªæŸ¥è¯¢æ£€ç´¢å¤±è´¥ ({query}): {e}")
            return []
    
    def deduplicate_by_doi(self, documents: List[Dict]) -> List[Dict]:
        """
        æŒ‰DOIå»é‡ï¼Œä¿ç•™æ¯ä¸ªDOIçš„æœ€é«˜ç›¸ä¼¼åº¦æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []
        
        # ä½¿ç”¨å­—å…¸å­˜å‚¨æ¯ä¸ªDOIçš„æœ€ä½³æ–‡æ¡£
        doi_to_best_doc: Dict[str, Dict] = {}
        
        for doc in documents:
            # æå–DOIï¼ˆæ”¯æŒ 'doi' å’Œ 'DOI' ä¸¤ç§å­—æ®µåï¼‰
            metadata = doc.get("metadata", {})
            doi = metadata.get("doi") or metadata.get("DOI")
            
            if not doi:
                # å¦‚æœæ²¡æœ‰DOIï¼Œä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                # ä½¿ç”¨ Python çš„ id() å‡½æ•°è·å–å¯¹è±¡çš„å”¯ä¸€æ ‡è¯†
                doi = f"no_doi_{id(doc)}"
            
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡é‡åˆ°è¿™ä¸ªDOIï¼Œæˆ–è€…å½“å‰æ–‡æ¡£çš„åˆ†æ•°æ›´é«˜
            if doi not in doi_to_best_doc or doc["score"] > doi_to_best_doc[doi]["score"]:
                doi_to_best_doc[doi] = doc
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰åˆ†æ•°æ’åº
        deduped_docs = list(doi_to_best_doc.values())
        deduped_docs.sort(key=lambda x: x["score"], reverse=True)
        
        logger.info(f"âœ… å»é‡å®Œæˆ: {len(documents)} -> {len(deduped_docs)} ä¸ªæ–‡æ¡£")
        return deduped_docs
    
    def retrieve(
        self, 
        queries: List[str], 
        top_k_per_query: int = 20
    ) -> MultiQueryResult:
        """
        æ‰§è¡Œå¤šæŸ¥è¯¢æ£€ç´¢ï¼ˆä½¿ç”¨å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–ï¼‰
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            top_k_per_query: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°
            
        Returns:
            å¤šæŸ¥è¯¢æ£€ç´¢ç»“æœ
        """
        start_time = time.time()
        
        if not queries:
            logger.warning("âš ï¸ æŸ¥è¯¢åˆ—è¡¨ä¸ºç©º")
            return MultiQueryResult(
                documents=[],
                query_contributions={},
                total_before_dedup=0,
                total_after_dedup=0,
                retrieval_time=0.0
            )
        
        logger.info(f"ğŸ” å¼€å§‹å¤šæŸ¥è¯¢æ£€ç´¢: {len(queries)} ä¸ªæŸ¥è¯¢")
        
        # 1. æ‰¹é‡ç”Ÿæˆembedding
        try:
            embeddings = self._generate_embeddings_batch(queries)
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ç”Ÿæˆembeddingå¤±è´¥: {e}")
            return MultiQueryResult(
                documents=[],
                query_contributions={},
                total_before_dedup=0,
                total_after_dedup=0,
                retrieval_time=time.time() - start_time
            )
        
        # 2. å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        all_documents = []
        query_contributions = {}
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢
        max_workers = min(len(queries), 3)  # æœ€å¤š3ä¸ªå¹¶è¡Œçº¿ç¨‹
        logger.info(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œçº¿ç¨‹æ‰§è¡ŒæŸ¥è¯¢")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
            future_to_query = {
                executor.submit(self._retrieve_single, query, embedding, top_k_per_query): query
                for query, embedding in zip(queries, embeddings)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    docs = future.result()
                    all_documents.extend(docs)
                    query_contributions[query] = len(docs)
                except Exception as e:
                    logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥ ({query}): {e}")
                    query_contributions[query] = 0
        
        total_before_dedup = len(all_documents)
        
        # 3. å»é‡
        deduped_documents = self.deduplicate_by_doi(all_documents)
        total_after_dedup = len(deduped_documents)
        
        retrieval_time = time.time() - start_time
        
        logger.info(f"âœ… å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆ:")
        logger.info(f"   - æŸ¥è¯¢æ•°é‡: {len(queries)}")
        logger.info(f"   - å¹¶è¡Œçº¿ç¨‹: {max_workers}")
        logger.info(f"   - å»é‡å‰: {total_before_dedup} ä¸ªæ–‡æ¡£")
        logger.info(f"   - å»é‡å: {total_after_dedup} ä¸ªæ–‡æ¡£")
        logger.info(f"   - è€—æ—¶: {retrieval_time:.2f}s")
        logger.info(f"   - å„æŸ¥è¯¢è´¡çŒ®: {query_contributions}")
        
        return MultiQueryResult(
            documents=deduped_documents,
            query_contributions=query_contributions,
            total_before_dedup=total_before_dedup,
            total_after_dedup=total_after_dedup,
            retrieval_time=retrieval_time
        )
