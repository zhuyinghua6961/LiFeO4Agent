"""
ç¤¾åŒºæ‘˜è¦ä¸“å®¶ - Community Expert
åŠŸèƒ½ï¼šåŸºäºç¤¾åŒºæ‘˜è¦å‘é‡æ•°æ®åº“è¿›è¡ŒæŠ€æœ¯åˆ†æå’Œå…³ç³»æ´å¯Ÿ
"""
from typing import Dict, List, Any, Optional
import logging

from backend.services.llm_service import LLMService
from backend.repositories.vector_repository import CommunityVectorRepository

logger = logging.getLogger(__name__)


class CommunityExpert:
    """ç¤¾åŒºæ‘˜è¦ä¸“å®¶ - å¤„ç†æŠ€æœ¯æœºåˆ¶åˆ†æã€å¤šå› ç´ å…³ç³»ç ”ç©¶ç­‰"""
    
    def __init__(
        self, 
        community_repo: Optional[CommunityVectorRepository] = None,
        llm_service: Optional[LLMService] = None
    ):
        """
        åˆå§‹åŒ–ç¤¾åŒºæ‘˜è¦ä¸“å®¶
        
        Args:
            community_repo: ç¤¾åŒºå‘é‡æ•°æ®åº“ä»“å‚¨
            llm_service: LLMæœåŠ¡å®ä¾‹
        """
        self._community_repo = community_repo or CommunityVectorRepository()
        self._llm = llm_service
        
        logger.info("ğŸ˜ï¸ ç¤¾åŒºæ‘˜è¦ä¸“å®¶åˆå§‹åŒ–å®Œæˆ")
    
    def can_handle(self, question: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦é€‚åˆä½¿ç”¨ç¤¾åŒºæ‘˜è¦åˆ†æ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            True=é€‚åˆç¤¾åŒºåˆ†æ, False=ä¸é€‚åˆ
        """
        question_lower = question.lower()
        
        # æŠ€æœ¯åˆ†æç±»å…³é”®è¯
        technical_keywords = [
            "æœºåˆ¶", "mechanism", "å…³ç³»", "relationship",
            "å½±å“", "impact", "è¶‹åŠ¿", "trend",
            "è§„å¾‹", "pattern", "åˆ†æ", "analysis",
            "ä¸ºä»€ä¹ˆ", "why", "å¦‚ä½•", "how",
            "ç ”ç©¶è¿›å±•", "progress", "å‘å±•", "development"
        ]
        
        return any(kw in question_lower for kw in technical_keywords)
    
    def search(
        self,
        query: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        æœç´¢ç¤¾åŒºæ‘˜è¦
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            results = self._community_repo.search(
                query=query,
                n_results=top_k
            )
            
            if not results.get("success"):
                return {
                    "success": False,
                    "error": results.get("error", "ç¤¾åŒºæœç´¢å¤±è´¥"),
                    "documents": [],
                    "metadatas": [],
                    "distances": []
                }
            
            return {
                "success": True,
                "query_type": "community_analysis",
                "documents": results.get("documents", []),
                "metadatas": results.get("metadatas", []),
                "distances": results.get("distances", []),
                "result_count": len(results.get("documents", []))
            }
            
        except Exception as e:
            logger.error(f"ç¤¾åŒºæœç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "documents": [],
                "metadatas": [],
                "distances": []
            }
    
    def analyze(
        self,
        query: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        ç»¼åˆåˆ†æï¼ˆæœç´¢ + LLM åˆæˆï¼‰
        
        Args:
            query: åˆ†ææŸ¥è¯¢
            top_k: æ£€ç´¢æ•°é‡
            
        Returns:
            åˆ†æç»“æœ
        """
        # 1. æœç´¢ç¤¾åŒºæ‘˜è¦
        search_results = self.search(query, top_k)
        
        if not search_results.get("success"):
            return search_results
        
        # 2. ä½¿ç”¨ LLM åˆæˆç­”æ¡ˆï¼ˆå¦‚æœæœ‰ LLM æœåŠ¡ï¼‰
        if self._llm and search_results.get("documents"):
            try:
                # æ ¼å¼åŒ–ç¤¾åŒºæ‘˜è¦
                formatted_summaries = []
                for i, (doc, metadata) in enumerate(zip(
                    search_results["documents"],
                    search_results["metadatas"]
                ), 1):
                    summary_text = f"""
ç¤¾åŒºæ‘˜è¦ {i}:
  - çº§åˆ«: {metadata.get('level', 'Unknown')}
  - å®ä½“æ•°: {len(metadata.get('entities', []))}
  - å†…å®¹: {doc}
"""
                    formatted_summaries.append(summary_text)
                
                summaries_text = "\n".join(formatted_summaries)
                
                # æ„å»ºæç¤ºè¯
                prompt = f"""åŸºäºä»¥ä¸‹ç¤¾åŒºæ‘˜è¦ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€ç¤¾åŒºæ‘˜è¦ä¿¡æ¯ã€‘
{summaries_text}

è¯·æä¾›æ·±å…¥çš„æŠ€æœ¯åˆ†æå’Œæ´å¯Ÿã€‚"""
                
                from langchain_core.messages import HumanMessage, SystemMessage
                messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªææ–™ç§‘å­¦æŠ€æœ¯åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»ç¤¾åŒºçº§åˆ«çš„çŸ¥è¯†ä¸­æå–æ´å¯Ÿã€‚"),
                    HumanMessage(content=prompt)
                ]
                
                answer = self._llm.invoke(messages).content
                
                search_results["final_answer"] = answer
                
            except Exception as e:
                logger.error(f"LLM åˆæˆç­”æ¡ˆå¤±è´¥: {e}")
                search_results["llm_error"] = str(e)
        
        return search_results
