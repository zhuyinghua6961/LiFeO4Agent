"""
è¯­ä¹‰æœç´¢ä¸“å®¶ - Semantic Expert
åŠŸèƒ½ï¼šåŸºäºå‘é‡æ•°æ®åº“è¿›è¡Œæ–‡çŒ®è¯­ä¹‰æœç´¢
"""
from typing import Dict, List, Any, Optional
import logging
import os
import json
import re
import requests

from backend.services.llm_service import LLMService
from backend.repositories.vector_repository import VectorRepository
from backend.utils.pdf_loader import PDFManager
from backend.utils.doi_inserter import ProgrammaticDOIInserter

logger = logging.getLogger(__name__)


class SemanticExpert:
    """è¯­ä¹‰æœç´¢ä¸“å®¶ - å¤„ç†åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ–‡çŒ®æ£€ç´¢"""
    
    def __init__(
        self, 
        vector_repo: VectorRepository,
        llm_service: Optional[LLMService] = None
    ):
        """
        åˆå§‹åŒ–è¯­ä¹‰æœç´¢ä¸“å®¶
        
        Args:
            vector_repo: å‘é‡æ•°æ®åº“ä»“å‚¨
            llm_service: LLMæœåŠ¡å®ä¾‹ï¼ˆç”¨äºç»“æœå¢å¼ºï¼‰
        """
        self._vector_repo = vector_repo
        self._llm = llm_service
        
        # åŠ è½½promptæ¨¡æ¿
        self._search_prompt = self._build_search_prompt()
        self._semantic_synthesis_prompt = self._load_prompt("semantic_synthesis_prompt_clean.txt")
        self._broad_question_prompt = self._load_prompt("broad_question_synthesis_prompt.txt")
        
        # åˆå§‹åŒ–PDFç®¡ç†å™¨
        from backend.config.settings import settings
        self._pdf_manager = PDFManager(
            papers_dir=settings.papers_dir,
            mapping_file=settings.doi_to_pdf_mapping
        ) if hasattr(settings, 'papers_dir') else None
        
        # åˆå§‹åŒ–DOIæ’å…¥å™¨
        self._doi_inserter = ProgrammaticDOIInserter(
            similarity_threshold=0.22,  # åŸºäºå®é™…æµ‹è¯•ä¼˜åŒ–çš„é˜ˆå€¼
            seq_weight=0.4,  # å‘é‡ç›¸ä¼¼åº¦æƒé‡æ›´é«˜,å› ä¸ºLLMä¼šé‡ç»„è¡¨è¾¾
            vector_weight=0.6,
            max_compare_chars=1000
        )
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½®
        self._broad_threshold = getattr(settings, 'broad_similarity_threshold', 0.55)  # ä»0.65é™åˆ°0.55
        self._precise_threshold = getattr(settings, 'precise_similarity_threshold', 0.45)  # ä»0.5é™åˆ°0.45
        
        # BGE APIé…ç½®ï¼ˆç”¨äºç”ŸæˆæŸ¥è¯¢embeddingï¼‰
        self._bge_api_url = settings.bge_api_url
        
        logger.info("ğŸ“š è¯­ä¹‰æœç´¢ä¸“å®¶åˆå§‹åŒ–å®Œæˆ")
    
    def _load_prompt(self, filename: str) -> str:
        """åŠ è½½promptæ¨¡æ¿æ–‡ä»¶"""
        try:
            from backend.config.settings import settings
            prompt_path = os.path.join(settings.base_dir, "config", "prompts", filename)
            
            if os.path.exists(prompt_path):
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                logger.warning(f"Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
                return ""
        except Exception as e:
            logger.error(f"åŠ è½½promptå¤±è´¥ ({filename}): {e}")
            return ""
    
    def _build_search_prompt(self) -> str:
        """æ„å»ºè¯­ä¹‰æœç´¢æç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªæ–‡çŒ®æ£€ç´¢ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºè¯­ä¹‰æœç´¢æŸ¥è¯¢ã€‚

## æœç´¢ç­–ç•¥

1. **æå–æ ¸å¿ƒæ¦‚å¿µ**ï¼š
   - ææ–™åç§°ï¼ˆå¦‚ LiFePO4, NMC, LCOï¼‰
   - åˆæˆæ–¹æ³•ï¼ˆå¦‚æ°´çƒ­æ³•ã€æº¶èƒ¶å‡èƒ¶æ³•ã€çƒç£¨æ³•ï¼‰
   - æ”¹æ€§ç­–ç•¥ï¼ˆå¦‚ç¢³åŒ…è¦†ã€ç¦»å­æºæ‚ã€è¡¨é¢æ”¹æ€§ï¼‰
   - æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚é«˜å¯¼ç”µæ€§ã€é«˜å®¹é‡ã€é•¿å¾ªç¯ï¼‰

2. **æ„å»ºæœç´¢æŸ¥è¯¢**ï¼š
   - ä½¿ç”¨ç®€æ´çš„å…³é”®è¯ç»„åˆ
   - å¯ä»¥åŒ…å«å¤šä¸ªæ¦‚å¿µï¼Œç”¨ç©ºæ ¼æˆ–é€—å·åˆ†éš”
   - ä¿æŒæŸ¥è¯¢ç®€æ´ï¼ˆä¸è¶…è¿‡50å­—ï¼‰

3. **æœç´¢ç»“æœæ’åº**ï¼š
   - ç›¸å…³æ€§ä¼˜å…ˆ
   - è€ƒè™‘æ–‡çŒ®çš„æ–°è¿‘åº¦
   - ä¼˜å…ˆè¿”å›åŒ…å«å®Œæ•´æ‘˜è¦çš„æ–‡çŒ®

## è¾“å‡ºè¦æ±‚

åªè¿”å›æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç¤ºä¾‹ï¼š
- è¾“å…¥ï¼š"æœ‰å“ªäº›å…³äºé«˜å¯¼ç”µæ€§LiFePO4çš„ç ”ç©¶ï¼Ÿ"
- è¾“å‡ºï¼š"é«˜å¯¼ç”µæ€§ LiFePO4"
- è¾“å…¥ï¼š"æ°´çƒ­åˆæˆæ³•åˆ¶å¤‡çš„ç£·é…¸é“é”‚ææ–™æ–‡çŒ®"
- è¾“å‡ºï¼š"æ°´çƒ­åˆæˆ ç£·é…¸é“é”‚"
- è¾“å…¥ï¼š"ç¢³åŒ…è¦†æ”¹æ€§çš„ç›¸å…³ç ”ç©¶"
- è¾“å‡ºï¼š"ç¢³åŒ…è¦† æ”¹æ€§"
"""
    
    def can_handle(self, question: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦é€‚åˆä½¿ç”¨è¯­ä¹‰æœç´¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            True=é€‚åˆè¯­ä¹‰æœç´¢, False=ä¸é€‚åˆ
        """
        question_lower = question.lower()
        
        # è¯­ä¹‰æœç´¢å…³é”®è¯
        semantic_keywords = [
            "æ–‡çŒ®", "è®ºæ–‡", "ç ”ç©¶", "æ–‡ç« ", "æŠ¥é“",
            "å…³äº", "ç›¸å…³", "æœ‰å“ªäº›", "å“ªäº›",
            "æœç´¢", "æŸ¥æ‰¾", "å¯»æ‰¾", "æ£€ç´¢",
            "ææ–™", "æ–¹æ³•", "åˆ¶å¤‡", "åˆæˆ",
            "æ”¹æ€§", "åŒ…è¦†", "æºæ‚", "ç»“æ„"
        ]
        
        # å¦‚æœé—®é¢˜åŒ…å«è¿™äº›è¯ï¼Œè®¤ä¸ºé€‚åˆè¯­ä¹‰æœç´¢
        return any(kw in question_lower for kw in semantic_keywords)
    
    def generate_search_query(self, question: str) -> str:
        """
        ç”Ÿæˆè¯­ä¹‰æœç´¢æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        if self._llm is None:
            # ä½¿ç”¨è§„åˆ™ç”Ÿæˆç®€å•æŸ¥è¯¢
            return self._generate_simple_query(question)
        
        try:
            from langchain_core.messages import HumanMessage, SystemMessage
            
            messages = [
                SystemMessage(content=self._search_prompt),
                HumanMessage(content=f"ç”¨æˆ·é—®é¢˜ï¼š{question}")
            ]
            
            response = self._llm.invoke(messages)
            query = response.content.strip()
            
            # å»é™¤å¯èƒ½çš„å¼•å·å’Œä»£ç å—æ ‡è®°
            query = query.strip('"\'')
            if "```" in query:
                query = query.split("```")[0].strip()
            
            return query
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
            return self._generate_simple_query(question)
    
    def _generate_simple_query(self, question: str) -> str:
        """
        ä½¿ç”¨è§„åˆ™ç”Ÿæˆç®€å•çš„æœç´¢æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = [
            "æœ‰å“ªäº›", "æœ‰æ²¡æœ‰", "è¯·æŸ¥æ‰¾", "æœç´¢", "æŸ¥æ‰¾",
            "å…³äº", "è¯·ç»™æˆ‘", "å¸®æˆ‘æ‰¾", "æˆ‘æƒ³æ‰¾"
        ]
        
        query = question
        for prefix in prefixes:
            if question.startswith(prefix):
                query = question[len(prefix):].strip()
                break
        
        # ç§»é™¤å¸¸è§åç¼€
        suffixes = ["çš„ç ”ç©¶", "çš„æ–‡çŒ®", "çš„æ–‡ç« ", "çš„ç›¸å…³", "çš„å†…å®¹", "ï¼Ÿ", "?"]
        for suffix in suffixes:
            if query.endswith(suffix):
                query = query[:-len(suffix)].strip()
                break
        
        return query if query else question
    
    def search(
        self, 
        question: str, 
        top_k: int = 15,  # ä»10å¢åŠ åˆ°15
        with_scores: bool = False,
        filter_metadata: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯­ä¹‰æœç´¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            with_scores: æ˜¯å¦è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
            filter_metadata: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æœç´¢ç»“æœ
        """
        # ç§»é™¤ can_handle æ£€æŸ¥ï¼Œå…è®¸æ‰€æœ‰é—®é¢˜è¿›è¡Œè¯­ä¹‰æœç´¢
        
        try:
            # ç”Ÿæˆæœç´¢æŸ¥è¯¢
            logger.info("\n" + "="*80)
            logger.info("ğŸ“ [æ­¥éª¤2] æå–å…³é”®è¯")
            search_query = self.generate_search_query(question)
            logger.info(f"å…³é”®è¯: {search_query}")
            logger.info("="*80)
            
            # ç”ŸæˆæŸ¥è¯¢çš„embeddingå‘é‡ï¼ˆä½¿ç”¨BGE APIï¼‰
            logger.info("\n" + "="*80)
            logger.info("ğŸ”¢ [æ­¥éª¤3] ç”ŸæˆæŸ¥è¯¢å‘é‡(Embedding)")
            logger.info(f"BGE APIåœ°å€: {self._bge_api_url}")
            logger.info(f"è¾“å…¥æ–‡æœ¬: {search_query}")
            try:
                response = requests.post(
                    self._bge_api_url,
                    json={"input": [search_query]},
                    timeout=30
                )
                response.raise_for_status()
                query_embedding = response.json()["data"][0]["embedding"]
                logger.info(f"âœ… æˆåŠŸç”Ÿæˆembedding")
                logger.info(f"å‘é‡ç»´åº¦: {len(query_embedding)}")
                logger.info(f"å‘é‡å‰5ç»´: {query_embedding[:5]}")
                logger.info("="*80)
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {e}")
                return {
                    "success": False,
                    "error": f"ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {str(e)}",
                    "error_step": "generate_embedding",
                    "expert": "semantic",
                    "documents": []
                }
            
            # æ‰§è¡Œæœç´¢
            logger.info("\n" + "="*80)
            logger.info("ğŸ” [æ­¥éª¤4] æŸ¥è¯¢å‘é‡æ•°æ®åº“")
            logger.info(f"æ£€ç´¢æ•°é‡: top_k={top_k}")
            results = self._vector_repo.search(
                query_embedding=query_embedding,
                n_results=top_k,
                where_filter=filter_metadata
            )
            
            if not results.get('success'):
                logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {results.get('error')}")
                return {
                    "success": False,
                    "error": results.get('error', 'æœç´¢å¤±è´¥'),
                    "error_step": "vector_search",
                    "expert": "semantic"
                }
            
            # æ ¼å¼åŒ–ç»“æœ
            documents = []
            docs = results.get('documents', [])
            metadatas = results.get('metadatas', [])
            distances = results.get('distances', [])
            ids = results.get('ids', [])
            
            for i, doc_content in enumerate(docs):
                doc_data = {
                    "id": ids[i] if i < len(ids) else str(i),
                    "content": doc_content,
                }
                if i < len(metadatas) and metadatas[i]:
                    doc_data["metadata"] = metadatas[i]
                if with_scores and i < len(distances):
                    # ChromaDB ä½¿ç”¨ cosine è·ç¦» (èŒƒå›´ 0-2)
                    # ä½™å¼¦ç›¸ä¼¼åº¦ = 1 - (cosine_distance / 2)
                    # è·ç¦»è¶Šå°,ç›¸ä¼¼åº¦è¶Šé«˜
                    distance = distances[i]
                    similarity = 1 - (distance / 2.0)  # è½¬æ¢ä¸º 0-1 èŒƒå›´çš„ç›¸ä¼¼åº¦
                    doc_data["score"] = max(0.0, min(1.0, similarity))  # ç¡®ä¿åœ¨ 0-1 èŒƒå›´å†…
                documents.append(doc_data)
            
            # åº”ç”¨ç›¸ä¼¼åº¦è¿‡æ»¤
            filtered_documents = self._filter_by_similarity(
                documents=documents,
                question=question,
                with_scores=with_scores
            )
            
            logger.info(f"âœ… æ£€ç´¢æˆåŠŸ")
            logger.info(f"åŸå§‹ç»“æœæ•°: {len(documents)}")
            logger.info(f"è¿‡æ»¤åç»“æœæ•°: {len(filtered_documents)}")
            logger.info("\nå‰3æ¡æ£€ç´¢ç»“æœé¢„è§ˆ:")
            for i, doc in enumerate(filtered_documents[:3], 1):
                score = doc.get('score', 0)
                content_preview = doc.get('content', '')[:100]
                doi = doc.get('metadata', {}).get('DOI', 'N/A')
                logger.info(f"  [{i}] ç›¸ä¼¼åº¦={score:.4f}, DOI={doi}")
                logger.info(f"      å†…å®¹: {content_preview}...")
            logger.info("="*80)
            
            return {
                "success": True,
                "expert": "semantic",
                "search_query": search_query,
                "result_count": len(filtered_documents),
                "original_count": len(documents),
                "documents": filtered_documents,
                "question": question
            }
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "error_step": "search",
                "expert": "semantic"
            }
    
    def search_by_material(self, material: str, top_k: int = 5) -> Dict[str, Any]:
        """
        æŒ‰ææ–™åç§°æœç´¢æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            material: ææ–™åç§°
            top_k: ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        return self.search(f"å…³äº{material}çš„æ–‡çŒ®", top_k=top_k)
    
    def search_by_method(self, method: str, top_k: int = 5) -> Dict[str, Any]:
        """
        æŒ‰åˆæˆæ–¹æ³•æœç´¢æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            method: åˆæˆæ–¹æ³•åç§°
            top_k: ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        return self.search(f"{method}åˆ¶å¤‡çš„ææ–™æ–‡çŒ®", top_k=top_k)
    
    def search_by_modification(
        self, 
        modification: str, 
        material: str = "LiFePO4",
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        æŒ‰æ”¹æ€§ç­–ç•¥æœç´¢æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            modification: æ”¹æ€§ç­–ç•¥
            material: ææ–™åç§°
            top_k: ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        return self.search(f"{modification}æ”¹æ€§çš„{material}", top_k=top_k)
    
    def find_similar(
        self, 
        document_text: str, 
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼çš„æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            document_text: æ–‡æ¡£å†…å®¹
            top_k: ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼æ–‡çŒ®åˆ—è¡¨
        """
        try:
            results = self._vector_repo.find_similar(
                document_text=document_text,
                top_k=top_k
            )
            
            documents = []
            for doc in results:
                doc_data = {
                    "content": doc.page_content if hasattr(doc, 'page_content') else str(doc),
                }
                if hasattr(doc, 'metadata') and doc.metadata:
                    doc_data["metadata"] = doc.metadata
                documents.append(doc_data)
            
            return {
                "success": True,
                "expert": "semantic",
                "result_count": len(documents),
                "documents": documents
            }
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼æ–‡çŒ®å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "expert": "semantic"
            }
    
    def aggregate_results(
        self, 
        search_results: Dict[str, Any],
        llm_enhanced: bool = True
    ) -> Dict[str, Any]:
        """
        èšåˆæœç´¢ç»“æœï¼ˆå¯é€‰ï¼šä½¿ç”¨LLMå¢å¼ºï¼‰
        
        Args:
            search_results: æœç´¢ç»“æœ
            llm_enhanced: æ˜¯å¦ä½¿ç”¨LLMå¢å¼º
            
        Returns:
            èšåˆåçš„ç»“æœ
        """
        if not search_results.get("success"):
            return search_results
        
        documents = search_results.get("documents", [])
        
        # æå–å…³é”®ä¿¡æ¯
        summary = {
            "total_found": search_results["result_count"],
            "expert": "semantic",
            "search_query": search_results.get("search_query", ""),
            "key_topics": [],
            "material_mentioned": [],
            "methods_mentioned": []
        }
        
        # ç®€å•æå–å…³é”®è¯
        for doc in documents[:10]:  # åªåˆ†æå‰10ä¸ª
            content = doc.get("content", "")
            metadata = doc.get("metadata", {})
            
            # ä»æ‘˜è¦ä¸­æå–ææ–™å’Œæ–¹æ³•
            if "LiFePO4" in content or "LFP" in content:
                if "LiFePO4" not in summary["material_mentioned"]:
                    summary["material_mentioned"].append("LiFePO4")
            
            if "NMC" in content or "NCM" in content:
                if "NMC" not in summary["material_mentioned"]:
                    summary["material_mentioned"].append("NMC")
            
            # å¸¸è§æ–¹æ³•
            methods = ["æ°´çƒ­", "æº¶èƒ¶å‡èƒ¶", "çƒç£¨", "å…±æ²‰æ·€", "å–·é›¾å¹²ç‡¥"]
            for method in methods:
                if method in content and method not in summary["methods_mentioned"]:
                    summary["methods_mentioned"].append(method)
        
            return {
                "success": True,
                "expert": "semantic",
                "summary": summary,
                "documents": documents,
                "llm_enhanced": llm_enhanced
            }
    
    def _is_broad_question(self, question: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå®½æ³›é—®é¢˜"""
        broad_keywords = [
            "æœ‰å“ªäº›", "å“ªäº›", "ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ",
            "ç»¼è¿°", "æ¦‚è¿°", "æ€»ç»“", "ä»‹ç»", "å‘å±•",
            "ç ”ç©¶è¿›å±•", "ç ”ç©¶ç°çŠ¶", "åº”ç”¨", "å‰æ™¯"
        ]
        return any(kw in question for kw in broad_keywords)
    
    def _filter_by_similarity(
        self,
        documents: List[Dict],
        question: str,
        with_scores: bool = True
    ) -> List[Dict]:
        """æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœ"""
        if not with_scores or not documents:
            return documents
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹ï¼Œé€‰æ‹©é˜ˆå€¼
        is_broad = self._is_broad_question(question)
        threshold = self._broad_threshold if is_broad else self._precise_threshold
        
        filtered = []
        filtered_count = 0
        
        for doc in documents:
            score = doc.get('score', 1.0)
            if score >= threshold:
                filtered.append(doc)
            else:
                filtered_count += 1
        
        logger.info(
            f"ç›¸ä¼¼åº¦è¿‡æ»¤: é˜ˆå€¼={threshold:.2f} ({'å®½æ³›' if is_broad else 'ç²¾ç¡®'}é—®é¢˜), "
            f"ä¿ç•™={len(filtered)}, è¿‡æ»¤={filtered_count}"
        )
        
        return filtered
    
    def _extract_dois(self, documents: List[Dict]) -> List[str]:
        """ä»æ–‡æ¡£ä¸­æå–DOI"""
        dois = []
        for doc in documents:
            metadata = doc.get('metadata', {})
            # ä»metadataä¸­æå–DOI
            doi = metadata.get('doi') or metadata.get('DOI')
            if doi:
                dois.append(doi)
            else:
                # ä»å†…å®¹ä¸­æå–DOI
                content = doc.get('content', '')
                # ä¿®æ­£æ­£åˆ™: æ’é™¤æ–¹æ‹¬å·å’Œå…¶ä»–ç¬¦å·
                doi_match = re.search(r'10\.\d+/[^\s)\]\>]+', content)
                if doi_match:
                    dois.append(doi_match.group())
        return dois
    
    def _load_pdf_contents(
        self,
        dois: List[str],
        max_pages: int = 30,
        max_chars: int = 20000
    ) -> Dict[str, str]:
        """åŠ è½½å¤šä¸ªDOIçš„PDFå†…å®¹"""
        if not self._pdf_manager:
            return {}
        
        pdf_contents = {}
        for doi in dois[:3]:  # æœ€å¤šåŠ è½½3ç¯‡
            content = self._pdf_manager.load_pdf_by_doi(
                doi=doi,
                max_pages=max_pages,
                max_chars=max_chars
            )
            if content:
                pdf_contents[doi] = content
        
        return pdf_contents
    
    def query_with_details(
        self,
        question: str,
        top_k: int = 20,  # ä»20ä¿æŒä¸å˜ï¼Œä½†ä¼šè¢«searchçš„é»˜è®¤å€¼15è¦†ç›–
        load_pdf: bool = True
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬PDFåŠ è½½æƒ…å†µï¼‰"""
        search_result = self.search(question, top_k=top_k, with_scores=True)
        
        if not search_result.get('success'):
            return {
                'answer': 'æ£€ç´¢å¤±è´¥',
                'pdf_info': {'error': search_result.get('error')}
            }
        
        documents = search_result.get('documents', [])
        if not documents:
            return {
                'answer': 'æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚',
                'pdf_info': {'documents_found': 0}
            }
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹
        is_broad = self._is_broad_question(question)
        
        # åˆå§‹åŒ–PDFä¿¡æ¯
        pdf_info = {
            'documents_found': len(documents),
            'is_broad_question': is_broad,
            'dois_found': 0,
            'pdf_loaded': 0,
            'pdf_failed': 0
        }
        
        # å®½æ³›é—®é¢˜ï¼šä¸åŠ è½½PDF
        if is_broad:
            logger.info("æ£€æµ‹åˆ°å®½æ³›é—®é¢˜ï¼Œä½¿ç”¨å®½æ³›é—®é¢˜åˆæˆæ¨¡æ¿ï¼ˆä¸åŠ è½½PDFï¼‰")
            answer = self._synthesize_broad_answer(question, documents)
            return {
                'answer': answer,
                'pdf_info': pdf_info
            }
        
        # ç²¾ç¡®é—®é¢˜ï¼šåŠ è½½PDF
        pdf_contents = {}
        if load_pdf and self._pdf_manager:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“„ [æ­¥éª¤5] åŠ è½½PDFåŸæ–‡")
            dois = self._extract_dois(documents)
            pdf_info['dois_found'] = len(dois)
            logger.info(f"æå–åˆ° {len(dois)} ä¸ªDOI")
            
            if dois:
                pdf_contents = self._load_pdf_contents(dois)
                pdf_info['pdf_loaded'] = len(pdf_contents)
                pdf_info['pdf_failed'] = len(dois) - len(pdf_contents)
                logger.info(f"\næ­£åœ¨åŠ è½½PDFåŸæ–‡ (æœ€å¤š3ç¯‡):")
                for idx, (doi, content) in enumerate(pdf_contents.items(), 1):
                    progress = f"[{idx}/{len(pdf_contents)}]"
                    size_kb = len(content) / 1024
                    logger.info(f"  {progress} âœ… {doi} ({size_kb:.1f}KB)")
                if pdf_info['pdf_failed'] > 0:
                    logger.info(f"  âš ï¸  {pdf_info['pdf_failed']} ç¯‡PDFåŠ è½½å¤±è´¥")
            else:
                logger.info("âš ï¸  æœªæå–åˆ°DOI")
            logger.info("="*80)
        
        answer = self._synthesize_semantic_answer(question, documents, pdf_contents)
        return {
            'answer': answer,
            'pdf_info': pdf_info
        }
    
    def _synthesize_semantic_answer(
        self,
        user_question: str,
        documents: List[Dict],
        pdf_contents: Optional[Dict[str, str]] = None
    ) -> str:
        """åˆæˆè¯­ä¹‰æœç´¢ç­”æ¡ˆï¼ˆç²¾ç¡®é—®é¢˜ï¼‰"""
        if not self._llm or not self._semantic_synthesis_prompt:
            return self._format_simple_answer(documents)
        
        try:
            # æ„å»ºæ–‡çŒ®åˆ—è¡¨ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡æ‰©å±•ï¼‰
            logger.info("\n" + "="*80)
            logger.info("ğŸ“– [æ­¥éª¤5.5] æ‰©å±•ä¸Šä¸‹æ–‡çª—å£")
            logger.info(f"åŸå§‹æ®µè½æ•°: {len(documents)}")
            
            literature_list = []
            for i, doc in enumerate(documents[:10], 1):
                chunk_id = doc.get('id')
                
                # è·å–å¸¦ä¸Šä¸‹æ–‡çš„å®Œæ•´å†…å®¹
                context_result = self._vector_repo.get_chunk_with_context(
                    chunk_id=chunk_id,
                    window=2  # å‰åå„2ä¸ªæ®µè½
                )
                
                if context_result.get('success'):
                    full_text = context_result['full_text']
                    context_range = context_result['context_range']
                    main_meta = context_result['metadata']
                    
                    logger.info(f"  [{i}] æ‰©å±•æˆåŠŸ: {context_result['context_chunks']}ä¸ªæ®µè½")
                    logger.info(f"      èŒƒå›´: ç¬¬{context_range['start_page']}-{context_range['end_page']}é¡µ")
                    logger.info(f"      é•¿åº¦: {len(full_text)} å­—ç¬¦")
                    
                    lit = {
                        "åºå·": i,
                        "å†…å®¹": full_text,  # ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œä¸æˆªæ–­
                        "æ ¸å¿ƒæ®µè½": context_result['main_text'][:200] + "...",  # æ ‡æ³¨æ ¸å¿ƒæ®µè½
                        "ä¸Šä¸‹æ–‡ä¿¡æ¯": f"ç¬¬{main_meta.get('page')}é¡µç¬¬{main_meta.get('chunk_index_in_page', 0)+1}æ®µï¼ˆå«å‰åå„2æ®µï¼‰"
                    }
                else:
                    # å¦‚æœè·å–ä¸Šä¸‹æ–‡å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
                    logger.warning(f"  [{i}] æ‰©å±•å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ®µè½")
                    lit = {
                        "åºå·": i,
                        "å†…å®¹": doc.get('content', ''),
                        "æ ¸å¿ƒæ®µè½": doc.get('content', '')[:200] + "...",
                        "ä¸Šä¸‹æ–‡ä¿¡æ¯": "ä»…æ ¸å¿ƒæ®µè½"
                    }
                
                if doc.get('metadata'):
                    lit["å…ƒæ•°æ®"] = doc['metadata']
                literature_list.append(lit)
            
            logger.info(f"âœ… ä¸Šä¸‹æ–‡æ‰©å±•å®Œæˆï¼Œå…± {len(literature_list)} ç¯‡æ–‡çŒ®")
            logger.info("="*80)
            
            literature_json = json.dumps(literature_list, ensure_ascii=False, indent=2)
            
            # æ·»åŠ PDFåŸæ–‡
            pdf_section = ""
            if pdf_contents:
                pdf_section = "\n\n## ğŸ“„ ç›¸å…³è®ºæ–‡åŸæ–‡æ‘˜è¦\n"
                for doi, content in pdf_contents.items():
                    pdf_section += f"\n### DOI: {doi}\n{content[:5000]}\n"
            
            prompt = self._semantic_synthesis_prompt.replace("{user_question}", user_question)
            prompt = prompt.replace("{literature_results}", literature_json)
            prompt = prompt.replace("{pdf_contents}", pdf_section if pdf_section else "æ— PDFåŸæ–‡")
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ“‹ [æ­¥éª¤6] æ„å»ºPrompt")
            logger.info(f"æ–‡çŒ®æ‘˜è¦: {len(literature_list)} ç¯‡")
            logger.info(f"PDFåŸæ–‡: {len(pdf_contents) if pdf_contents else 0} ç¯‡")
            logger.info(f"Prompté•¿åº¦: {len(prompt):,} å­—ç¬¦ (~{len(prompt)//4:,} tokens)")
            logger.info(f"\nPrompté¢„è§ˆ (å‰200å­—):")
            logger.info(prompt[:200] + "...")
            logger.info("="*80)
            
            from langchain_core.messages import HumanMessage
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ¤– [æ­¥éª¤7] ç”Ÿæˆå›ç­”")
            response = self._llm.invoke([HumanMessage(content=prompt)])
            pure_answer = response.content.strip()
            logger.info(f"âœ… LLMç”Ÿæˆçº¯å‡€ç­”æ¡ˆå®Œæˆ ({len(pure_answer)} å­—ç¬¦)")
            logger.info("="*80)
            
            # ç¨‹åºåŒ–æ’å…¥DOI
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Œ [æ­¥éª¤8] ç¨‹åºåŒ–æ’å…¥DOI")
            search_result_for_insert = {
                'documents': [doc.get('content', '') for doc in documents],
                'metadatas': [doc.get('metadata', {}) for doc in documents],
                'distances': [1.0 - doc.get('score', 0.5) for doc in documents]  # è½¬æ¢å›è·ç¦»
            }
            answer_with_doi = self._doi_inserter.insert_dois(pure_answer, search_result_for_insert)
            logger.info("="*80)
            
            return answer_with_doi
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰ç­”æ¡ˆåˆæˆå¤±è´¥: {e}")
            return self._format_simple_answer(documents)
    
    def _synthesize_broad_answer(
        self,
        user_question: str,
        documents: List[Dict]
    ) -> str:
        """åˆæˆå®½æ³›é—®é¢˜ç­”æ¡ˆ"""
        if not self._llm or not self._broad_question_prompt:
            return self._format_simple_answer(documents)
        
        try:
            # æå–æ–‡çŒ®æ‘˜è¦
            summaries = []
            for i, doc in enumerate(documents[:15], 1):
                summaries.append({
                    "åºå·": i,
                    "æ‘˜è¦": doc.get('content', '')[:800]
                })
            
            summaries_json = json.dumps(summaries, ensure_ascii=False, indent=2)
            
            prompt = self._broad_question_prompt.replace("{user_question}", user_question)
            prompt = prompt.replace("{literature_summaries}", summaries_json)
            
            from langchain_core.messages import HumanMessage
            
            response = self._llm.invoke([HumanMessage(content=prompt)])
            return response.content.strip()
            
        except Exception as e:
            logger.error(f"å®½æ³›é—®é¢˜ç­”æ¡ˆåˆæˆå¤±è´¥: {e}")
            return self._format_simple_answer(documents)
    
    def _format_simple_answer(self, documents: List[Dict]) -> str:
        """ç®€å•æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆæ— LLMæ—¶ä½¿ç”¨ï¼‰"""
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚"
        
        answer = f"æ‰¾åˆ° {len(documents)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼š\n\n"
        for i, doc in enumerate(documents[:10], 1):
            content = doc.get('content', '')[:200]
            score = doc.get('score', 0)
            answer += f"{i}. [ç›¸ä¼¼åº¦: {score:.2f}] {content}...\n\n"
        
        if len(documents) > 10:
            answer += f"... è¿˜æœ‰ {len(documents) - 10} ç¯‡æ–‡çŒ®æœªæ˜¾ç¤º"
        
        return answer
    
    def query(self, question: str, load_pdf: bool = True) -> str:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›æ ¼å¼åŒ–çš„ç­”æ¡ˆ"""
        result = self.search(question=question, top_k=15, with_scores=True)  # ä»20æ”¹ä¸º15
        
        if not result.get('success'):
            return f"æœç´¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        documents = result.get('documents', [])
        
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚"
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹
        is_broad = self._is_broad_question(question)
        
        # å®½æ³›é—®é¢˜ï¼šä¸åŠ è½½PDFï¼Œä½¿ç”¨å®½æ³›é—®é¢˜æ¨¡æ¿
        if is_broad:
            logger.info("æ£€æµ‹åˆ°å®½æ³›é—®é¢˜ï¼Œä½¿ç”¨å®½æ³›é—®é¢˜åˆæˆæ¨¡æ¿")
            return self._synthesize_broad_answer(question, documents)
        
        # ç²¾ç¡®é—®é¢˜ï¼šåŠ è½½PDFï¼Œä½¿ç”¨ç²¾ç¡®é—®é¢˜æ¨¡æ¿
        pdf_contents = {}
        if load_pdf and self._pdf_manager:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“„ [æ­¥éª¤5] æå–DOIå¹¶åŠ è½½PDFåŸæ–‡")
            dois = self._extract_dois(documents)
            logger.info(f"æå–åˆ°çš„DOIåˆ—è¡¨: {dois}")
            if dois:
                pdf_contents = self._load_pdf_contents(dois)
                logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(pdf_contents)} ç¯‡PDF")
                for doi, content in pdf_contents.items():
                    logger.info(f"  - {doi}: {len(content)} å­—ç¬¦")
            else:
                logger.info("âš ï¸  æœªæå–åˆ°DOI")
            logger.info("="*80)
        
        return self._synthesize_semantic_answer(question, documents, pdf_contents)
