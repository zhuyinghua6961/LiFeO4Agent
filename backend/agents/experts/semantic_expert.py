"""
è¯­ä¹‰æœç´¢ä¸“å®¶ - Semantic Expert
åŠŸèƒ½ï¼šåŸºäºå‘é‡æ•°æ®åº“è¿›è¡Œæ–‡çŒ®è¯­ä¹‰æœç´¢
"""
from typing import Dict, List, Any, Optional
import logging
import os
import json
import re
import requests

from backend.services.llm_service import LLMService
from backend.repositories.vector_repository import VectorRepository
from backend.utils.pdf_loader import PDFManager

logger = logging.getLogger(__name__)


class SemanticExpert:
    """è¯­ä¹‰æœç´¢ä¸“å®¶ - å¤„ç†åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ–‡çŒ®æ£€ç´¢"""
    
    def __init__(
        self, 
        vector_repo: VectorRepository,
        llm_service: Optional[LLMService] = None
    ):
        """
        åˆå§‹åŒ–è¯­ä¹‰æœç´¢ä¸“å®¶
        
        Args:
            vector_repo: å‘é‡æ•°æ®åº“ä»“å‚¨
            llm_service: LLMæœåŠ¡å®ä¾‹ï¼ˆç”¨äºç»“æœå¢å¼ºï¼‰
        """
        self._vector_repo = vector_repo
        self._llm = llm_service
        
        # åŠ è½½promptæ¨¡æ¿
        self._search_prompt = self._build_search_prompt()
        self._semantic_synthesis_prompt = self._load_prompt("semantic_synthesis_prompt_clean.txt")
        self._semantic_synthesis_prompt_robust = self._load_prompt("semantic_synthesis_prompt_robust.txt")
        self._broad_question_prompt = self._load_prompt("broad_question_synthesis_prompt.txt")
        
        # åˆå§‹åŒ–PDFç®¡ç†å™¨
        from backend.config.settings import settings
        self._pdf_manager = PDFManager(
            papers_dir=settings.papers_dir,
            mapping_file=settings.doi_to_pdf_mapping
        ) if hasattr(settings, 'papers_dir') else None
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½®
        self._broad_threshold = getattr(settings, 'similarity_threshold_broad', 0.3)
        self._precise_threshold = getattr(settings, 'similarity_threshold_precise', 0.3)
        
        # BGE APIé…ç½®ï¼ˆç”¨äºç”ŸæˆæŸ¥è¯¢embeddingå’Œå¥å­embeddingï¼‰
        self._bge_api_url = settings.bge_api_url
        
        # åˆå§‹åŒ–å¥å­çº§æ•°æ®åº“ï¼ˆäºŒçº§æ£€ç´¢ï¼‰
        self._sentence_collection = self._init_sentence_db()
        
        # åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºç»„ä»¶
        self._query_expander = None
        self._multi_query_retriever = None
        self._sentence_reranker = None
        
        # æ ¹æ®é…ç½®åˆå§‹åŒ–æ–°ç»„ä»¶
        if settings.enable_query_expansion or settings.enable_reranking:
            self._init_expansion_components()
        
        logger.info("ğŸ“š è¯­ä¹‰æœç´¢ä¸“å®¶åˆå§‹åŒ–å®Œæˆ")
    
    def _load_prompt(self, filename: str) -> str:
        """åŠ è½½promptæ¨¡æ¿æ–‡ä»¶"""
        try:
            from backend.config.settings import settings
            prompt_path = os.path.join(settings.base_dir, "config", "prompts", filename)
            
            if os.path.exists(prompt_path):
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                logger.warning(f"Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
                return ""
        except Exception as e:
            logger.error(f"åŠ è½½promptå¤±è´¥ ({filename}): {e}")
            return ""
    
    def _init_sentence_db(self):
        """åˆå§‹åŒ–å¥å­çº§æ•°æ®åº“ï¼ˆäºŒçº§æ£€ç´¢ï¼‰"""
        try:
            import chromadb
            from chromadb.config import Settings
            
            # å¥å­æ•°æ®åº“è·¯å¾„
            sentence_db_path = "/Users/zhuyinghua/Desktop/code/vector_sentence"
            
            client = chromadb.PersistentClient(
                path=sentence_db_path,
                settings=Settings(anonymized_telemetry=False)
            )
            
            collection = client.get_collection(
                name="lfp_papers_sentences_v1"
            )
            
            count = collection.count()
            logger.info(f"âœ… å¥å­çº§æ•°æ®åº“è¿æ¥æˆåŠŸ")
            logger.info(f"   è·¯å¾„: {sentence_db_path}")
            logger.info(f"   Collection: lfp_papers_sentences_v1")
            logger.info(f"   å¥å­æ•°é‡: {count:,}")
            
            return collection
            
        except Exception as e:
            logger.warning(f"âš ï¸  å¥å­çº§æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            logger.warning(f"   å°†å›é€€åˆ°PDFæœç´¢æ¨¡å¼")
            return None
    
    def _init_expansion_components(self):
        """åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºç»„ä»¶"""
        try:
            from backend.agents.query_expander import QueryExpander
            from backend.agents.multi_query_retriever import MultiQueryRetriever
            from backend.agents.sentence_reranker import SentenceReranker
            from backend.config.settings import settings
            
            # åˆå§‹åŒ–QueryExpander
            self._query_expander = QueryExpander(llm_service=self._llm)
            logger.info("âœ… QueryExpander åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–MultiQueryRetriever
            self._multi_query_retriever = MultiQueryRetriever(
                vector_repo=self._vector_repo,
                bge_api_url=self._bge_api_url
            )
            logger.info("âœ… MultiQueryRetriever åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–SentenceRerankerï¼ˆå¦‚æœå¥å­æ•°æ®åº“å¯ç”¨ï¼‰
            if self._sentence_collection:
                self._sentence_reranker = SentenceReranker(
                    sentence_collection=self._sentence_collection,
                    bge_api_url=self._bge_api_url
                )
                logger.info("âœ… SentenceReranker åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸  å¥å­æ•°æ®åº“ä¸å¯ç”¨ï¼ŒSentenceReranker æœªåˆå§‹åŒ–")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•ç»„ä»¶å¤±è´¥: {e}")
            logger.warning("âš ï¸  å°†ä½¿ç”¨åŸæœ‰çš„å•æŸ¥è¯¢ç­–ç•¥")
            self._query_expander = None
            self._multi_query_retriever = None
            self._sentence_reranker = None
    
    def _build_search_prompt(self) -> str:
        """æ„å»ºè¯­ä¹‰æœç´¢æç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªæ–‡çŒ®æ£€ç´¢ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºè¯­ä¹‰æœç´¢æŸ¥è¯¢ã€‚

## æœç´¢ç­–ç•¥

1. **æå–æ ¸å¿ƒæ¦‚å¿µ**ï¼š
   - ææ–™åç§°ï¼ˆå¦‚ LiFePO4, NMC, LCOï¼‰
   - åˆæˆæ–¹æ³•ï¼ˆå¦‚æ°´çƒ­æ³•ã€æº¶èƒ¶å‡èƒ¶æ³•ã€çƒç£¨æ³•ï¼‰
   - æ”¹æ€§ç­–ç•¥ï¼ˆå¦‚ç¢³åŒ…è¦†ã€ç¦»å­æºæ‚ã€è¡¨é¢æ”¹æ€§ï¼‰
   - æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚é«˜å¯¼ç”µæ€§ã€é«˜å®¹é‡ã€é•¿å¾ªç¯ï¼‰

2. **æ„å»ºæœç´¢æŸ¥è¯¢**ï¼š
   - ä½¿ç”¨ç®€æ´çš„å…³é”®è¯ç»„åˆ
   - å¯ä»¥åŒ…å«å¤šä¸ªæ¦‚å¿µï¼Œç”¨ç©ºæ ¼æˆ–é€—å·åˆ†éš”
   - ä¿æŒæŸ¥è¯¢ç®€æ´ï¼ˆä¸è¶…è¿‡50å­—ï¼‰

3. **æœç´¢ç»“æœæ’åº**ï¼š
   - ç›¸å…³æ€§ä¼˜å…ˆ
   - è€ƒè™‘æ–‡çŒ®çš„æ–°è¿‘åº¦
   - ä¼˜å…ˆè¿”å›åŒ…å«å®Œæ•´æ‘˜è¦çš„æ–‡çŒ®

## è¾“å‡ºè¦æ±‚

åªè¿”å›æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç¤ºä¾‹ï¼š
- è¾“å…¥ï¼š"æœ‰å“ªäº›å…³äºé«˜å¯¼ç”µæ€§LiFePO4çš„ç ”ç©¶ï¼Ÿ"
- è¾“å‡ºï¼š"é«˜å¯¼ç”µæ€§ LiFePO4"
- è¾“å…¥ï¼š"æ°´çƒ­åˆæˆæ³•åˆ¶å¤‡çš„ç£·é…¸é“é”‚ææ–™æ–‡çŒ®"
- è¾“å‡ºï¼š"æ°´çƒ­åˆæˆ ç£·é…¸é“é”‚"
- è¾“å…¥ï¼š"ç¢³åŒ…è¦†æ”¹æ€§çš„ç›¸å…³ç ”ç©¶"
- è¾“å‡ºï¼š"ç¢³åŒ…è¦† æ”¹æ€§"
"""
    
    def can_handle(self, question: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦é€‚åˆä½¿ç”¨è¯­ä¹‰æœç´¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            True=é€‚åˆè¯­ä¹‰æœç´¢, False=ä¸é€‚åˆ
        """
        question_lower = question.lower()
        
        # è¯­ä¹‰æœç´¢å…³é”®è¯
        semantic_keywords = [
            "æ–‡çŒ®", "è®ºæ–‡", "ç ”ç©¶", "æ–‡ç« ", "æŠ¥é“",
            "å…³äº", "ç›¸å…³", "æœ‰å“ªäº›", "å“ªäº›",
            "æœç´¢", "æŸ¥æ‰¾", "å¯»æ‰¾", "æ£€ç´¢",
            "ææ–™", "æ–¹æ³•", "åˆ¶å¤‡", "åˆæˆ",
            "æ”¹æ€§", "åŒ…è¦†", "æºæ‚", "ç»“æ„"
        ]
        
        # å¦‚æœé—®é¢˜åŒ…å«è¿™äº›è¯ï¼Œè®¤ä¸ºé€‚åˆè¯­ä¹‰æœç´¢
        return any(kw in question_lower for kw in semantic_keywords)
    
    def generate_search_query(self, question: str) -> str:
        """
        ç”Ÿæˆè¯­ä¹‰æœç´¢æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        if self._llm is None:
            # ä½¿ç”¨è§„åˆ™ç”Ÿæˆç®€å•æŸ¥è¯¢
            return self._generate_simple_query(question)
        
        try:
            from langchain_core.messages import HumanMessage, SystemMessage
            
            messages = [
                SystemMessage(content=self._search_prompt),
                HumanMessage(content=f"ç”¨æˆ·é—®é¢˜ï¼š{question}")
            ]
            
            response = self._llm.invoke(messages)
            query = response.content.strip()
            
            # å»é™¤å¯èƒ½çš„å¼•å·å’Œä»£ç å—æ ‡è®°
            query = query.strip('"\'')
            if "```" in query:
                query = query.split("```")[0].strip()
            
            return query
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
            return self._generate_simple_query(question)
    
    def _generate_simple_query(self, question: str) -> str:
        """
        ä½¿ç”¨è§„åˆ™ç”Ÿæˆç®€å•çš„æœç´¢æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = [
            "æœ‰å“ªäº›", "æœ‰æ²¡æœ‰", "è¯·æŸ¥æ‰¾", "æœç´¢", "æŸ¥æ‰¾",
            "å…³äº", "è¯·ç»™æˆ‘", "å¸®æˆ‘æ‰¾", "æˆ‘æƒ³æ‰¾"
        ]
        
        query = question
        for prefix in prefixes:
            if question.startswith(prefix):
                query = question[len(prefix):].strip()
                break
        
        # ç§»é™¤å¸¸è§åç¼€
        suffixes = ["çš„ç ”ç©¶", "çš„æ–‡çŒ®", "çš„æ–‡ç« ", "çš„ç›¸å…³", "çš„å†…å®¹", "ï¼Ÿ", "?"]
        for suffix in suffixes:
            if query.endswith(suffix):
                query = query[:-len(suffix)].strip()
                break
        
        return query if query else question
    
    def search(
        self, 
        question: str, 
        top_k: int = 15,  # ä»10å¢åŠ åˆ°15
        with_scores: bool = True,  # æ”¹ä¸ºé»˜è®¤Trueï¼Œéœ€è¦ç›¸ä¼¼åº¦åˆ†æ•°
        filter_metadata: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯­ä¹‰æœç´¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            with_scores: æ˜¯å¦è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
            filter_metadata: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æœç´¢ç»“æœ
        """
        # ç§»é™¤ can_handle æ£€æŸ¥ï¼Œå…è®¸æ‰€æœ‰é—®é¢˜è¿›è¡Œè¯­ä¹‰æœç´¢
        
        try:
            # ç”Ÿæˆæœç´¢æŸ¥è¯¢
            logger.info("\n" + "="*80)
            logger.info("ğŸ“ [æ­¥éª¤2] æå–å…³é”®è¯")
            search_query = self.generate_search_query(question)
            logger.info(f"å…³é”®è¯: {search_query}")
            logger.info("="*80)
            
            # ç”ŸæˆæŸ¥è¯¢çš„embeddingå‘é‡ï¼ˆä½¿ç”¨BGE APIï¼‰
            logger.info("\n" + "="*80)
            logger.info("ğŸ”¢ [æ­¥éª¤3] ç”ŸæˆæŸ¥è¯¢å‘é‡(Embedding)")
            logger.info(f"BGE APIåœ°å€: {self._bge_api_url}")
            logger.info(f"è¾“å…¥æ–‡æœ¬: {search_query}")
            try:
                response = requests.post(
                    self._bge_api_url,
                    json={"input": [search_query]},
                    timeout=30
                )
                response.raise_for_status()
                query_embedding = response.json()["data"][0]["embedding"]
                logger.info(f"âœ… æˆåŠŸç”Ÿæˆembedding")
                logger.info(f"å‘é‡ç»´åº¦: {len(query_embedding)}")
                logger.info(f"å‘é‡å‰5ç»´: {query_embedding[:5]}")
                logger.info("="*80)
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {e}")
                return {
                    "success": False,
                    "error": f"ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {str(e)}",
                    "error_step": "generate_embedding",
                    "expert": "semantic",
                    "documents": []
                }
            
            # æ‰§è¡Œæœç´¢
            logger.info("\n" + "="*80)
            logger.info("ğŸ” [æ­¥éª¤4] æŸ¥è¯¢å‘é‡æ•°æ®åº“")
            logger.info(f"æ£€ç´¢æ•°é‡: top_k={top_k}")
            results = self._vector_repo.search(
                query_embedding=query_embedding,
                n_results=top_k,
                where_filter=filter_metadata
            )
            
            if not results.get('success'):
                logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {results.get('error')}")
                return {
                    "success": False,
                    "error": results.get('error', 'æœç´¢å¤±è´¥'),
                    "error_step": "vector_search",
                    "expert": "semantic"
                }
            
            # æ ¼å¼åŒ–ç»“æœ
            documents = []
            docs = results.get('documents', [])
            metadatas = results.get('metadatas', [])
            distances = results.get('distances', [])
            ids = results.get('ids', [])
            
            for i, doc_content in enumerate(docs):
                doc_data = {
                    "id": ids[i] if i < len(ids) else str(i),
                    "content": doc_content,
                }
                if i < len(metadatas) and metadatas[i]:
                    doc_data["metadata"] = metadatas[i]
                if with_scores and i < len(distances):
                    # ChromaDB ä½¿ç”¨ cosine è·ç¦» (èŒƒå›´ 0-2)
                    # ä½™å¼¦ç›¸ä¼¼åº¦ = 1 - (cosine_distance / 2)
                    # è·ç¦»è¶Šå°,ç›¸ä¼¼åº¦è¶Šé«˜
                    distance = distances[i]
                    similarity = 1 - (distance / 2.0)  # è½¬æ¢ä¸º 0-1 èŒƒå›´çš„ç›¸ä¼¼åº¦
                    doc_data["score"] = max(0.0, min(1.0, similarity))  # ç¡®ä¿åœ¨ 0-1 èŒƒå›´å†…
                documents.append(doc_data)
            
            # åº”ç”¨ç›¸ä¼¼åº¦è¿‡æ»¤
            filtered_documents = self._filter_by_similarity(
                documents=documents,
                question=question,
                with_scores=with_scores
            )
            
            logger.info(f"âœ… æ£€ç´¢æˆåŠŸ")
            logger.info(f"åŸå§‹ç»“æœæ•°: {len(documents)}")
            logger.info(f"è¿‡æ»¤åç»“æœæ•°: {len(filtered_documents)}")
            logger.info("\nå‰3æ¡æ£€ç´¢ç»“æœé¢„è§ˆ:")
            for i, doc in enumerate(filtered_documents[:3], 1):
                score = doc.get('score', 0)
                content_preview = doc.get('content', '')[:100]
                doi = doc.get('metadata', {}).get('DOI', 'N/A')
                logger.info(f"  [{i}] ç›¸ä¼¼åº¦={score:.4f}, DOI={doi}")
                logger.info(f"      å†…å®¹: {content_preview}...")
            logger.info("="*80)
            
            return {
                "success": True,
                "expert": "semantic",
                "search_query": search_query,
                "result_count": len(filtered_documents),
                "original_count": len(documents),
                "documents": filtered_documents,
                "question": question
            }
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "error_step": "search",
                "expert": "semantic"
            }
    
    def search_with_expansion(
        self,
        question: str,
        top_k: int = 15,
        enable_expansion: bool = True,
        enable_reranking: bool = True
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºçš„æ£€ç´¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            enable_expansion: æ˜¯å¦å¯ç”¨æŸ¥è¯¢æ‰©å±•
            enable_reranking: æ˜¯å¦å¯ç”¨é‡æ’åº
            
        Returns:
            æ£€ç´¢ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - success: æ˜¯å¦æˆåŠŸ
            - expert: ä¸“å®¶ç±»å‹
            - documents: æ–‡æ¡£åˆ—è¡¨
            - expansion_info: æŸ¥è¯¢æ‰©å±•ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - retrieval_info: æ£€ç´¢ä¿¡æ¯
            - reranking_info: é‡æ’åºä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - timing: å„é˜¶æ®µè€—æ—¶
        """
        import time
        from backend.config.settings import settings
        
        timing = {}
        overall_start = time.time()
        
        # æ£€æŸ¥é…ç½®å’Œç»„ä»¶å¯ç”¨æ€§
        enable_expansion = enable_expansion and settings.enable_query_expansion and self._query_expander is not None
        enable_reranking = enable_reranking and settings.enable_reranking and self._sentence_reranker is not None
        
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ å¼€å§‹æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºæ£€ç´¢")
        logger.info(f"   é—®é¢˜: {question}")
        logger.info(f"   æŸ¥è¯¢æ‰©å±•: {'å¯ç”¨' if enable_expansion else 'ç¦ç”¨'}")
        logger.info(f"   é‡æ’åº: {'å¯ç”¨' if enable_reranking else 'ç¦ç”¨'}")
        logger.info("="*80)
        
        try:
            # ========== æ­¥éª¤1: æŸ¥è¯¢æ‰©å±• ==========
            expansion_info = {}
            queries = [question]  # é»˜è®¤åªä½¿ç”¨åŸå§‹æŸ¥è¯¢
            
            if enable_expansion:
                logger.info("\n" + "="*80)
                logger.info("ğŸ“ [æ­¥éª¤1] æŸ¥è¯¢æ‰©å±•")
                expansion_start = time.time()
                
                try:
                    expansion_result = self._query_expander.expand(question)
                    queries = expansion_result.all_queries
                    expansion_info = {
                        "original_query": expansion_result.original_query,
                        "english_query": expansion_result.english_query,
                        "synonym_query": expansion_result.synonym_query,
                        "all_queries": expansion_result.all_queries,
                        "translation_method": expansion_result.translation_method,
                        "expansion_time": expansion_result.expansion_time
                    }
                    timing["expansion"] = expansion_result.expansion_time
                    
                    logger.info(f"âœ… æŸ¥è¯¢æ‰©å±•æˆåŠŸ: {len(queries)} ä¸ªæŸ¥è¯¢")
                    for i, q in enumerate(queries, 1):
                        logger.info(f"   [{i}] {q}")
                    logger.info("="*80)
                    
                except Exception as e:
                    logger.error(f"âŒ æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
                    logger.warning("âš ï¸  å›é€€åˆ°å•æŸ¥è¯¢ç­–ç•¥")
                    queries = [question]
                    expansion_info = {"error": str(e), "fallback": True}
                    timing["expansion"] = time.time() - expansion_start
            else:
                logger.info("\næŸ¥è¯¢æ‰©å±•å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
            
            # ========== æ­¥éª¤2: å¤šæŸ¥è¯¢æ£€ç´¢ ==========
            logger.info("\n" + "="*80)
            logger.info("ğŸ” [æ­¥éª¤2] å¤šæŸ¥è¯¢æ£€ç´¢")
            retrieval_start = time.time()
            
            retrieval_info = {}
            documents = []
            
            try:
                if enable_expansion and self._multi_query_retriever and len(queries) > 1:
                    # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢å™¨
                    multi_result = self._multi_query_retriever.retrieve(
                        queries=queries,
                        top_k_per_query=20  # æ¯ä¸ªæŸ¥è¯¢è¿”å›20ä¸ªç»“æœ
                    )
                    
                    # è½¬æ¢æ–‡æ¡£æ ¼å¼
                    for doc in multi_result.documents:
                        documents.append({
                            "id": doc.get("id"),
                            "content": doc.get("text"),
                            "metadata": doc.get("metadata"),
                            "score": doc.get("score"),
                            "source_query": doc.get("source_query")
                        })
                    
                    retrieval_info = {
                        "query_count": len(queries),
                        "query_contributions": multi_result.query_contributions,
                        "total_before_dedup": multi_result.total_before_dedup,
                        "total_after_dedup": multi_result.total_after_dedup,
                        "retrieval_time": multi_result.retrieval_time
                    }
                    timing["retrieval"] = multi_result.retrieval_time
                    
                    logger.info(f"âœ… å¤šæŸ¥è¯¢æ£€ç´¢æˆåŠŸ")
                    logger.info(f"   å»é‡å‰: {multi_result.total_before_dedup} ä¸ªæ–‡æ¡£")
                    logger.info(f"   å»é‡å: {multi_result.total_after_dedup} ä¸ªæ–‡æ¡£")
                    logger.info("="*80)
                    
                else:
                    # å›é€€åˆ°å•æŸ¥è¯¢
                    logger.info("ä½¿ç”¨å•æŸ¥è¯¢æ£€ç´¢")
                    search_result = self.search(
                        question=queries[0],
                        top_k=20,
                        with_scores=True
                    )
                    
                    if search_result.get("success"):
                        documents = search_result.get("documents", [])
                        retrieval_info = {
                            "query_count": 1,
                            "result_count": len(documents),
                            "retrieval_time": time.time() - retrieval_start
                        }
                        timing["retrieval"] = retrieval_info["retrieval_time"]
                        
                        logger.info(f"âœ… å•æŸ¥è¯¢æ£€ç´¢æˆåŠŸ: {len(documents)} ä¸ªæ–‡æ¡£")
                        logger.info("="*80)
                    else:
                        raise Exception(search_result.get("error", "æ£€ç´¢å¤±è´¥"))
                        
            except Exception as e:
                logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
                timing["retrieval"] = time.time() - retrieval_start
                return {
                    "success": False,
                    "error": f"æ£€ç´¢å¤±è´¥: {str(e)}",
                    "expert": "semantic",
                    "expansion_info": expansion_info,
                    "timing": timing
                }
            
            # ========== æ­¥éª¤3: å¥å­çº§é‡æ’åº ==========
            reranking_info = {}
            
            if enable_reranking and documents:
                logger.info("\n" + "="*80)
                logger.info("ğŸ”„ [æ­¥éª¤3] å¥å­çº§é‡æ’åº")
                reranking_start = time.time()
                
                try:
                    # é™åˆ¶å€™é€‰æ•°é‡
                    candidates_to_rerank = documents[:settings.rerank_top_k]
                    logger.info(f"   å€™é€‰æ•°é‡: {len(candidates_to_rerank)}")
                    
                    rerank_result = self._sentence_reranker.rerank(
                        query=question,  # ä½¿ç”¨åŸå§‹é—®é¢˜
                        candidates=candidates_to_rerank,
                        top_k=top_k
                    )
                    
                    documents = rerank_result.documents
                    reranking_info = {
                        "candidates_count": len(candidates_to_rerank),
                        "similarity_scores": rerank_result.similarity_scores,
                        "top_3_changes": rerank_result.top_3_changes,
                        "reranking_time": rerank_result.reranking_time
                    }
                    timing["reranking"] = rerank_result.reranking_time
                    
                    logger.info(f"âœ… é‡æ’åºæˆåŠŸ")
                    logger.info(f"   è¿”å›æ•°é‡: {len(documents)}")
                    logger.info(f"   Top-3å˜åŒ–: {rerank_result.top_3_changes}")
                    logger.info("="*80)
                    
                except Exception as e:
                    logger.error(f"âŒ é‡æ’åºå¤±è´¥: {e}")
                    logger.warning("âš ï¸  ä½¿ç”¨åŸå§‹æ’åº")
                    documents = documents[:top_k]
                    reranking_info = {"error": str(e), "fallback": True}
                    timing["reranking"] = time.time() - reranking_start
            else:
                # ä¸ä½¿ç”¨é‡æ’åºï¼Œç›´æ¥æˆªå–top_k
                documents = documents[:top_k]
                if not enable_reranking:
                    logger.info("\né‡æ’åºå·²ç¦ç”¨")
            
            # ========== è¿”å›ç»“æœ ==========
            timing["total"] = time.time() - overall_start
            
            logger.info("\n" + "="*80)
            logger.info("âœ… æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºæ£€ç´¢å®Œæˆ")
            logger.info(f"   æ€»è€—æ—¶: {timing['total']:.2f}s")
            logger.info(f"   - æŸ¥è¯¢æ‰©å±•: {timing.get('expansion', 0):.2f}s")
            logger.info(f"   - æ£€ç´¢: {timing.get('retrieval', 0):.2f}s")
            logger.info(f"   - é‡æ’åº: {timing.get('reranking', 0):.2f}s")
            logger.info(f"   æœ€ç»ˆè¿”å›: {len(documents)} ä¸ªæ–‡æ¡£")
            logger.info("="*80)
            
            return {
                "success": True,
                "expert": "semantic",
                "question": question,
                "result_count": len(documents),
                "documents": documents,
                "expansion_info": expansion_info,
                "retrieval_info": retrieval_info,
                "reranking_info": reranking_info,
                "timing": timing
            }
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºæ£€ç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # å›é€€åˆ°åŸæœ‰çš„searchæ–¹æ³•
            logger.warning("âš ï¸  å›é€€åˆ°åŸæœ‰çš„å•æŸ¥è¯¢ç­–ç•¥")
            try:
                fallback_result = self.search(question, top_k=top_k, with_scores=True)
                fallback_result["fallback"] = True
                fallback_result["fallback_reason"] = str(e)
                fallback_result["timing"] = {"total": time.time() - overall_start}
                return fallback_result
            except Exception as fallback_error:
                logger.error(f"âŒ å›é€€ç­–ç•¥ä¹Ÿå¤±è´¥: {fallback_error}")
                return {
                    "success": False,
                    "error": f"æ£€ç´¢å¤±è´¥: {str(e)}, å›é€€ä¹Ÿå¤±è´¥: {str(fallback_error)}",
                    "expert": "semantic",
                    "timing": {"total": time.time() - overall_start}
                }
    
    def search_by_material(self, material: str, top_k: int = 5) -> Dict[str, Any]:
        """
        æŒ‰ææ–™åç§°æœç´¢æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            material: ææ–™åç§°
            top_k: ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        return self.search(f"å…³äº{material}çš„æ–‡çŒ®", top_k=top_k)
    
    def search_by_method(self, method: str, top_k: int = 5) -> Dict[str, Any]:
        """
        æŒ‰åˆæˆæ–¹æ³•æœç´¢æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            method: åˆæˆæ–¹æ³•åç§°
            top_k: ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        return self.search(f"{method}åˆ¶å¤‡çš„ææ–™æ–‡çŒ®", top_k=top_k)
    
    def search_by_modification(
        self, 
        modification: str, 
        material: str = "LiFePO4",
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        æŒ‰æ”¹æ€§ç­–ç•¥æœç´¢æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            modification: æ”¹æ€§ç­–ç•¥
            material: ææ–™åç§°
            top_k: ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        return self.search(f"{modification}æ”¹æ€§çš„{material}", top_k=top_k)
    
    def find_similar(
        self, 
        document_text: str, 
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼çš„æ–‡çŒ®ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            document_text: æ–‡æ¡£å†…å®¹
            top_k: ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼æ–‡çŒ®åˆ—è¡¨
        """
        try:
            results = self._vector_repo.find_similar(
                document_text=document_text,
                top_k=top_k
            )
            
            documents = []
            for doc in results:
                doc_data = {
                    "content": doc.page_content if hasattr(doc, 'page_content') else str(doc),
                }
                if hasattr(doc, 'metadata') and doc.metadata:
                    doc_data["metadata"] = doc.metadata
                documents.append(doc_data)
            
            return {
                "success": True,
                "expert": "semantic",
                "result_count": len(documents),
                "documents": documents
            }
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼æ–‡çŒ®å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "expert": "semantic"
            }
    
    def aggregate_results(
        self, 
        search_results: Dict[str, Any],
        llm_enhanced: bool = True
    ) -> Dict[str, Any]:
        """
        èšåˆæœç´¢ç»“æœï¼ˆå¯é€‰ï¼šä½¿ç”¨LLMå¢å¼ºï¼‰
        
        Args:
            search_results: æœç´¢ç»“æœ
            llm_enhanced: æ˜¯å¦ä½¿ç”¨LLMå¢å¼º
            
        Returns:
            èšåˆåçš„ç»“æœ
        """
        if not search_results.get("success"):
            return search_results
        
        documents = search_results.get("documents", [])
        
        # æå–å…³é”®ä¿¡æ¯
        summary = {
            "total_found": search_results["result_count"],
            "expert": "semantic",
            "search_query": search_results.get("search_query", ""),
            "key_topics": [],
            "material_mentioned": [],
            "methods_mentioned": []
        }
        
        # ç®€å•æå–å…³é”®è¯
        for doc in documents[:10]:  # åªåˆ†æå‰10ä¸ª
            content = doc.get("content", "")
            metadata = doc.get("metadata", {})
            
            # ä»æ‘˜è¦ä¸­æå–ææ–™å’Œæ–¹æ³•
            if "LiFePO4" in content or "LFP" in content:
                if "LiFePO4" not in summary["material_mentioned"]:
                    summary["material_mentioned"].append("LiFePO4")
            
            if "NMC" in content or "NCM" in content:
                if "NMC" not in summary["material_mentioned"]:
                    summary["material_mentioned"].append("NMC")
            
            # å¸¸è§æ–¹æ³•
            methods = ["æ°´çƒ­", "æº¶èƒ¶å‡èƒ¶", "çƒç£¨", "å…±æ²‰æ·€", "å–·é›¾å¹²ç‡¥"]
            for method in methods:
                if method in content and method not in summary["methods_mentioned"]:
                    summary["methods_mentioned"].append(method)
        
            return {
                "success": True,
                "expert": "semantic",
                "summary": summary,
                "documents": documents,
                "llm_enhanced": llm_enhanced
            }
    
    def _is_broad_question(self, question: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå®½æ³›é—®é¢˜"""
        broad_keywords = [
            "æœ‰å“ªäº›", "å“ªäº›", "ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ",
            "ç»¼è¿°", "æ¦‚è¿°", "æ€»ç»“", "ä»‹ç»", "å‘å±•",
            "ç ”ç©¶è¿›å±•", "ç ”ç©¶ç°çŠ¶", "åº”ç”¨", "å‰æ™¯"
        ]
        return any(kw in question for kw in broad_keywords)
    
    def _filter_by_similarity(
        self,
        documents: List[Dict],
        question: str,
        with_scores: bool = True
    ) -> List[Dict]:
        """æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœ"""
        if not with_scores or not documents:
            return documents
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹ï¼Œé€‰æ‹©é˜ˆå€¼
        is_broad = self._is_broad_question(question)
        threshold = self._broad_threshold if is_broad else self._precise_threshold
        
        filtered = []
        filtered_count = 0
        
        for doc in documents:
            score = doc.get('score', 1.0)
            if score >= threshold:
                filtered.append(doc)
            else:
                filtered_count += 1
        
        logger.info(
            f"ç›¸ä¼¼åº¦è¿‡æ»¤: é˜ˆå€¼={threshold:.2f} ({'å®½æ³›' if is_broad else 'ç²¾ç¡®'}é—®é¢˜), "
            f"ä¿ç•™={len(filtered)}, è¿‡æ»¤={filtered_count}"
        )
        
        return filtered
    
    def _extract_dois(self, documents: List[Dict]) -> List[str]:
        """ä»æ–‡æ¡£ä¸­æå–DOI"""
        dois = []
        for doc in documents:
            metadata = doc.get('metadata', {})
            # ä»metadataä¸­æå–DOI
            doi = metadata.get('doi') or metadata.get('DOI')
            if doi:
                dois.append(doi)
            else:
                # ä»å†…å®¹ä¸­æå–DOI
                content = doc.get('content', '')
                # ä¿®æ­£æ­£åˆ™: æ’é™¤æ–¹æ‹¬å·å’Œå…¶ä»–ç¬¦å·
                doi_match = re.search(r'10\.\d+/[^\s)\]\>]+', content)
                if doi_match:
                    dois.append(doi_match.group())
        return dois
    
    def _load_pdf_contents(
        self,
        dois: List[str],
        max_pages: int = 30,
        max_chars: int = 20000
    ) -> Dict[str, str]:
        """åŠ è½½å¤šä¸ªDOIçš„PDFå†…å®¹"""
        if not self._pdf_manager:
            return {}
        
        pdf_contents = {}
        for doi in dois[:3]:  # æœ€å¤šåŠ è½½3ç¯‡
            content = self._pdf_manager.load_pdf_by_doi(
                doi=doi,
                max_pages=max_pages,
                max_chars=max_chars
            )
            if content:
                pdf_contents[doi] = content
        
        return pdf_contents
    
    def query_with_details(
        self,
        question: str,
        top_k: int = 20,
        load_pdf: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬PDFåŠ è½½æƒ…å†µå’Œä½ç½®ä¿¡æ¯ï¼‰
        
        æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ä½¿ç”¨æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºï¼Œæˆ–ä½¿ç”¨åŸæœ‰çš„å•æŸ¥è¯¢ç­–ç•¥ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            load_pdf: æ˜¯å¦åŠ è½½PDFåŸæ–‡
            
        Returns:
            åŒ…å«answerã€pdf_infoã€doi_locationsçš„å­—å…¸
        """
        from backend.config.settings import settings
        
        # æ ¹æ®é…ç½®é€‰æ‹©æ£€ç´¢ç­–ç•¥
        use_expansion = settings.enable_query_expansion or settings.enable_reranking
        
        if use_expansion:
            # ä½¿ç”¨æ–°çš„æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºç­–ç•¥
            logger.info("ğŸš€ ä½¿ç”¨æŸ¥è¯¢æ‰©å±•å’Œé‡æ’åºç­–ç•¥")
            search_result = self.search_with_expansion(
                question=question,
                top_k=top_k,
                enable_expansion=settings.enable_query_expansion,
                enable_reranking=settings.enable_reranking
            )
        else:
            # ä½¿ç”¨åŸæœ‰çš„å•æŸ¥è¯¢ç­–ç•¥ï¼ˆå‘åå…¼å®¹ï¼‰
            logger.info("ğŸ“š ä½¿ç”¨åŸæœ‰çš„å•æŸ¥è¯¢ç­–ç•¥")
            search_result = self.search(question, top_k=top_k, with_scores=True)
        
        # å¤„ç†æ£€ç´¢å¤±è´¥
        if not search_result.get('success'):
            return {
                'answer': 'æ£€ç´¢å¤±è´¥',
                'pdf_info': {'error': search_result.get('error')},
                'doi_locations': {}
            }
        
        documents = search_result.get('documents', [])
        if not documents:
            return {
                'answer': 'æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚',
                'pdf_info': {'documents_found': 0},
                'doi_locations': {}
            }
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹
        is_broad = self._is_broad_question(question)
        
        # åˆå§‹åŒ–PDFä¿¡æ¯
        pdf_info = {
            'documents_found': len(documents),
            'is_broad_question': is_broad,
            'dois_found': 0,
            'pdf_loaded': 0,
            'pdf_failed': 0,
            'used_expansion': use_expansion,  # è®°å½•æ˜¯å¦ä½¿ç”¨äº†æŸ¥è¯¢æ‰©å±•
            'expansion_info': search_result.get('expansion_info', {}),  # æŸ¥è¯¢æ‰©å±•ä¿¡æ¯
            'retrieval_info': search_result.get('retrieval_info', {}),  # æ£€ç´¢ä¿¡æ¯
            'reranking_info': search_result.get('reranking_info', {}),  # é‡æ’åºä¿¡æ¯
            'timing': search_result.get('timing', {})  # è€—æ—¶ä¿¡æ¯
        }
        
        # å®½æ³›é—®é¢˜ï¼šä¸åŠ è½½PDF
        if is_broad:
            logger.info("æ£€æµ‹åˆ°å®½æ³›é—®é¢˜ï¼Œä½¿ç”¨å®½æ³›é—®é¢˜åˆæˆæ¨¡æ¿ï¼ˆä¸åŠ è½½PDFï¼‰")
            answer, doi_locations = self._synthesize_broad_answer(question, documents)
            return {
                'answer': answer,
                'pdf_info': pdf_info,
                'doi_locations': doi_locations
            }
        
        # ç²¾ç¡®é—®é¢˜ï¼šåŠ è½½PDF
        pdf_contents = {}
        if load_pdf and self._pdf_manager:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“„ [æ­¥éª¤5] åŠ è½½PDFåŸæ–‡")
            dois = self._extract_dois(documents)
            pdf_info['dois_found'] = len(dois)
            logger.info(f"æå–åˆ° {len(dois)} ä¸ªDOI")
            
            if dois:
                pdf_contents = self._load_pdf_contents(dois)
                pdf_info['pdf_loaded'] = len(pdf_contents)
                pdf_info['pdf_failed'] = len(dois) - len(pdf_contents)
                logger.info(f"\næ­£åœ¨åŠ è½½PDFåŸæ–‡ (æœ€å¤š3ç¯‡):")
                for idx, (doi, content) in enumerate(pdf_contents.items(), 1):
                    progress = f"[{idx}/{len(pdf_contents)}]"
                    size_kb = len(content) / 1024
                    logger.info(f"  {progress} âœ… {doi} ({size_kb:.1f}KB)")
                if pdf_info['pdf_failed'] > 0:
                    logger.info(f"  âš ï¸  {pdf_info['pdf_failed']} ç¯‡PDFåŠ è½½å¤±è´¥")
            else:
                logger.info("âš ï¸  æœªæå–åˆ°DOI")
            logger.info("="*80)
        
        answer, doi_locations = self._synthesize_semantic_answer(question, documents, pdf_contents)
        return {
            'answer': answer,
            'pdf_info': pdf_info,
            'doi_locations': doi_locations
        }
    
    def _synthesize_semantic_answer(
        self,
        user_question: str,
        documents: List[Dict],
        pdf_contents: Optional[Dict[str, str]] = None
    ) -> tuple:
        """åˆæˆè¯­ä¹‰æœç´¢ç­”æ¡ˆï¼ˆç²¾ç¡®é—®é¢˜ï¼‰"""
        from backend.config.settings import settings
        
        # æ ¹æ®é…ç½®é€‰æ‹©prompt
        use_robust = getattr(settings, 'use_robust_prompt', True)
        
        if not self._llm:
            return self._format_simple_answer(documents), {}
        
        # é€‰æ‹©promptæ¨¡æ¿
        if use_robust and self._semantic_synthesis_prompt_robust:
            logger.info("ğŸ“ ä½¿ç”¨å¢å¼ºå‹Promptï¼ˆé€‚åˆç¢ç‰‡åŒ–æ–‡æœ¬ï¼‰")
            return self._synthesize_with_robust_prompt(user_question, documents, pdf_contents)
        elif self._semantic_synthesis_prompt:
            logger.info("ğŸ“ ä½¿ç”¨æ ‡å‡†Prompt")
            return self._synthesize_with_standard_prompt(user_question, documents, pdf_contents)
        else:
            return self._format_simple_answer(documents), {}
    
    def _synthesize_with_robust_prompt(
        self,
        user_question: str,
        documents: List[Dict],
        pdf_contents: Optional[Dict[str, str]] = None
    ) -> tuple:
        """ä½¿ç”¨å¢å¼ºå‹promptåˆæˆç­”æ¡ˆï¼ˆé€‚åˆç¢ç‰‡åŒ–æ–‡æœ¬ï¼‰"""
        try:
            # æ„å»ºç®€åŒ–çš„æ–‡çŒ®åˆ—è¡¨ï¼ˆä¸éœ€è¦ä¸Šä¸‹æ–‡æ‰©å±•ï¼‰
            logger.info("\n" + "="*80)
            logger.info("ğŸ“– [æ­¥éª¤5] å‡†å¤‡æ–‡çŒ®å†…å®¹")
            
            literature_list = []
            for i, doc in enumerate(documents[:15], 1):  # ä½¿ç”¨å‰15ä¸ªæ–‡æ¡£
                content = doc.get('content', '')
                metadata = doc.get('metadata', {})
                doi = metadata.get('DOI') or metadata.get('doi', f'æ–‡çŒ®{i}')
                
                lit_text = f"[{i}] DOI: {doi}\nå†…å®¹: {content}\n"
                literature_list.append(lit_text)
            
            context = "\n".join(literature_list)
            logger.info(f"âœ… å‡†å¤‡å®Œæˆï¼Œå…± {len(literature_list)} ä¸ªæ–‡çŒ®ç‰‡æ®µ")
            logger.info("="*80)
            
            # ä½¿ç”¨å¢å¼ºå‹promptæ¨¡æ¿
            prompt = self._semantic_synthesis_prompt_robust.replace("{question}", user_question)
            prompt = prompt.replace("{context}", context)
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ“‹ [æ­¥éª¤6] æ„å»ºå¢å¼ºå‹Prompt")
            logger.info(f"æ–‡çŒ®ç‰‡æ®µ: {len(literature_list)} ä¸ª")
            logger.info(f"Promptæ€»é•¿åº¦: {len(prompt):,} å­—ç¬¦")
            logger.info("="*80)
            
            from langchain_core.messages import HumanMessage
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ¤– [æ­¥éª¤7] ç”Ÿæˆå›ç­”")
            response = self._llm.invoke([HumanMessage(content=prompt)])
            pure_answer = response.content.strip()
            logger.info(f"âœ… LLMç”Ÿæˆçº¯å‡€ç­”æ¡ˆå®Œæˆ ({len(pure_answer)} å­—ç¬¦)")
            logger.info("="*80)
            
            # ä½¿ç”¨embeddingçš„DOIæ’å…¥æ–¹æ³•
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Œ [æ­¥éª¤8] åŸºäºEmbeddingçš„DOIæ’å…¥")
            answer_with_doi, doi_locations = self._insert_dois_by_embedding(
                answer=pure_answer,
                documents=documents,
                pdf_contents=pdf_contents
            )
            logger.info("="*80)
            
            return answer_with_doi, doi_locations
            
        except Exception as e:
            logger.error(f"å¢å¼ºå‹ç­”æ¡ˆåˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._format_simple_answer(documents), {}
    
    def _synthesize_with_standard_prompt(
        self,
        user_question: str,
        documents: List[Dict],
        pdf_contents: Optional[Dict[str, str]] = None
    ) -> tuple:
        """ä½¿ç”¨æ ‡å‡†promptåˆæˆç­”æ¡ˆï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        if not self._llm or not self._semantic_synthesis_prompt:
            return self._format_simple_answer(documents), {}
        
        try:
            # æ„å»ºæ–‡çŒ®åˆ—è¡¨ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡æ‰©å±•ï¼‰
            # ç²¾ç¡®é—®é¢˜ï¼šä½¿ç”¨10ç¯‡æ–‡çŒ®ï¼ˆä»20å‡å°‘åˆ°10ï¼Œé¿å…promptè¿‡é•¿ï¼‰
            logger.info("\n" + "="*80)
            logger.info("ğŸ“– [æ­¥éª¤5.5] æ‰©å±•ä¸Šä¸‹æ–‡çª—å£")
            logger.info(f"åŸå§‹æ®µè½æ•°: {len(documents)}")
            
            literature_list = []
            num_abstracts = 10  # ä½¿ç”¨10ç¯‡æ–‡çŒ®
            for i, doc in enumerate(documents[:num_abstracts], 1):
                chunk_id = doc.get('id')
                
                # è·å–å¸¦ä¸Šä¸‹æ–‡çš„å®Œæ•´å†…å®¹
                context_result = self._vector_repo.get_chunk_with_context(
                    chunk_id=chunk_id,
                    window=2  # å‰åå„2ä¸ªæ®µè½
                )
                
                if context_result.get('success'):
                    full_text = context_result['full_text']
                    context_range = context_result['context_range']
                    main_meta = context_result['metadata']
                    
                    logger.info(f"  [{i}] æ‰©å±•æˆåŠŸ: {context_result['context_chunks']}ä¸ªæ®µè½")
                    logger.info(f"      èŒƒå›´: ç¬¬{context_range['start_page']}-{context_range['end_page']}é¡µ")
                    logger.info(f"      é•¿åº¦: {len(full_text)} å­—ç¬¦")
                    
                    lit = {
                        "åºå·": i,
                        "å†…å®¹": full_text,  # ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œä¸æˆªæ–­
                        "æ ¸å¿ƒæ®µè½": context_result['main_text'][:200] + "...",  # æ ‡æ³¨æ ¸å¿ƒæ®µè½
                        "ä¸Šä¸‹æ–‡ä¿¡æ¯": f"ç¬¬{main_meta.get('page')}é¡µç¬¬{main_meta.get('chunk_index_in_page', 0)+1}æ®µï¼ˆå«å‰åå„2æ®µï¼‰"
                    }
                else:
                    # å¦‚æœè·å–ä¸Šä¸‹æ–‡å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
                    logger.warning(f"  [{i}] æ‰©å±•å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ®µè½")
                    lit = {
                        "åºå·": i,
                        "å†…å®¹": doc.get('content', ''),
                        "æ ¸å¿ƒæ®µè½": doc.get('content', '')[:200] + "...",
                        "ä¸Šä¸‹æ–‡ä¿¡æ¯": "ä»…æ ¸å¿ƒæ®µè½"
                    }
                
                if doc.get('metadata'):
                    lit["å…ƒæ•°æ®"] = doc['metadata']
                literature_list.append(lit)
            
            logger.info(f"âœ… ä¸Šä¸‹æ–‡æ‰©å±•å®Œæˆï¼Œå…± {len(literature_list)} ç¯‡æ–‡çŒ®æ‘˜è¦")
            logger.info("="*80)
            
            literature_json = json.dumps(literature_list, ensure_ascii=False, indent=2)
            
            # æ·»åŠ PDFåŸæ–‡ï¼ˆé™åˆ¶é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿ï¼‰
            pdf_section = ""
            if pdf_contents:
                pdf_section = "\n\n## ğŸ“„ ç›¸å…³è®ºæ–‡åŸæ–‡\n"
                for doi, content in pdf_contents.items():
                    # é™åˆ¶æ¯ç¯‡PDFæœ€å¤š10000å­—ç¬¦
                    truncated_content = content[:10000]
                    if len(content) > 10000:
                        truncated_content += "\n... (å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"
                    pdf_section += f"\n### DOI: {doi}\n{truncated_content}\n"
                    logger.info(f"  æ·»åŠ PDFå…¨æ–‡: {doi} ({len(truncated_content)} å­—ç¬¦)")
            
            # ä¿®æ”¹Promptï¼Œç¦æ­¢LLMç¼–é€ æ–‡çŒ®å¼•ç”¨
            prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ–‡çŒ®ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡çŒ®ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

**é‡è¦è§„åˆ™**ï¼š
1. åŸºäºæ£€ç´¢åˆ°çš„æ–‡çŒ®ä¸­çš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜
2. **ç»å¯¹ç¦æ­¢**æåŠå…·ä½“çš„"æ–‡çŒ®X"ã€"è¡¨X"ã€"å›¾X"ç­‰å¼•ç”¨æ ‡æ³¨
3. **ç»å¯¹ç¦æ­¢**ç¼–é€ ä»»ä½•æ–‡çŒ®ä¸­æ²¡æœ‰çš„ä¿¡æ¯
4. ç›´æ¥é™ˆè¿°äº‹å®ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¼•ç”¨æ ‡æ³¨ï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ DOIå¼•ç”¨ï¼‰
5. å¦‚æœæ–‡çŒ®ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜"æ£€ç´¢åˆ°çš„æ–‡çŒ®ä¸­æœªæåŠ"
6. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šã€å‡†ç¡®ã€ç®€æ´

**æ£€ç´¢åˆ°çš„æ–‡çŒ®**ï¼š
{literature_results}

{pdf_contents}

**ç”¨æˆ·é—®é¢˜**ï¼š{user_question}

è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦æåŠæ–‡çŒ®ç¼–å·æˆ–å¼•ç”¨æ ‡æ³¨ã€‚
"""
            
            prompt = prompt_template.replace("{user_question}", user_question)
            prompt = prompt.replace("{literature_results}", literature_json)
            prompt = prompt.replace("{pdf_contents}", pdf_section if pdf_section else "")
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ“‹ [æ­¥éª¤6] æ„å»ºPrompt")
            logger.info(f"æ–‡çŒ®æ‘˜è¦: {len(literature_list)} ç¯‡")
            logger.info(f"PDFåŸæ–‡: {len(pdf_contents) if pdf_contents else 0} ç¯‡ï¼ˆå®Œæ•´å†…å®¹ï¼Œä¸æˆªæ–­ï¼‰")
            if pdf_contents:
                total_pdf_chars = sum(len(content) for content in pdf_contents.values())
                logger.info(f"PDFæ€»å­—ç¬¦æ•°: {total_pdf_chars:,} å­—ç¬¦")
            logger.info(f"Promptæ€»é•¿åº¦: {len(prompt):,} å­—ç¬¦ (~{len(prompt)//4:,} tokens)")
            logger.info("="*80)
            
            from langchain_core.messages import HumanMessage
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ¤– [æ­¥éª¤7] ç”Ÿæˆå›ç­”")
            response = self._llm.invoke([HumanMessage(content=prompt)])
            pure_answer = response.content.strip()
            logger.info(f"âœ… LLMç”Ÿæˆçº¯å‡€ç­”æ¡ˆå®Œæˆ ({len(pure_answer)} å­—ç¬¦)")
            logger.info("="*80)
            
            # ä½¿ç”¨æ–°çš„åŸºäºembeddingçš„DOIæ’å…¥æ–¹æ³•
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Œ [æ­¥éª¤8] åŸºäºEmbeddingçš„DOIæ’å…¥")
            answer_with_doi, doi_locations = self._insert_dois_by_embedding(
                answer=pure_answer,
                documents=documents,
                pdf_contents=pdf_contents
            )
            logger.info("="*80)
            
            return answer_with_doi, doi_locations
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰ç­”æ¡ˆåˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._format_simple_answer(documents), {}
    
    def _insert_dois_by_embedding(
        self,
        answer: str,
        documents: List[Dict],
        pdf_contents: Optional[Dict[str, str]] = None
    ) -> tuple:
        """
        åŸºäºäºŒçº§æ£€ç´¢çš„DOIæ’å…¥æ–¹æ³•
        
        å·¥ä½œæµç¨‹ï¼š
        ã€ä¸€çº§æ£€ç´¢ã€‘æ®µè½çº§æ•°æ®åº“
        1. ä»æ£€ç´¢ç»“æœæå–å€™é€‰DOIæ± 
        
        ã€äºŒçº§æ£€ç´¢ã€‘å¥å­çº§æ•°æ®åº“
        2. æ‹†åˆ†ç­”æ¡ˆä¸ºå¥å­
        3. æ‰¹é‡ç”Ÿæˆå¥å­çš„embedding
        4. åœ¨å¥å­æ•°æ®åº“ä¸­æœç´¢ï¼ˆåªåŒ¹é…å€™é€‰DOIï¼‰
        5. ç›¸ä¼¼åº¦>0.6æ—¶æ’å…¥DOI
        
        ä¼˜å…ˆä½¿ç”¨å¥å­çº§æ•°æ®åº“ï¼Œå¤±è´¥æ—¶å›é€€åˆ°PDFæœç´¢
        
        Args:
            answer: LLMç”Ÿæˆçš„çº¯å‡€ç­”æ¡ˆ
            documents: æ£€ç´¢åˆ°çš„æ–‡çŒ®åˆ—è¡¨ï¼ˆä¸€çº§æ£€ç´¢ç»“æœï¼‰
            pdf_contents: PDFå…¨æ–‡å†…å®¹ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
            
        Returns:
            (answer_with_doi, doi_locations)
        """
        if not answer:
            return answer, {}
        
        # å¦‚æœæœ‰å¥å­æ•°æ®åº“ï¼Œä½¿ç”¨äºŒçº§æ£€ç´¢
        if self._sentence_collection:
            logger.info("\n" + "="*80)
            logger.info("ğŸ¯ ä½¿ç”¨äºŒçº§æ£€ç´¢æ¨¡å¼ï¼ˆå¥å­çº§æ•°æ®åº“ï¼‰")
            logger.info("="*80)
            return self._insert_dois_by_sentence_db(answer, documents)
        
        # å¦åˆ™å›é€€åˆ°PDFæœç´¢
        else:
            logger.info("\n" + "="*80)
            logger.info("âš ï¸  å›é€€åˆ°PDFæœç´¢æ¨¡å¼")
            logger.info("="*80)
            return self._insert_dois_by_pdf_search(answer, documents, pdf_contents)
        
        # 1. æ‹†åˆ†ç­”æ¡ˆä¸ºå¥å­
        sentences = self._split_sentences_for_doi(answer)
        logger.info(f"æ‹†åˆ†ä¸º {len(sentences)} ä¸ªå¥å­")
        
        if not sentences:
            return answer, {}
        
        # 2. æ‰¹é‡ç”Ÿæˆembedding
        try:
            logger.info(f"æ­£åœ¨ä¸º {len(sentences)} ä¸ªå¥å­ç”Ÿæˆembedding...")
            response = requests.post(
                self._bge_api_url,
                json={"input": sentences},
                timeout=60
            )
            response.raise_for_status()
            embeddings = [item["embedding"] for item in response.json()["data"]]
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªembedding")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {e}")
            return answer, {}
        
        # 3. å‡†å¤‡æœç´¢èŒƒå›´ï¼šæ£€ç´¢åˆ°çš„æ–‡çŒ®ID + PDFæ®µè½
        doc_ids = [doc.get('id') for doc in documents if doc.get('id')]
        logger.info(f"æ£€ç´¢åˆ°çš„æ–‡çŒ®: {len(doc_ids)} ç¯‡")
        
        # 3.1 å°†PDFå†…å®¹åˆ†æ®µå¹¶ç”Ÿæˆembeddingï¼ˆç”¨äºæœç´¢ï¼‰
        pdf_chunks = []  # [(doi, chunk_text, chunk_embedding, chunk_index)]
        if pdf_contents:
            logger.info(f"æ­£åœ¨å¤„ç† {len(pdf_contents)} ç¯‡PDFå…¨æ–‡...")
            all_chunks_text = []
            chunk_metadata = []  # [(doi, chunk_index)]
            
            for doi, content in pdf_contents.items():
                # å°†PDFæŒ‰æ®µè½åˆ†å‰²ï¼ˆæ¯1000å­—ç¬¦ä¸€æ®µï¼‰
                chunk_size = 1000
                for i in range(0, len(content), chunk_size):
                    chunk = content[i:i+chunk_size]
                    if len(chunk) > 100:  # è·³è¿‡å¤ªçŸ­çš„æ®µè½
                        all_chunks_text.append(chunk)
                        chunk_metadata.append((doi, i // chunk_size))
            
            # æ‰¹é‡ç”ŸæˆPDFæ®µè½çš„embedding
            if all_chunks_text:
                try:
                    logger.info(f"æ­£åœ¨ä¸º {len(all_chunks_text)} ä¸ªPDFæ®µè½ç”Ÿæˆembedding...")
                    chunk_response = requests.post(
                        self._bge_api_url,
                        json={"input": all_chunks_text},
                        timeout=120
                    )
                    chunk_response.raise_for_status()
                    chunk_embeddings = [item["embedding"] for item in chunk_response.json()["data"]]
                    
                    # ç»„è£…pdf_chunks
                    for i, (chunk_text, (doi, chunk_idx)) in enumerate(zip(all_chunks_text, chunk_metadata)):
                        pdf_chunks.append((doi, chunk_text, chunk_embeddings[i], chunk_idx))
                    
                    logger.info(f"âœ… PDFæ®µè½embeddingç”Ÿæˆå®Œæˆ: {len(pdf_chunks)} ä¸ªæ®µè½")
                except Exception as e:
                    logger.error(f"âŒ PDFæ®µè½embeddingç”Ÿæˆå¤±è´¥: {e}")
                    pdf_chunks = []
        
        if not doc_ids and not pdf_chunks:
            logger.warning("âš ï¸  æ²¡æœ‰å¯æœç´¢çš„å†…å®¹ï¼Œæ— æ³•æ’å…¥DOI")
            return answer, {}
        
        # 4. ä¸ºæ¯ä¸ªå¥å­æœç´¢æœ€ç›¸å…³çš„æ®µè½ï¼ˆæ£€ç´¢åˆ°çš„æ–‡çŒ® + PDFæ®µè½ï¼‰
        answer_with_doi = ""
        doi_locations = {}
        matched_count = 0
        similarity_threshold = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
        for sentence, embedding in zip(sentences, embeddings):
            # è·³è¿‡ç©ºè¡Œã€æ ‡é¢˜è¡Œã€è¡¨æ ¼è¡Œ
            sent_strip = sentence.strip()
            if not sent_strip or sent_strip.startswith('#') or '|' in sent_strip:
                answer_with_doi += sentence
                continue
            
            try:
                # 4.1 åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ï¼ˆæ£€ç´¢åˆ°çš„æ–‡çŒ®ï¼‰
                best_similarity = 0.0
                best_doi = None
                best_content = None
                best_meta = {}
                best_source = 'abstract'  # 'abstract' or 'pdf'
                
                # æœç´¢æ£€ç´¢åˆ°çš„æ–‡çŒ®
                if doc_ids:
                    results = self._vector_repo._collection.query(
                        query_embeddings=[embedding],
                        n_results=50,
                        include=["documents", "metadatas", "distances"]
                    )
                    
                    if results and results["metadatas"] and results["metadatas"][0]:
                        result_ids = results.get("ids", [[]])[0]
                        for i, result_id in enumerate(result_ids):
                            if result_id in doc_ids:
                                similarity = 1 - (results["distances"][0][i] / 2.0)
                                
                                if similarity > best_similarity:
                                    meta = results["metadatas"][0][i]
                                    doi = meta.get('doi') or meta.get('DOI', 'N/A')
                                    if doi != 'N/A':
                                        best_similarity = similarity
                                        best_doi = doi
                                        best_content = results["documents"][0][i]
                                        best_meta = meta
                                        best_source = 'abstract'
                
                # 4.2 åœ¨PDFæ®µè½ä¸­æœç´¢ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                if pdf_chunks:
                    import numpy as np
                    # è®¡ç®—ä¸æ‰€æœ‰PDFæ®µè½çš„ç›¸ä¼¼åº¦
                    for doi, chunk, chunk_embedding, chunk_idx in pdf_chunks:
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        similarity = np.dot(embedding, chunk_embedding) / (
                            np.linalg.norm(embedding) * np.linalg.norm(chunk_embedding)
                        )
                        
                        if similarity > best_similarity:
                            best_similarity = float(similarity)
                            best_doi = doi
                            best_content = chunk
                            best_meta = {'page': 0, 'chunk_index_in_page': chunk_idx}
                            best_source = 'pdf'
                
                # 4.3 å¦‚æœæ‰¾åˆ°äº†åŒ¹é…ï¼Œæ’å…¥DOI
                if best_doi and best_similarity > similarity_threshold:
                    # éªŒè¯å¼•ç”¨å‡†ç¡®æ€§ï¼ˆæ£€æŸ¥å…³é”®æ•°å€¼ï¼‰
                    is_valid = self._validate_citation(sent_strip, best_content)
                    
                    # è°ƒæ•´éªŒè¯é€»è¾‘ï¼šPDFæ¥æºçš„åŒ¹é…æ›´å®½æ¾
                    should_insert = False
                    if best_source == 'pdf':
                        # PDFæ¥æºï¼šç›¸ä¼¼åº¦>0.5å³å¯æ’å…¥
                        should_insert = best_similarity >= 0.5
                    else:
                        # æ‘˜è¦æ¥æºï¼šéœ€è¦éªŒè¯é€šè¿‡æˆ–é«˜ç›¸ä¼¼åº¦
                        should_insert = is_valid or best_similarity >= 0.7
                    
                    if should_insert:
                        answer_with_doi += f"{sent_strip} (doi={best_doi})\n"
                        matched_count += 1
                        
                        # è®°å½•ä½ç½®ä¿¡æ¯
                        if best_doi not in doi_locations:
                            doi_locations[best_doi] = []
                        
                        # è®¡ç®—ç½®ä¿¡åº¦
                        if is_valid and best_similarity >= 0.7:
                            confidence = 'high'
                        elif (is_valid and best_similarity >= 0.5) or best_similarity >= 0.7:
                            confidence = 'medium'
                        else:
                            confidence = 'low'
                        
                        doi_locations[best_doi].append({
                            'sentence': sent_strip,
                            'page': best_meta.get('page', 0),
                            'chunk_index_in_page': best_meta.get('chunk_index_in_page', 0),
                            'similarity': float(best_similarity),
                            'confidence': confidence,
                            'source_text': best_content,
                            'validated': is_valid,
                            'source': best_source  # æ ‡è®°æ¥æº
                        })
                        
                        logger.debug(f"   âœ… æ’å…¥DOI: {best_doi} (ç›¸ä¼¼åº¦={best_similarity:.3f}, æ¥æº={best_source}, éªŒè¯={'é€šè¿‡' if is_valid else 'æœªé€šè¿‡'})")
                    else:
                        answer_with_doi += sentence
                        logger.debug(f"   âš ï¸  è·³è¿‡DOIæ’å…¥: éªŒè¯å¤±è´¥ä¸”ç›¸ä¼¼åº¦ä¸å¤Ÿ ({best_similarity:.3f})")
                else:
                    answer_with_doi += sentence
                    
            except Exception as e:
                logger.error(f"æœç´¢å¥å­å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                answer_with_doi += sentence
        
        logger.info(f"âœ… DOIæ’å…¥å®Œæˆ: {matched_count}/{len(sentences)} ä¸ªå¥å­åŒ¹é…æˆåŠŸ")
        logger.info(f"æ’å…¥äº† {len(doi_locations)} ä¸ªä¸åŒçš„DOI")
        
        return answer_with_doi, doi_locations
    
    def _insert_dois_by_sentence_db(
        self,
        answer: str,
        documents: List[Dict]
    ) -> tuple:
        """
        ä½¿ç”¨å¥å­çº§æ•°æ®åº“è¿›è¡ŒäºŒçº§æ£€ç´¢å’ŒDOIæ’å…¥
        
        å·¥ä½œæµç¨‹:
        1. ä»ä¸€çº§æ£€ç´¢ç»“æœæå–å€™é€‰DOIæ± 
        2. æ‹†åˆ†ç­”æ¡ˆä¸ºå¥å­
        3. æ‰¹é‡ç”Ÿæˆå¥å­embedding
        4. åœ¨å¥å­æ•°æ®åº“ä¸­æœç´¢ï¼ˆåªåŒ¹é…å€™é€‰DOIï¼‰
        5. ç›¸ä¼¼åº¦>0.6æ—¶æ’å…¥DOI
        
        Args:
            answer: LLMç”Ÿæˆçš„ç­”æ¡ˆ
            documents: ä¸€çº§æ£€ç´¢ç»“æœ
            
        Returns:
            (answer_with_doi, doi_locations)
        """
        # 1. æå–å€™é€‰DOIæ± 
        candidate_dois = self._extract_candidate_dois(documents)
        logger.info(f"[ä¸€çº§æ£€ç´¢] å€™é€‰DOIæ± : {len(candidate_dois)} ä¸ª")
        
        if not candidate_dois:
            logger.warning("âš ï¸  æ²¡æœ‰å€™é€‰DOIï¼Œæ— æ³•æ’å…¥å¼•ç”¨")
            return answer, {}
        
        # 2. æ‹†åˆ†ç­”æ¡ˆä¸ºå¥å­
        sentences = self._split_sentences_for_doi(answer)
        logger.info(f"[äºŒçº§æ£€ç´¢] æ‹†åˆ†ä¸º {len(sentences)} ä¸ªå¥å­")
        
        if not sentences:
            return answer, {}
        
        # 3. æ‰¹é‡ç”Ÿæˆå¥å­embedding
        try:
            logger.info(f"[äºŒçº§æ£€ç´¢] æ­£åœ¨ä¸º {len(sentences)} ä¸ªå¥å­ç”Ÿæˆembedding...")
            response = requests.post(
                self._bge_api_url,
                json={"input": sentences},
                timeout=60
            )
            response.raise_for_status()
            embeddings = [item["embedding"] for item in response.json()["data"]]
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªembedding")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {e}")
            return answer, {}
        
        # 4. åœ¨å¥å­æ•°æ®åº“ä¸­æœç´¢å¹¶æ’å…¥DOI
        answer_with_doi = ""
        doi_locations = {}
        matched_count = 0
        similarity_threshold = 0.6  # å¥å­çº§é˜ˆå€¼æé«˜åˆ°0.6
        
        for sentence, embedding in zip(sentences, embeddings):
            # è·³è¿‡ç©ºè¡Œã€æ ‡é¢˜è¡Œã€è¡¨æ ¼è¡Œ
            sent_strip = sentence.strip()
            if not sent_strip or sent_strip.startswith('#') or '|' in sent_strip:
                answer_with_doi += sentence
                continue
            
            try:
                # åœ¨å¥å­æ•°æ®åº“ä¸­æœç´¢
                results = self._sentence_collection.query(
                    query_embeddings=[embedding],
                    n_results=50,  # å¤šå–ä¸€äº›ç»“æœç”¨äºè¿‡æ»¤
                    include=["documents", "metadatas", "distances"]
                )
                
                # è¿‡æ»¤ï¼šåªä¿ç•™å€™é€‰DOIæ± ä¸­çš„ç»“æœ
                best_match = None
                best_similarity = 0.0
                
                if results and results["metadatas"] and results["metadatas"][0]:
                    for i, meta in enumerate(results["metadatas"][0]):
                        # å…¼å®¹DOIå­—æ®µï¼ˆå¤§å°å†™ï¼‰
                        doi = meta.get('DOI') or meta.get('doi')
                        
                        # åªè€ƒè™‘å€™é€‰DOIæ± ä¸­çš„ç»“æœ
                        if doi and doi in candidate_dois:
                            # è®¡ç®—ç›¸ä¼¼åº¦
                            distance = results["distances"][0][i]
                            similarity = 1 - (distance / 2.0)
                            
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_match = {
                                    'doi': doi,
                                    'content': results["documents"][0][i],
                                    'metadata': meta,
                                    'similarity': similarity
                                }
                
                # æ’å…¥DOIï¼ˆé˜ˆå€¼0.6ï¼‰
                if best_match and best_similarity > similarity_threshold:
                    answer_with_doi += f"{sent_strip} (doi={best_match['doi']})\n"
                    matched_count += 1
                    
                    # è®°å½•ä½ç½®ä¿¡æ¯
                    if best_match['doi'] not in doi_locations:
                        doi_locations[best_match['doi']] = []
                    
                    # è®¡ç®—ç½®ä¿¡åº¦
                    if best_similarity > 0.75:
                        confidence = 'high'
                    elif best_similarity > 0.6:
                        confidence = 'medium'
                    else:
                        confidence = 'low'
                    
                    doi_locations[best_match['doi']].append({
                        'sentence': sent_strip,
                        'similarity': float(best_similarity),
                        'confidence': confidence,
                        'source_sentence': best_match['content'],
                        'sentence_index': best_match['metadata'].get('sentence_index'),
                        'has_number': best_match['metadata'].get('has_number'),
                        'has_unit': best_match['metadata'].get('has_unit'),
                        'source': 'sentence_db'  # æ ‡è®°æ¥æº
                    })
                    
                    logger.debug(f"   âœ… æ’å…¥DOI: {best_match['doi']} (ç›¸ä¼¼åº¦={best_similarity:.3f}, ç½®ä¿¡åº¦={confidence})")
                else:
                    answer_with_doi += sentence
                    if best_match:
                        logger.debug(f"   âš ï¸  è·³è¿‡DOIæ’å…¥: ç›¸ä¼¼åº¦ä¸å¤Ÿ ({best_similarity:.3f} < {similarity_threshold})")
                    
            except Exception as e:
                logger.error(f"æœç´¢å¥å­å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                answer_with_doi += sentence
        
        logger.info(f"âœ… [äºŒçº§æ£€ç´¢] DOIæ’å…¥å®Œæˆ: {matched_count}/{len(sentences)} ä¸ªå¥å­åŒ¹é…æˆåŠŸ")
        logger.info(f"æ’å…¥äº† {len(doi_locations)} ä¸ªä¸åŒçš„DOI")
        
        return answer_with_doi, doi_locations
    
    def _insert_dois_by_pdf_search(
        self,
        answer: str,
        documents: List[Dict],
        pdf_contents: Optional[Dict[str, str]] = None
    ) -> tuple:
        """
        ä½¿ç”¨PDFæœç´¢è¿›è¡ŒDOIæ’å…¥ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
        
        è¿™æ˜¯åŸæœ‰çš„å®ç°ï¼Œå½“å¥å­æ•°æ®åº“ä¸å¯ç”¨æ—¶ä½¿ç”¨
        """
        # ç®€åŒ–å®ç°ï¼šåªåœ¨æ£€ç´¢åˆ°çš„æ–‡çŒ®ä¸­æœç´¢ï¼Œä¸ä½¿ç”¨PDF
        logger.warning("âš ï¸  å¥å­æ•°æ®åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„DOIæ’å…¥")
        logger.warning("âš ï¸  å»ºè®®æ£€æŸ¥å¥å­æ•°æ®åº“è¿æ¥")
        
        # æå–å€™é€‰DOI
        candidate_dois = self._extract_candidate_dois(documents)
        
        if not candidate_dois:
            return answer, {}
        
        # ç®€å•ç­–ç•¥ï¼šä¸ºæ¯ä¸ªæ®µè½çš„DOIæ’å…¥ä¸€æ¬¡å¼•ç”¨
        answer_with_doi = answer
        doi_locations = {}
        
        for doi in candidate_dois:
            if doi not in doi_locations:
                doi_locations[doi] = [{
                    'sentence': 'æ•´ä½“å¼•ç”¨',
                    'confidence': 'low',
                    'source': 'fallback'
                }]
        
        return answer_with_doi, doi_locations
    
    def _split_sentences_for_doi(self, text: str) -> List[str]:
        """æŒ‰ä¸­æ–‡æ ‡ç‚¹æ‹†åˆ†å¥å­ï¼ˆç”¨äºDOIæ’å…¥ï¼‰"""
        sentences = []
        current = ""
        
        for char in text:
            current += char
            if char in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n']:
                sent = current.strip()
                if sent and not sent.startswith('#'):  # è·³è¿‡æ ‡é¢˜
                    sentences.append(sent)
                current = ""
        
        if current.strip():
            sentences.append(current.strip())
        
        return sentences
    
    def _extract_candidate_dois(self, documents: List[Dict]) -> set:
        """
        ä»ä¸€çº§æ£€ç´¢ç»“æœä¸­æå–å€™é€‰DOIæ± 
        
        Args:
            documents: ä¸€çº§æ£€ç´¢è¿”å›çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å€™é€‰DOIé›†åˆ
        """
        candidate_dois = set()
        
        for doc in documents:
            meta = doc.get('metadata', {})
            # å…¼å®¹ 'doi' å’Œ 'DOI' ä¸¤ç§å­—æ®µå
            doi = meta.get('doi') or meta.get('DOI')
            
            if doi and doi != 'N/A' and 'unknown' not in doi.lower():
                candidate_dois.add(doi)
        
        return candidate_dois
    
    def _validate_citation(self, sentence: str, source_text: str) -> bool:
        """
        éªŒè¯å¼•ç”¨æ˜¯å¦å‡†ç¡®ï¼ˆæ£€æŸ¥å…³é”®æ•°å€¼ï¼‰
        
        Args:
            sentence: ç­”æ¡ˆä¸­çš„å¥å­
            source_text: åŸæ–‡ç‰‡æ®µ
            
        Returns:
            True=éªŒè¯é€šè¿‡, False=éªŒè¯å¤±è´¥
        """
        import re
        
        # æå–å¥å­ä¸­çš„æ•°å€¼ï¼ˆå¸¦å•ä½ï¼‰
        numbers = re.findall(r'\d+\.?\d*\s*[VvmAhÎ©%KkWw]+', sentence)
        
        if not numbers:
            return True  # æ²¡æœ‰æ•°å€¼ï¼Œæ— æ³•éªŒè¯ï¼Œé»˜è®¤é€šè¿‡
        
        # æ£€æŸ¥åŸæ–‡æ˜¯å¦åŒ…å«è¿™äº›æ•°å€¼
        for num in numbers:
            # æ¸…ç†æ•°å€¼ï¼Œå»é™¤ç©ºæ ¼ï¼Œç»Ÿä¸€å¤§å°å†™
            num_clean = num.replace(' ', '').lower()
            source_clean = source_text.replace(' ', '').lower()
            
            if num_clean not in source_clean:
                logger.debug(f"   âš ï¸  æ•°å€¼éªŒè¯å¤±è´¥: '{num}' ä¸åœ¨åŸæ–‡ä¸­")
                return False  # æ•°å€¼ä¸åŒ¹é…
        
        logger.debug(f"   âœ… æ•°å€¼éªŒè¯é€šè¿‡: {numbers}")
        return True  # æ‰€æœ‰æ•°å€¼éƒ½åŒ¹é…
    
    def _synthesize_broad_answer(
        self,
        user_question: str,
        documents: List[Dict]
    ) -> tuple:
        """åˆæˆå®½æ³›é—®é¢˜ç­”æ¡ˆ"""
        if not self._llm or not self._broad_question_prompt:
            return self._format_simple_answer(documents)
        
        try:
            # å®½æ³›é—®é¢˜ï¼šä½¿ç”¨æ›´å¤šæ‘˜è¦ï¼ˆ10ç¯‡ï¼‰ï¼Œä½†ä¸æ‰©å±•ä¸Šä¸‹æ–‡ï¼Œä¸åŠ è½½PDF
            logger.info("\n" + "="*80)
            logger.info("ğŸ“– [å®½æ³›é—®é¢˜] æå–æ–‡çŒ®æ‘˜è¦")
            logger.info(f"ä½¿ç”¨ 10 ç¯‡æ‘˜è¦ï¼ˆä¸æ‰©å±•ä¸Šä¸‹æ–‡ï¼ŒèŠ‚çœtokenï¼‰")
            
            summaries = []
            for i, doc in enumerate(documents[:10], 1):
                # ä½¿ç”¨åŸå§‹å†…å®¹ï¼Œä¸æ‰©å±•ä¸Šä¸‹æ–‡
                content = doc.get('content', '')
                summaries.append({
                    "åºå·": i,
                    "æ‘˜è¦": content[:1000]  # æ¯ç¯‡æœ€å¤š1000å­—ç¬¦
                })
                logger.info(f"  [{i}] æ‘˜è¦é•¿åº¦: {len(content[:1000])} å­—ç¬¦")
            
            logger.info(f"âœ… æ‘˜è¦æå–å®Œæˆï¼Œå…± {len(summaries)} ç¯‡")
            logger.info("="*80)
            
            summaries_json = json.dumps(summaries, ensure_ascii=False, indent=2)
            
            prompt = self._broad_question_prompt.replace("{user_question}", user_question)
            prompt = prompt.replace("{literature_summaries}", summaries_json)
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ“‹ [æ­¥éª¤6] æ„å»ºPromptï¼ˆå®½æ³›é—®é¢˜ï¼‰")
            logger.info(f"æ–‡çŒ®æ‘˜è¦: {len(summaries)} ç¯‡ï¼ˆåŸå§‹å†…å®¹ï¼Œä¸æ‰©å±•ï¼‰")
            logger.info(f"Promptæ€»é•¿åº¦: {len(prompt):,} å­—ç¬¦ (~{len(prompt)//4:,} tokens)")
            logger.info("="*80)
            
            from langchain_core.messages import HumanMessage
            
            response = self._llm.invoke([HumanMessage(content=prompt)])
            return response.content.strip(), {}  # å®½æ³›é—®é¢˜ä¸æ’å…¥DOI
            
        except Exception as e:
            logger.error(f"å®½æ³›é—®é¢˜ç­”æ¡ˆåˆæˆå¤±è´¥: {e}")
            return self._format_simple_answer(documents)
    
    def _format_simple_answer(self, documents: List[Dict]) -> tuple:
        """ç®€å•æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆæ— LLMæ—¶ä½¿ç”¨ï¼‰"""
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚", {}
        
        answer = f"æ‰¾åˆ° {len(documents)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼š\n\n"
        for i, doc in enumerate(documents[:10], 1):
            content = doc.get('content', '')[:200]
            score = doc.get('score', 0)
            answer += f"{i}. [ç›¸ä¼¼åº¦: {score:.2f}] {content}...\n\n"
        
        if len(documents) > 10:
            answer += f"... è¿˜æœ‰ {len(documents) - 10} ç¯‡æ–‡çŒ®æœªæ˜¾ç¤º"
        
        return answer, {}
    
    def query(self, question: str, load_pdf: bool = True) -> tuple:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›æ ¼å¼åŒ–çš„ç­”æ¡ˆå’ŒDOIä½ç½®ä¿¡æ¯"""
        result = self.search(question=question, top_k=20, with_scores=True)  # æ”¹å›20ï¼Œä¸ºembeddingåŒ¹é…æä¾›æ›´å¤šå€™é€‰
        
        if not result.get('success'):
            return f"æœç´¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}", {}
        
        documents = result.get('documents', [])
        
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚", {}
        
        # åˆ¤æ–­é—®é¢˜ç±»å‹
        is_broad = self._is_broad_question(question)
        
        # å®½æ³›é—®é¢˜ï¼šä¸åŠ è½½PDFï¼Œä½¿ç”¨å®½æ³›é—®é¢˜æ¨¡æ¿
        if is_broad:
            logger.info("æ£€æµ‹åˆ°å®½æ³›é—®é¢˜ï¼Œä½¿ç”¨å®½æ³›é—®é¢˜åˆæˆæ¨¡æ¿")
            return self._synthesize_broad_answer(question, documents)
        
        # ç²¾ç¡®é—®é¢˜ï¼šåŠ è½½PDFï¼Œä½¿ç”¨ç²¾ç¡®é—®é¢˜æ¨¡æ¿
        pdf_contents = {}
        if load_pdf and self._pdf_manager:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“„ [æ­¥éª¤5] æå–DOIå¹¶åŠ è½½PDFåŸæ–‡")
            dois = self._extract_dois(documents)
            logger.info(f"æå–åˆ°çš„DOIåˆ—è¡¨: {dois}")
            if dois:
                pdf_contents = self._load_pdf_contents(dois)
                logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(pdf_contents)} ç¯‡PDF")
                for doi, content in pdf_contents.items():
                    logger.info(f"  - {doi}: {len(content)} å­—ç¬¦")
            else:
                logger.info("âš ï¸  æœªæå–åˆ°DOI")
            logger.info("="*80)
        
        return self._synthesize_semantic_answer(question, documents, pdf_contents)
