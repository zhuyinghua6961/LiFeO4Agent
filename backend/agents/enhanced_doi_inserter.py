"""
å¢å¼ºçš„DOIæ’å…¥å™¨
ç¡®ä¿å‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„æ‰€æœ‰DOIéƒ½æœ‰å¼•ç”¨ä½ç½®
è¿è¡Œç¯å¢ƒ: conda run -n py310
"""
import logging
import requests
from typing import List, Dict, Any, Tuple, Optional

from backend.models.citation_location import CitationLocation
from backend.agents.reverse_citation_finder import ReverseCitationFinder

logger = logging.getLogger(__name__)


class EnhancedDOIInserter:
    """
    å¢å¼ºçš„DOIæ’å…¥å™¨
    
    ç»“åˆä¸¤ç§ç­–ç•¥ç¡®ä¿å®Œæ•´è¦†ç›–ï¼š
    1. æ­¥éª¤1ï¼šä¸ºç­”æ¡ˆä¸­çš„æ¯ä¸ªå¥å­æ‰¾æœ€ç›¸å…³çš„DOIï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    2. æ­¥éª¤2ï¼šä¸ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„æ¯ä¸ªDOIæ‰¾æœ€ç›¸å…³çš„ç­”æ¡ˆå¥å­ï¼ˆæ–°å¢ï¼‰
    
    æœ€ç»ˆåˆå¹¶ä¸¤ç§æ–¹å¼çš„å¼•ç”¨ä½ç½®ï¼Œç¡®ä¿å‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„æ‰€æœ‰DOIéƒ½æœ‰å¼•ç”¨ä½ç½®ã€‚
    """
    
    def __init__(
        self,
        sentence_collection,      # å¥å­çº§æ•°æ®åº“collection
        paragraph_collection,     # æ®µè½çº§æ•°æ®åº“collection
        bge_api_url: str
    ):
        """
        åˆå§‹åŒ–å¢å¼ºçš„DOIæ’å…¥å™¨
        
        Args:
            sentence_collection: ChromaDBå¥å­çº§collection
            paragraph_collection: ChromaDBæ®µè½çº§collection
            bge_api_url: BGE APIåœ°å€
        """
        self.sentence_collection = sentence_collection
        self.paragraph_collection = paragraph_collection
        self.bge_api_url = bge_api_url
        
        # åˆå§‹åŒ–åå‘æŸ¥æ‰¾å™¨
        self.reverse_finder = ReverseCitationFinder(
            sentence_collection=sentence_collection,
            paragraph_collection=paragraph_collection,
            bge_api_url=bge_api_url
        )
        
        logger.info("âœ… EnhancedDOIInserter åˆå§‹åŒ–æˆåŠŸ")
    
    def insert_dois_with_full_coverage(
        self,
        answer: str,
        documents: List[Dict],
        reference_dois: List[str],
        similarity_threshold: float = 0.3
    ) -> Tuple[str, Dict[str, List[CitationLocation]]]:
        """
        æ’å…¥DOIå¹¶ç¡®ä¿å‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„æ‰€æœ‰DOIéƒ½æœ‰å¼•ç”¨ä½ç½®
        
        Args:
            answer: LLMç”Ÿæˆçš„çº¯å‡€ç­”æ¡ˆ
            documents: ä¸€çº§æ£€ç´¢ç»“æœï¼ˆæ®µè½çº§ï¼‰
            reference_dois: å‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„DOIï¼ˆtop-5ï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            - answer_with_dois: æ’å…¥DOIåçš„ç­”æ¡ˆ
            - doi_locations: DOIåˆ°å¼•ç”¨ä½ç½®åˆ—è¡¨çš„æ˜ å°„
        """
        if not answer or not answer.strip():
            logger.warning("âš ï¸ ç­”æ¡ˆä¸ºç©ºï¼Œæ— æ³•æ’å…¥DOI")
            return answer, {}
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ å¼€å§‹å¢å¼ºçš„DOIæ’å…¥")
        logger.info(f"   ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
        logger.info(f"   å‚è€ƒæ–‡çŒ®æ•°: {len(reference_dois)}")
        logger.info(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        logger.info("="*80)
        
        # æ‹†åˆ†ç­”æ¡ˆä¸ºå¥å­
        answer_sentences = self._split_sentences(answer)
        logger.info(f"ğŸ“ æ‹†åˆ†ä¸º {len(answer_sentences)} ä¸ªå¥å­")
        
        if not answer_sentences:
            logger.warning("âš ï¸ ç­”æ¡ˆæ‹†åˆ†å¤±è´¥")
            return answer, {}
        
        # ========== æ­¥éª¤1ï¼šä¸ºç­”æ¡ˆå¥å­æ‰¾DOIï¼ˆåŸæœ‰é€»è¾‘ï¼‰==========
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Œ [æ­¥éª¤1] ä¸ºç­”æ¡ˆå¥å­æ‰¾æœ€ç›¸å…³çš„DOI")
        logger.info("="*80)
        
        step1_locations = self._find_dois_for_sentences(
            answer_sentences=answer_sentences,
            documents=documents,
            similarity_threshold=similarity_threshold
        )
        
        step1_doi_count = len(step1_locations)
        logger.info(f"âœ… æ­¥éª¤1å®Œæˆ: æ‰¾åˆ° {step1_doi_count} ä¸ªDOIçš„å¼•ç”¨ä½ç½®")
        
        # ========== æ­¥éª¤2ï¼šä¸ºå‚è€ƒæ–‡çŒ®DOIæ‰¾ç­”æ¡ˆå¥å­ï¼ˆæ–°å¢ï¼‰==========
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Œ [æ­¥éª¤2] ä¸ºå‚è€ƒæ–‡çŒ®DOIæ‰¾æœ€ç›¸å…³çš„ç­”æ¡ˆå¥å­")
        logger.info("="*80)
        
        step2_locations = self._find_sentences_for_dois(
            reference_dois=reference_dois,
            answer_sentences=answer_sentences,
            similarity_threshold=similarity_threshold
        )
        
        step2_doi_count = len(step2_locations)
        logger.info(f"âœ… æ­¥éª¤2å®Œæˆ: æ‰¾åˆ° {step2_doi_count} ä¸ªDOIçš„å¼•ç”¨ä½ç½®")
        
        # ========== æ­¥éª¤3ï¼šåˆå¹¶å¼•ç”¨ä½ç½® ==========
        logger.info("\n" + "="*80)
        logger.info("ğŸ”„ [æ­¥éª¤3] åˆå¹¶å¼•ç”¨ä½ç½®")
        logger.info("="*80)
        
        merged_locations = self._merge_locations(step1_locations, step2_locations)
        
        logger.info(f"âœ… åˆå¹¶å®Œæˆ: å…± {len(merged_locations)} ä¸ªDOIæœ‰å¼•ç”¨ä½ç½®")
        
        # æ£€æŸ¥è¦†ç›–ç‡
        covered_dois = set(merged_locations.keys())
        missing_dois = set(reference_dois) - covered_dois
        coverage_rate = len(covered_dois) / len(reference_dois) * 100 if reference_dois else 0
        
        logger.info(f"ğŸ“Š è¦†ç›–ç‡ç»Ÿè®¡:")
        logger.info(f"   å‚è€ƒæ–‡çŒ®æ€»æ•°: {len(reference_dois)}")
        logger.info(f"   å·²è¦†ç›–: {len(covered_dois)} ({coverage_rate:.1f}%)")
        logger.info(f"   æœªè¦†ç›–: {len(missing_dois)}")
        
        if missing_dois:
            logger.warning(f"âš ï¸ ä»¥ä¸‹DOIæœªæ‰¾åˆ°å¼•ç”¨ä½ç½®:")
            for doi in missing_dois:
                logger.warning(f"   - {doi}")
        
        # ========== æ­¥éª¤4ï¼šæ’å…¥DOIåˆ°ç­”æ¡ˆ ==========
        logger.info("\n" + "="*80)
        logger.info("âœï¸ [æ­¥éª¤4] æ’å…¥DOIåˆ°ç­”æ¡ˆ")
        logger.info("="*80)
        
        answer_with_dois = self._insert_dois_to_answer(
            answer_sentences=answer_sentences,
            doi_locations=merged_locations
        )
        
        logger.info(f"âœ… DOIæ’å…¥å®Œæˆ")
        logger.info("="*80)
        
        return answer_with_dois, merged_locations
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        æ‹†åˆ†æ–‡æœ¬ä¸ºå¥å­åˆ—è¡¨
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        sentences = []
        current = ""
        
        for char in text:
            current += char
            # ä¸­æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·
            if char in ['ã€‚', 'ï¼Ÿ', 'ï¼', '\n']:
                if current.strip():
                    sentences.append(current)
                current = ""
        
        # æ·»åŠ æœ€åä¸€ä¸ªå¥å­
        if current.strip():
            sentences.append(current)
        
        return sentences
    
    def _find_dois_for_sentences(
        self,
        answer_sentences: List[str],
        documents: List[Dict],
        similarity_threshold: float
    ) -> Dict[str, List[CitationLocation]]:
        """
        æ­¥éª¤1ï¼šä¸ºç­”æ¡ˆä¸­çš„æ¯ä¸ªå¥å­æ‰¾æœ€ç›¸å…³çš„DOI
        
        è¿™æ˜¯åŸæœ‰çš„DOIæ’å…¥é€»è¾‘ï¼š
        - ä¸ºæ¯ä¸ªç­”æ¡ˆå¥å­ç”Ÿæˆembedding
        - åœ¨å¥å­çº§æ•°æ®åº“ä¸­æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡çŒ®å¥å­
        - æå–DOIå¹¶è®°å½•å¼•ç”¨ä½ç½®
        
        Args:
            answer_sentences: ç­”æ¡ˆå¥å­åˆ—è¡¨
            documents: ä¸€çº§æ£€ç´¢ç»“æœ
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            DOIåˆ°å¼•ç”¨ä½ç½®åˆ—è¡¨çš„æ˜ å°„
        """
        doi_locations = {}
        
        # æå–å€™é€‰DOIæ± ï¼ˆä»ä¸€çº§æ£€ç´¢ç»“æœï¼‰
        candidate_dois = self._extract_candidate_dois(documents)
        logger.info(f"   å€™é€‰DOIæ± : {len(candidate_dois)} ä¸ª")
        
        if not candidate_dois:
            logger.warning("âš ï¸ æ²¡æœ‰å€™é€‰DOI")
            return doi_locations
        
        # æ‰¹é‡ç”Ÿæˆç­”æ¡ˆå¥å­çš„embedding
        try:
            valid_sentences = [(i, s) for i, s in enumerate(answer_sentences) if s.strip()]
            sentence_texts = [s for _, s in valid_sentences]
            
            logger.info(f"   æ­£åœ¨ä¸º {len(sentence_texts)} ä¸ªå¥å­ç”Ÿæˆembedding...")
            response = requests.post(
                self.bge_api_url,
                json={"input": sentence_texts},
                timeout=60
            )
            response.raise_for_status()
            embeddings = [item["embedding"] for item in response.json()["data"]]
            logger.info(f"   âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ªembedding")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {e}")
            return doi_locations
        
        # ä¸ºæ¯ä¸ªå¥å­æŸ¥æ‰¾æœ€ç›¸å…³çš„DOI
        matched_count = 0
        for (sent_idx, sentence), embedding in zip(valid_sentences, embeddings):
            # è·³è¿‡ç‰¹æ®Šè¡Œ
            sent_strip = sentence.strip()
            if not sent_strip or sent_strip.startswith('#') or '|' in sent_strip:
                continue
            
            try:
                # åœ¨å¥å­çº§æ•°æ®åº“ä¸­æŸ¥è¯¢
                results = self.sentence_collection.query(
                    query_embeddings=[embedding],
                    n_results=50,
                    include=["documents", "metadatas", "distances"]
                )
                
                # æ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼ˆåªè€ƒè™‘å€™é€‰DOIæ± ï¼‰
                best_match = None
                best_similarity = 0.0
                
                if results and results["metadatas"] and results["metadatas"][0]:
                    for i, meta in enumerate(results["metadatas"][0]):
                        doi = meta.get('DOI') or meta.get('doi')
                        
                        if doi and doi in candidate_dois:
                            distance = results["distances"][0][i]
                            similarity = 1 - (distance / 2.0)
                            
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_match = {
                                    'doi': doi,
                                    'content': results["documents"][0][i],
                                    'metadata': meta,
                                    'similarity': similarity
                                }
                
                # å¦‚æœæ‰¾åˆ°åŒ¹é…ä¸”è¶…è¿‡é˜ˆå€¼
                if best_match and best_similarity >= similarity_threshold:
                    doi = best_match['doi']
                    
                    # è·å–é¡µç ï¼ˆä»æ®µè½çº§æ•°æ®åº“ï¼‰
                    page = self._get_page_for_doi(doi)
                    
                    # åˆ›å»ºCitationLocation
                    citation = CitationLocation.from_sentence_db(
                        doi=doi,
                        answer_sentence=sent_strip,
                        answer_sentence_index=sent_idx,
                        sentence_text=best_match['content'],
                        sentence_metadata=best_match['metadata'],
                        similarity=best_similarity,
                        page=page
                    )
                    
                    # æ·»åŠ åˆ°ç»“æœ
                    if doi not in doi_locations:
                        doi_locations[doi] = []
                    doi_locations[doi].append(citation)
                    
                    matched_count += 1
                    logger.debug(f"   âœ… å¥å­{sent_idx}: {doi} (ç›¸ä¼¼åº¦={best_similarity:.3f})")
                    
            except Exception as e:
                logger.error(f"âŒ æŸ¥è¯¢å¥å­å¤±è´¥ (idx={sent_idx}): {e}")
                continue
        
        logger.info(f"   åŒ¹é…æˆåŠŸ: {matched_count}/{len(valid_sentences)} ä¸ªå¥å­")
        
        return doi_locations
    
    def _find_sentences_for_dois(
        self,
        reference_dois: List[str],
        answer_sentences: List[str],
        similarity_threshold: float
    ) -> Dict[str, List[CitationLocation]]:
        """
        æ­¥éª¤2ï¼šä¸ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„æ¯ä¸ªDOIæ‰¾æœ€ç›¸å…³çš„ç­”æ¡ˆå¥å­
        
        è¿™æ˜¯æ–°å¢çš„åå‘æŸ¥æ‰¾é€»è¾‘ï¼š
        - å¯¹äºæ¯ä¸ªå‚è€ƒæ–‡çŒ®DOI
        - ä½¿ç”¨ReverseCitationFinderæ‰¾åˆ°ç­”æ¡ˆä¸­æœ€ç›¸å…³çš„å¥å­
        - è®°å½•å¼•ç”¨ä½ç½®
        
        Args:
            reference_dois: å‚è€ƒæ–‡çŒ®DOIåˆ—è¡¨
            answer_sentences: ç­”æ¡ˆå¥å­åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            DOIåˆ°å¼•ç”¨ä½ç½®åˆ—è¡¨çš„æ˜ å°„
        """
        doi_locations = {}
        
        logger.info(f"   æ­£åœ¨ä¸º {len(reference_dois)} ä¸ªå‚è€ƒæ–‡çŒ®DOIæŸ¥æ‰¾å¼•ç”¨ä½ç½®...")
        
        for doi in reference_dois:
            try:
                # ä½¿ç”¨åå‘æŸ¥æ‰¾å™¨
                citations = self.reverse_finder.find_citations_for_doi(
                    doi=doi,
                    answer_sentences=answer_sentences,
                    top_k=3,  # æ¯ä¸ªDOIæœ€å¤š3ä¸ªå¼•ç”¨ä½ç½®
                    similarity_threshold=similarity_threshold
                )
                
                if citations:
                    doi_locations[doi] = citations
                    logger.debug(f"   âœ… {doi}: æ‰¾åˆ° {len(citations)} ä¸ªå¼•ç”¨ä½ç½®")
                else:
                    logger.debug(f"   âš ï¸ {doi}: æœªæ‰¾åˆ°å¼•ç”¨ä½ç½®")
                    
            except Exception as e:
                logger.error(f"âŒ åå‘æŸ¥æ‰¾å¤±è´¥ ({doi}): {e}")
                continue
        
        logger.info(f"   åå‘æŸ¥æ‰¾å®Œæˆ: {len(doi_locations)}/{len(reference_dois)} ä¸ªDOIæœ‰å¼•ç”¨ä½ç½®")
        
        return doi_locations
    
    def _merge_locations(
        self,
        step1_locations: Dict[str, List[CitationLocation]],
        step2_locations: Dict[str, List[CitationLocation]]
    ) -> Dict[str, List[CitationLocation]]:
        """
        åˆå¹¶ä¸¤ç§æ–¹å¼çš„å¼•ç”¨ä½ç½®
        
        åˆå¹¶ç­–ç•¥ï¼š
        1. å¯¹äºæ¯ä¸ªDOIï¼Œåˆå¹¶ä¸¤ç§æ–¹å¼æ‰¾åˆ°çš„å¼•ç”¨ä½ç½®
        2. å»é‡ï¼šç›¸åŒç­”æ¡ˆå¥å­ç´¢å¼•çš„ä½ç½®åªä¿ç•™ç›¸ä¼¼åº¦æœ€é«˜çš„
        3. æ’åºï¼šæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        
        Args:
            step1_locations: æ­¥éª¤1çš„ç»“æœ
            step2_locations: æ­¥éª¤2çš„ç»“æœ
            
        Returns:
            åˆå¹¶åçš„å¼•ç”¨ä½ç½®æ˜ å°„
        """
        merged = {}
        
        # æ”¶é›†æ‰€æœ‰DOI
        all_dois = set(step1_locations.keys()) | set(step2_locations.keys())
        
        for doi in all_dois:
            locations = []
            
            # æ·»åŠ æ­¥éª¤1çš„ä½ç½®
            if doi in step1_locations:
                locations.extend(step1_locations[doi])
            
            # æ·»åŠ æ­¥éª¤2çš„ä½ç½®
            if doi in step2_locations:
                locations.extend(step2_locations[doi])
            
            # å»é‡ï¼šç›¸åŒç­”æ¡ˆå¥å­ç´¢å¼•åªä¿ç•™ç›¸ä¼¼åº¦æœ€é«˜çš„
            deduped = {}
            for loc in locations:
                sent_idx = loc.answer_sentence_index
                if sent_idx not in deduped or loc.similarity > deduped[sent_idx].similarity:
                    deduped[sent_idx] = loc
            
            # æ’åºï¼šæŒ‰ç›¸ä¼¼åº¦é™åº
            sorted_locations = sorted(deduped.values(), key=lambda x: x.similarity, reverse=True)
            
            # é™åˆ¶æ¯ä¸ªDOIæœ€å¤š5ä¸ªå¼•ç”¨ä½ç½®
            merged[doi] = sorted_locations[:5]
            
            logger.debug(f"   {doi}: åˆå¹¶ {len(locations)} â†’ å»é‡ {len(deduped)} â†’ ä¿ç•™ {len(merged[doi])}")
        
        return merged
    
    def _insert_dois_to_answer(
        self,
        answer_sentences: List[str],
        doi_locations: Dict[str, List[CitationLocation]]
    ) -> str:
        """
        å°†DOIæ’å…¥åˆ°ç­”æ¡ˆä¸­
        
        ç­–ç•¥ï¼š
        - ä¸ºæ¯ä¸ªç­”æ¡ˆå¥å­æ‰¾åˆ°å¯¹åº”çš„DOIï¼ˆå¦‚æœæœ‰ï¼‰
        - åœ¨å¥å­æœ«å°¾æ’å…¥ (doi=XXX)
        - å¦‚æœä¸€ä¸ªå¥å­æœ‰å¤šä¸ªDOIï¼Œåªæ’å…¥ç›¸ä¼¼åº¦æœ€é«˜çš„
        
        Args:
            answer_sentences: ç­”æ¡ˆå¥å­åˆ—è¡¨
            doi_locations: DOIåˆ°å¼•ç”¨ä½ç½®åˆ—è¡¨çš„æ˜ å°„
            
        Returns:
            æ’å…¥DOIåçš„ç­”æ¡ˆ
        """
        # æ„å»ºå¥å­ç´¢å¼•åˆ°DOIçš„æ˜ å°„
        sentence_to_doi = {}
        
        for doi, locations in doi_locations.items():
            for loc in locations:
                sent_idx = loc.answer_sentence_index
                if sent_idx not in sentence_to_doi or loc.similarity > sentence_to_doi[sent_idx][1]:
                    sentence_to_doi[sent_idx] = (doi, loc.similarity)
        
        # æ’å…¥DOI
        answer_with_dois = ""
        inserted_count = 0
        
        for i, sentence in enumerate(answer_sentences):
            sent_strip = sentence.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œç‰¹æ®Šè¡Œ
            if not sent_strip or sent_strip.startswith('#') or '|' in sent_strip:
                answer_with_dois += sentence
                continue
            
            # å¦‚æœè¿™ä¸ªå¥å­æœ‰å¯¹åº”çš„DOI
            if i in sentence_to_doi:
                doi, similarity = sentence_to_doi[i]
                # åœ¨å¥å­æœ«å°¾æ’å…¥DOIï¼ˆå»é™¤åŸæœ‰çš„æ¢è¡Œç¬¦ï¼Œç»Ÿä¸€æ·»åŠ ï¼‰
                answer_with_dois += f"{sent_strip} (doi={doi})\n"
                inserted_count += 1
                logger.debug(f"   æ’å…¥DOI: å¥å­{i} â†’ {doi} (ç›¸ä¼¼åº¦={similarity:.3f})")
            else:
                answer_with_dois += sentence
        
        logger.info(f"   æ’å…¥äº† {inserted_count} ä¸ªDOIå¼•ç”¨")
        
        return answer_with_dois
    
    def _extract_candidate_dois(self, documents: List[Dict]) -> set:
        """
        ä»ä¸€çº§æ£€ç´¢ç»“æœä¸­æå–å€™é€‰DOIæ± 
        
        Args:
            documents: ä¸€çº§æ£€ç´¢è¿”å›çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å€™é€‰DOIé›†åˆ
        """
        candidate_dois = set()
        
        for doc in documents:
            meta = doc.get('metadata', {})
            doi = meta.get('doi') or meta.get('DOI')
            
            if doi and doi != 'N/A' and 'unknown' not in doi.lower():
                candidate_dois.add(doi)
        
        return candidate_dois
    
    def _get_page_for_doi(self, doi: str) -> int:
        """
        ä»æ®µè½çº§æ•°æ®åº“è·å–DOIçš„é¡µç 
        
        Args:
            doi: æ–‡çŒ®DOI
            
        Returns:
            é¡µç ï¼ˆå¦‚æœæ‰¾ä¸åˆ°è¿”å›0ï¼‰
        """
        try:
            results = self.paragraph_collection.get(
                where={"doi": doi},
                limit=1,
                include=['metadatas']
            )
            
            if results and results['metadatas']:
                return results['metadatas'][0].get('page', 0)
            else:
                return 0
                
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ®µè½çº§æ•°æ®åº“å¤±è´¥ ({doi}): {e}")
            return 0
