"""
åå‘å¼•ç”¨æŸ¥æ‰¾å™¨
ä¸ºç»™å®šçš„DOIæ‰¾åˆ°ç­”æ¡ˆä¸­æœ€ç›¸å…³çš„å¥å­ä½ç½®
è¿è¡Œç¯å¢ƒ: conda run -n py310
"""
import logging
import requests
import hashlib
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from backend.models.citation_location import CitationLocation

logger = logging.getLogger(__name__)


class ReverseCitationFinder:
    """
    åå‘å¼•ç”¨æŸ¥æ‰¾å™¨
    
    ä¸ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¸­çš„DOIæ‰¾åˆ°ç­”æ¡ˆä¸­æœ€ç›¸å…³çš„å¥å­ï¼Œ
    å¹¶ä»æ•°æ®åº“ä¸­è·å–å‡†ç¡®çš„é¡µç å’Œæ®µè½ä½ç½®ä¿¡æ¯ã€‚
    """
    
    def __init__(
        self,
        sentence_collection,      # å¥å­çº§æ•°æ®åº“collection
        paragraph_collection,     # æ®µè½çº§æ•°æ®åº“collection
        bge_api_url: str
    ):
        """
        åˆå§‹åŒ–åå‘å¼•ç”¨æŸ¥æ‰¾å™¨
        
        Args:
            sentence_collection: ChromaDBå¥å­çº§collection (lfp_papers_sentences_v1)
            paragraph_collection: ChromaDBæ®µè½çº§collection (lfp_papers_v3)
            bge_api_url: BGE APIåœ°å€
        """
        self.sentence_collection = sentence_collection
        self.paragraph_collection = paragraph_collection
        self.bge_api_url = bge_api_url
        
        # ç¼“å­˜
        self._embedding_cache: Dict[str, List[float]] = {}
        self._page_cache: Dict[str, int] = {}  # DOI -> é¡µç ç¼“å­˜
        
        logger.info("âœ… ReverseCitationFinder åˆå§‹åŒ–æˆåŠŸ")
    
    def _get_query_hash(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç¼“å­˜ï¼‰"""
        return hashlib.md5(query.encode('utf-8')).hexdigest()
    
    def _generate_embedding(self, text: str) -> List[float]:
        """
        ç”Ÿæˆæ–‡æœ¬embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            embeddingå‘é‡
        """
        # æ£€æŸ¥ç¼“å­˜
        text_hash = self._get_query_hash(text)
        if text_hash in self._embedding_cache:
            return self._embedding_cache[text_hash]
        
        try:
            response = requests.post(
                self.bge_api_url,
                json={"input": [text]},
                timeout=30
            )
            response.raise_for_status()
            embedding = response.json()["data"][0]["embedding"]
            
            # ç¼“å­˜ç»“æœ
            self._embedding_cache[text_hash] = embedding
            
            return embedding
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {e}")
            raise
    
    def _get_page_for_doi(self, doi: str) -> int:
        """
        ä»æ®µè½çº§æ•°æ®åº“è·å–DOIçš„é¡µç 
        
        Args:
            doi: æ–‡çŒ®DOI
            
        Returns:
            é¡µç ï¼ˆå¦‚æœæ‰¾ä¸åˆ°è¿”å›0ï¼‰
        """
        # æ£€æŸ¥ç¼“å­˜
        if doi in self._page_cache:
            return self._page_cache[doi]
        
        try:
            # æŸ¥è¯¢æ®µè½çº§æ•°æ®åº“
            results = self.paragraph_collection.get(
                where={"doi": doi},
                limit=1,
                include=['metadatas']
            )
            
            if results and results['metadatas']:
                page = results['metadatas'][0].get('page', 0)
                self._page_cache[doi] = page
                return page
            else:
                logger.warning(f"âš ï¸ åœ¨æ®µè½çº§æ•°æ®åº“ä¸­æœªæ‰¾åˆ°DOI: {doi}")
                return 0
                
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ®µè½çº§æ•°æ®åº“å¤±è´¥ ({doi}): {e}")
            return 0
    
    def find_citations_for_doi(
        self,
        doi: str,
        answer_sentences: List[str],
        top_k: int = 3,
        similarity_threshold: float = 0.3
    ) -> List[CitationLocation]:
        """
        ä¸ºç»™å®šçš„DOIæ‰¾åˆ°ç­”æ¡ˆä¸­æœ€ç›¸å…³çš„å¥å­
        
        Args:
            doi: æ–‡çŒ®DOI
            answer_sentences: ç­”æ¡ˆçš„å¥å­åˆ—è¡¨
            top_k: è¿”å›çš„å¼•ç”¨ä½ç½®æ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            å¼•ç”¨ä½ç½®åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if not answer_sentences:
            logger.warning(f"âš ï¸ ç­”æ¡ˆå¥å­åˆ—è¡¨ä¸ºç©º")
            return []
        
        logger.info(f"ğŸ” ä¸ºDOIæŸ¥æ‰¾å¼•ç”¨ä½ç½®: {doi}")
        logger.info(f"   ç­”æ¡ˆå¥å­æ•°: {len(answer_sentences)}")
        
        try:
            # 1. ä¸ºæ¯ä¸ªç­”æ¡ˆå¥å­ç”Ÿæˆembedding
            answer_embeddings = []
            for i, sentence in enumerate(answer_sentences):
                if sentence.strip():
                    embedding = self._generate_embedding(sentence)
                    answer_embeddings.append((i, sentence, embedding))
            
            logger.info(f"   ç”Ÿæˆäº† {len(answer_embeddings)} ä¸ªembedding")
            
            # 2. åœ¨å¥å­çº§æ•°æ®åº“ä¸­æŸ¥è¯¢è¯¥DOIçš„å¥å­
            # æ³¨æ„ï¼šå¥å­çº§æ•°æ®åº“ä½¿ç”¨å¤§å†™çš„"DOI"å­—æ®µ
            all_citations = []
            
            for answer_idx, answer_sentence, answer_embedding in answer_embeddings:
                try:
                    # æŸ¥è¯¢å¥å­çº§æ•°æ®åº“
                    results = self.sentence_collection.query(
                        query_embeddings=[answer_embedding],
                        n_results=10,  # å¤šå–ä¸€äº›ç”¨äºç­›é€‰
                        where={"DOI": doi},  # æ³¨æ„å¤§å†™
                        include=["documents", "metadatas", "distances"]
                    )
                    
                    if not results or not results.get("documents") or not results["documents"][0]:
                        continue
                    
                    # 3. è®¡ç®—ç›¸ä¼¼åº¦å¹¶åˆ›å»ºCitationLocation
                    for i in range(len(results["documents"][0])):
                        distance = results["distances"][0][i]
                        similarity = 1 - (distance / 2.0)  # ChromaDBä½™å¼¦è·ç¦»è½¬æ¢
                        
                        if similarity >= similarity_threshold:
                            sentence_text = results["documents"][0][i]
                            sentence_metadata = results["metadatas"][0][i]
                            
                            # ä»æ®µè½çº§æ•°æ®åº“è·å–é¡µç 
                            page = self._get_page_for_doi(doi)
                            
                            # åˆ›å»ºCitationLocation
                            citation = CitationLocation.from_sentence_db(
                                doi=doi,
                                answer_sentence=answer_sentence,
                                answer_sentence_index=answer_idx,
                                sentence_text=sentence_text,
                                sentence_metadata=sentence_metadata,
                                similarity=similarity,
                                page=page
                            )
                            
                            all_citations.append(citation)
                            
                            # æ¯ä¸ªç­”æ¡ˆå¥å­åªä¿ç•™æœ€ä½³åŒ¹é…
                            break
                
                except Exception as e:
                    logger.warning(f"âš ï¸ æŸ¥è¯¢å¥å­å¤±è´¥ (answer_idx={answer_idx}): {e}")
                    continue
            
            # 4. æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›top-k
            all_citations.sort(key=lambda x: x.similarity, reverse=True)
            result = all_citations[:top_k]
            
            logger.info(f"âœ… æ‰¾åˆ° {len(result)} ä¸ªå¼•ç”¨ä½ç½®")
            if result:
                logger.info(f"   æœ€é«˜ç›¸ä¼¼åº¦: {result[0].similarity:.3f}")
                logger.info(f"   é¡µç : {result[0].page}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾å¼•ç”¨ä½ç½®å¤±è´¥: {e}")
            return []
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        self._embedding_cache.clear()
        self._page_cache.clear()
        logger.info("âœ… ç¼“å­˜å·²æ¸…é™¤")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            "embedding_cache_size": len(self._embedding_cache),
            "page_cache_size": len(self._page_cache)
        }
