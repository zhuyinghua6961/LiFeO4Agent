"""
å¥å­çº§é‡æ’åºå™¨
è´Ÿè´£ä½¿ç”¨å¥å­æ•°æ®åº“é‡æ–°æ’åºå€™é€‰æ–‡çŒ®
"""
import logging
import requests
import time
import hashlib
import numpy as np
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from functools import lru_cache

logger = logging.getLogger(__name__)


@dataclass
class RerankingResult:
    """é‡æ’åºç»“æœ"""
    documents: List[Dict]                      # é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
    similarity_scores: Dict[str, float]        # æ¯ä¸ªDOIçš„ç›¸ä¼¼åº¦
    reranking_time: float                      # é‡æ’åºè€—æ—¶ï¼ˆç§’ï¼‰
    top_3_changes: List[tuple[str, int, int]]  # top-3çš„æ’åå˜åŒ– (doi, old_rank, new_rank)


class SentenceReranker:
    """å¥å­çº§é‡æ’åºå™¨ç±»"""
    
    def __init__(
        self, 
        sentence_collection,
        bge_api_url: str
    ):
        """
        åˆå§‹åŒ–é‡æ’åºå™¨
        
        Args:
            sentence_collection: ChromaDBå¥å­çº§collectionå®ä¾‹
            bge_api_url: BGE APIåœ°å€
        """
        self.sentence_collection = sentence_collection
        self.bge_api_url = bge_api_url
        
        # åˆå§‹åŒ–ç¼“å­˜å­—å…¸ï¼ˆç”¨äºç¼“å­˜æŸ¥è¯¢embeddingå’Œç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
        self._embedding_cache: Dict[str, List[float]] = {}
        self._similarity_cache: Dict[str, float] = {}
        
        logger.info("âœ… SentenceReranker åˆå§‹åŒ–æˆåŠŸ")
    
    def _get_query_hash(self, query: str) -> str:
        """
        ç”ŸæˆæŸ¥è¯¢çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç¼“å­˜ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æŸ¥è¯¢çš„MD5å“ˆå¸Œå€¼
        """
        return hashlib.md5(query.encode('utf-8')).hexdigest()
    
    def _get_cache_key(self, query: str, doi: str) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            doi: æ–‡çŒ®DOI
            
        Returns:
            ç¼“å­˜é”®
        """
        query_hash = self._get_query_hash(query)
        return f"{query_hash}:{doi}"
    
    def _generate_query_embedding(self, query: str) -> List[float]:
        """
        ç”ŸæˆæŸ¥è¯¢embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æŸ¥è¯¢embeddingå‘é‡
            
        Raises:
            Exception: å¦‚æœAPIè°ƒç”¨å¤±è´¥
        """
        # æ£€æŸ¥ç¼“å­˜
        query_hash = self._get_query_hash(query)
        if query_hash in self._embedding_cache:
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æŸ¥è¯¢embedding")
            return self._embedding_cache[query_hash]
        
        try:
            response = requests.post(
                self.bge_api_url,
                json={"input": [query]},
                timeout=30
            )
            response.raise_for_status()
            embedding = response.json()["data"][0]["embedding"]
            
            # ç¼“å­˜ç»“æœ
            self._embedding_cache[query_hash] = embedding
            logger.info(f"âœ… ç”Ÿæˆå¹¶ç¼“å­˜æŸ¥è¯¢embedding")
            
            return embedding
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆæŸ¥è¯¢embeddingå¤±è´¥: {e}")
            raise
    
    def _clean_doi(self, doi: str) -> str:
        """
        æ¸…ç†DOIï¼Œå»æ‰å¸¸è§çš„åç¼€
        
        Args:
            doi: åŸå§‹DOI
            
        Returns:
            æ¸…ç†åçš„DOI
        """
        if not doi:
            return doi
        
        # å»æ‰å¸¸è§åç¼€
        suffixes = ["abstract", "full", "pdf", "epdf", "html"]
        doi_lower = doi.lower()
        
        for suffix in suffixes:
            if doi_lower.endswith(suffix):
                # å»æ‰åç¼€
                doi = doi[:-len(suffix)]
                logger.debug(f"æ¸…ç†DOI: å»æ‰åç¼€ '{suffix}' -> {doi}")
                break
        
        return doi.strip()
    
    def _batch_query_sentences(
        self, 
        query_embedding: List[float],
        dois: List[str],
        n_results_per_doi: int = 50
    ) -> Dict[str, List[Dict]]:
        """
        æ‰¹é‡æŸ¥è¯¢å¥å­æ•°æ®åº“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            query_embedding: æŸ¥è¯¢embedding
            dois: DOIåˆ—è¡¨
            n_results_per_doi: æ¯ä¸ªDOIè¿”å›çš„å¥å­æ•°é‡
            
        Returns:
            DOIåˆ°å¥å­åˆ—è¡¨çš„æ˜ å°„
        """
        doi_to_sentences = {}
        
        # æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–ï¼šå¯¹äºå¤šä¸ªDOIï¼Œå¯ä»¥è€ƒè™‘å¹¶è¡ŒæŸ¥è¯¢
        # ä½†ç”±äºChromaDBçš„é™åˆ¶ï¼Œè¿™é‡Œä»ç„¶ä¸²è¡ŒæŸ¥è¯¢ï¼Œä½†æ·»åŠ äº†æ›´å¥½çš„é”™è¯¯å¤„ç†
        
        for doi in dois:
            try:
                # åœ¨å¥å­æ•°æ®åº“ä¸­æŸ¥è¯¢è¯¥DOIçš„æ‰€æœ‰å¥å­
                # æ³¨æ„ï¼šå¥å­æ•°æ®åº“ä¸­DOIå­—æ®µæ˜¯å¤§å†™çš„"DOI"
                results = self.sentence_collection.query(
                    query_embeddings=[query_embedding],
                    n_results=n_results_per_doi,
                    where={"DOI": doi}
                )
                
                # æå–å¥å­å’Œç›¸ä¼¼åº¦
                sentences = []
                if results and results.get("documents"):
                    documents = results["documents"][0]
                    distances = results["distances"][0]
                    metadatas = results["metadatas"][0]
                    
                    # æ·»åŠ è°ƒè¯•æ—¥å¿—
                    if len(documents) > 0:
                        logger.debug(f"âœ… DOI {doi}: æ‰¾åˆ° {len(documents)} ä¸ªå¥å­")
                    else:
                        logger.warning(f"âš ï¸ DOI {doi}: æŸ¥è¯¢æˆåŠŸä½†æ²¡æœ‰è¿”å›å¥å­")
                    
                    for i in range(len(documents)):
                        # ChromaDBä½¿ç”¨ä½™å¼¦è·ç¦»ï¼ŒèŒƒå›´[0,2]
                        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼šsimilarity = 1 - (distance / 2)
                        # è¿™æ ·ç›¸ä¼¼åº¦èŒƒå›´æ˜¯[0,1]ï¼Œå…¶ä¸­1è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0è¡¨ç¤ºå®Œå…¨ä¸åŒ
                        distance = distances[i]
                        similarity = 1 - (distance / 2.0)
                        
                        sentences.append({
                            "text": documents[i],
                            "distance": distance,
                            "similarity": similarity,
                            "metadata": metadatas[i]
                        })
                else:
                    logger.warning(f"âš ï¸ DOI {doi}: æŸ¥è¯¢è¿”å›ç©ºç»“æœ")
                
                doi_to_sentences[doi] = sentences
                
            except Exception as e:
                logger.warning(f"âš ï¸ æŸ¥è¯¢DOIå¥å­å¤±è´¥ ({doi}): {e}")
                doi_to_sentences[doi] = []
        
        return doi_to_sentences
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        self._embedding_cache.clear()
        self._similarity_cache.clear()
        logger.info("âœ… ç¼“å­˜å·²æ¸…é™¤")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            "embedding_cache_size": len(self._embedding_cache),
            "similarity_cache_size": len(self._similarity_cache)
        }
    
    def _compute_max_sentence_similarity(
        self,
        query: str,
        doi: str,
        doi_sentences: List[Dict]
    ) -> float:
        """
        è®¡ç®—DOIçš„æœ€é«˜å¥å­ç›¸ä¼¼åº¦ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            doi: æ–‡çŒ®DOI
            doi_sentences: è¯¥DOIçš„å¥å­åˆ—è¡¨
            
        Returns:
            æœ€é«˜å¥å­ç›¸ä¼¼åº¦
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(query, doi)
        if cache_key in self._similarity_cache:
            logger.debug(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ç›¸ä¼¼åº¦åˆ†æ•°: {doi}")
            return self._similarity_cache[cache_key]
        
        if not doi_sentences:
            logger.warning(f"âš ï¸ DOI {doi} æ²¡æœ‰å¥å­æ•°æ®")
            return 0.0
        
        # æ‰¾åˆ°æœ€é«˜ç›¸ä¼¼åº¦
        max_similarity = max(s["similarity"] for s in doi_sentences)
        
        # ç¼“å­˜ç»“æœ
        self._similarity_cache[cache_key] = max_similarity
        
        return max_similarity
    
    def rerank(
        self,
        query: str,
        candidates: List[Dict],
        top_k: int = 15
    ) -> RerankingResult:
        """
        é‡æ–°æ’åºå€™é€‰æ–‡çŒ®
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            candidates: å€™é€‰æ–‡çŒ®åˆ—è¡¨
            top_k: è¿”å›çš„æ–‡çŒ®æ•°é‡
            
        Returns:
            é‡æ’åºç»“æœ
        """
        start_time = time.time()
        
        if not candidates:
            logger.warning("âš ï¸ å€™é€‰æ–‡çŒ®åˆ—è¡¨ä¸ºç©º")
            return RerankingResult(
                documents=[],
                similarity_scores={},
                reranking_time=0.0,
                top_3_changes=[]
            )
        
        logger.info(f"ğŸ”„ å¼€å§‹é‡æ’åº: {len(candidates)} ä¸ªå€™é€‰æ–‡çŒ®")
        
        # è®°å½•åŸå§‹æ’åï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        original_ranking = {}
        for i, doc in enumerate(candidates):
            metadata = doc.get("metadata", {})
            doi = metadata.get("doi") or metadata.get("DOI", f"no_doi_{i}")
            # æ¸…ç†DOI
            if doi and not doi.startswith("no_doi_"):
                doi = self._clean_doi(doi)
            original_ranking[doi] = i
        
        try:
            # 1. ç”ŸæˆæŸ¥è¯¢embedding
            query_embedding = self._generate_query_embedding(query)
            
            # 2. æå–æ‰€æœ‰å€™é€‰DOIå¹¶æ¸…ç†
            dois = []
            for doc in candidates:
                metadata = doc.get("metadata", {})
                doi = metadata.get("doi") or metadata.get("DOI")
                if doi:
                    # æ¸…ç†DOIï¼ˆå»æ‰abstractç­‰åç¼€ï¼‰
                    clean_doi = self._clean_doi(doi)
                    dois.append(clean_doi)
            
            if not dois:
                logger.warning("âš ï¸ å€™é€‰æ–‡çŒ®ä¸­æ²¡æœ‰æœ‰æ•ˆçš„DOI")
                # ä¸ºå€™é€‰æ–‡çŒ®æ·»åŠ rerank_scoreï¼ˆä½¿ç”¨åŸå§‹åˆ†æ•°ï¼‰
                for doc in candidates:
                    doc["rerank_score"] = doc.get("score", 0.0)
                return RerankingResult(
                    documents=candidates[:top_k],
                    similarity_scores={},
                    reranking_time=time.time() - start_time,
                    top_3_changes=[]
                )
            
            # 3. æ‰¹é‡æŸ¥è¯¢å¥å­æ•°æ®åº“
            doi_to_sentences = self._batch_query_sentences(
                query_embedding,
                dois,
                n_results_per_doi=50
            )
            
            # 4. è®¡ç®—æ¯ä¸ªDOIçš„æœ€é«˜å¥å­ç›¸ä¼¼åº¦
            similarity_scores = {}
            cache_hits = 0
            cache_misses = 0
            
            for doi, sentences in doi_to_sentences.items():
                cache_key = self._get_cache_key(query, doi)
                if cache_key in self._similarity_cache:
                    cache_hits += 1
                else:
                    cache_misses += 1
                
                max_sim = self._compute_max_sentence_similarity(query, doi, sentences)
                similarity_scores[doi] = max_sim
            
            logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­={cache_hits}, æœªå‘½ä¸­={cache_misses}")
            
            # 5. ä¸ºæ¯ä¸ªå€™é€‰æ–‡çŒ®æ·»åŠ é‡æ’åºåˆ†æ•°
            for doc in candidates:
                metadata = doc.get("metadata", {})
                doi = metadata.get("doi") or metadata.get("DOI")
                if doi:
                    # æ¸…ç†DOIåå†æŸ¥æ‰¾ç›¸ä¼¼åº¦
                    clean_doi = self._clean_doi(doi)
                    if clean_doi in similarity_scores:
                        doc["rerank_score"] = similarity_scores[clean_doi]
                    else:
                        # å¦‚æœæ²¡æœ‰å¥å­æ•°æ®ï¼Œä½¿ç”¨åŸå§‹åˆ†æ•°
                        doc["rerank_score"] = doc.get("score", 0.0)
                else:
                    # å¦‚æœæ²¡æœ‰å¥å­æ•°æ®ï¼Œä½¿ç”¨åŸå§‹åˆ†æ•°
                    doc["rerank_score"] = doc.get("score", 0.0)
            
            # 6. æŒ‰é‡æ’åºåˆ†æ•°é™åºæ’åˆ—
            reranked_docs = sorted(
                candidates,
                key=lambda x: x.get("rerank_score", 0.0),
                reverse=True
            )
            
            # 7. è®¡ç®—top-3çš„æ’åå˜åŒ–
            top_3_changes = []
            for i in range(min(3, len(reranked_docs))):
                doc = reranked_docs[i]
                metadata = doc.get("metadata", {})
                doi = metadata.get("doi") or metadata.get("DOI", f"no_doi_{i}")
                # æ¸…ç†DOI
                if doi and not doi.startswith("no_doi_"):
                    doi = self._clean_doi(doi)
                old_rank = original_ranking.get(doi, -1)
                new_rank = i
                top_3_changes.append((doi, old_rank, new_rank))
            
            # 8. è¿”å›top-kç»“æœ
            final_docs = reranked_docs[:top_k]
            
            reranking_time = time.time() - start_time
            
            logger.info(f"âœ… é‡æ’åºå®Œæˆ:")
            logger.info(f"   - å€™é€‰æ•°é‡: {len(candidates)}")
            logger.info(f"   - è¿”å›æ•°é‡: {len(final_docs)}")
            logger.info(f"   - è€—æ—¶: {reranking_time:.2f}s")
            logger.info(f"   - Top-3å˜åŒ–: {top_3_changes}")
            
            return RerankingResult(
                documents=final_docs,
                similarity_scores=similarity_scores,
                reranking_time=reranking_time,
                top_3_changes=top_3_changes
            )
            
        except Exception as e:
            logger.error(f"âŒ é‡æ’åºå¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ’åºï¼Œæ·»åŠ rerank_score
            for doc in candidates:
                doc["rerank_score"] = doc.get("score", 0.0)
            return RerankingResult(
                documents=candidates[:top_k],
                similarity_scores={},
                reranking_time=time.time() - start_time,
                top_3_changes=[]
            )
