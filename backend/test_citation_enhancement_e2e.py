"""
ç«¯åˆ°ç«¯æµ‹è¯•ï¼šå¼•ç”¨ä½ç½®å¢å¼ºåŠŸèƒ½
è¿è¡Œç¯å¢ƒ: cd backend && conda run -n py310 python test_citation_enhancement_e2e.py
"""
import sys
import os

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, backend_dir)

from agents.experts.semantic_expert import SemanticExpert
from repositories.vector_repository import VectorRepository
from services.llm_service import LLMService


def test_citation_enhancement():
    """æµ‹è¯•å¼•ç”¨ä½ç½®å¢å¼ºåŠŸèƒ½"""
    print("="*80)
    print("ğŸ§ª å¼•ç”¨ä½ç½®å¢å¼ºåŠŸèƒ½ - ç«¯åˆ°ç«¯æµ‹è¯•")
    print("="*80)
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("\nğŸ“¦ åˆå§‹åŒ–ç»„ä»¶...")
    vector_repo = VectorRepository()
    llm_service = LLMService()
    semantic_expert = SemanticExpert(vector_repo=vector_repo, llm_service=llm_service)
    
    # æµ‹è¯•é—®é¢˜
    question = "ç£·é…¸é“é”‚çš„ç”µå‹æ˜¯å¤šå°‘"
    print(f"\nâ“ æµ‹è¯•é—®é¢˜: {question}")
    
    # æ‰§è¡ŒæŸ¥è¯¢
    print("\nğŸ” æ‰§è¡ŒæŸ¥è¯¢...")
    result = semantic_expert.query_with_details(question, top_k=20, load_pdf=False)
    
    # æ£€æŸ¥ç»“æœ
    print("\nğŸ“Š æ£€æŸ¥ç»“æœ...")
    
    answer = result.get('answer', '')
    doi_locations = result.get('doi_locations', {})
    pdf_info = result.get('pdf_info', {})
    
    print(f"\nâœ… ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
    print(f"âœ… æ‰¾åˆ°çš„æ–‡çŒ®æ•°: {pdf_info.get('documents_found', 0)}")
    print(f"âœ… DOIä½ç½®æ˜ å°„æ•°: {len(doi_locations)}")
    
    # æ˜¾ç¤ºdoi_locationsè¯¦æƒ…
    if doi_locations:
        print(f"\nğŸ“ å¼•ç”¨ä½ç½®è¯¦æƒ…:")
        for doi, locations in doi_locations.items():
            print(f"\n  DOI: {doi}")
            print(f"  å¼•ç”¨ä½ç½®æ•°: {len(locations)}")
            for i, loc in enumerate(locations[:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ª
                print(f"\n    [{i}] é¡µç : {loc.get('page')}, æ®µè½: {loc.get('chunk_index_in_page')}")
                print(f"        ç›¸ä¼¼åº¦: {loc.get('similarity'):.3f}")
                print(f"        ç½®ä¿¡åº¦: {loc.get('confidence')}")
                print(f"        ç­”æ¡ˆå¥å­: {loc.get('answer_sentence', '')[:50]}...")
                print(f"        åŸæ–‡ç‰‡æ®µ: {loc.get('source_text', '')[:50]}...")
                if loc.get('has_number'):
                    print(f"        ğŸ“Š å«æ•°å€¼")
                if loc.get('has_unit'):
                    print(f"        ğŸ“ å«å•ä½")
    else:
        print("\nâš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¼•ç”¨ä½ç½®")
    
    # éªŒè¯è¦†ç›–ç‡
    print(f"\nğŸ“ˆ è¦†ç›–ç‡åˆ†æ:")
    documents_found = pdf_info.get('documents_found', 0)
    if documents_found > 0:
        # æå–å‰5ä¸ªDOIä½œä¸ºå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        search_result = semantic_expert.search(question, top_k=20, with_scores=True)
        if search_result.get('success'):
            reference_dois = []
            for doc in search_result['documents'][:5]:
                meta = doc.get('metadata', {})
                doi = meta.get('doi') or meta.get('DOI')
                if doi and doi != 'N/A':
                    reference_dois.append(doi)
            
            print(f"  å‚è€ƒæ–‡çŒ®åˆ—è¡¨: {len(reference_dois)} ä¸ªDOI")
            print(f"  æœ‰å¼•ç”¨ä½ç½®çš„DOI: {len(doi_locations)} ä¸ª")
            
            covered = len(doi_locations)
            total = len(reference_dois)
            coverage = (covered / total * 100) if total > 0 else 0
            print(f"  è¦†ç›–ç‡: {coverage:.1f}%")
            
            # æ£€æŸ¥å“ªäº›DOIæ²¡æœ‰å¼•ç”¨ä½ç½®
            missing_dois = set(reference_dois) - set(doi_locations.keys())
            if missing_dois:
                print(f"\n  âš ï¸  æœªè¦†ç›–çš„DOI:")
                for doi in missing_dois:
                    print(f"    - {doi}")
            else:
                print(f"\n  âœ… æ‰€æœ‰å‚è€ƒæ–‡çŒ®DOIéƒ½æœ‰å¼•ç”¨ä½ç½®ï¼")
    
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("="*80)


if __name__ == "__main__":
    test_citation_enhancement()
