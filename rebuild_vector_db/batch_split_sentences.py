"""
æ‰¹é‡åˆ‡åˆ† Markdown æ–‡ä»¶ä¸ºå¥å­å¹¶ä¿å­˜ä¸º JSON

æ‰«ææ¸…æ´—åçš„ Markdown ç›®å½•ï¼Œå¯¹æ¯ä¸ªæ–‡ä»¶è°ƒç”¨ SentenceSplitter.split()
ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶ï¼šrebuild_vector_db/sentences_data/{filename}_sentences.json

è¿è¡Œæ–¹å¼ï¼š
    conda run -n agent python rebuild_vector_db/batch_split_sentences.py
"""

import os
import sys
import json
from pathlib import Path
from tqdm import tqdm
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rebuild_vector_db.sentence_splitter import SentenceSplitter


def batch_split_sentences(
    input_dir: str = "qwen2.5B/output/cleaned",
    output_dir: str = "rebuild_vector_db/sentences_data",
    doi_mapping_file: str = "/mnt/fast18/zhu/LiFeO4Agent/doi_to_pdf_mapping.json",
    skip_existing: bool = True
) -> Dict[str, Any]:
    """
    æ‰¹é‡åˆ‡åˆ† Markdown æ–‡ä»¶ä¸ºå¥å­
    
    Args:
        input_dir: æ¸…æ´—åçš„ Markdown ç›®å½•
        output_dir: JSON è¾“å‡ºç›®å½•
        doi_mapping_file: DOI æ˜ å°„æ–‡ä»¶è·¯å¾„
        skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„ JSON æ–‡ä»¶
        
    Returns:
        Dict: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    input_path = Path(project_root) / input_dir
    output_path = Path(project_root) / output_dir
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æ‰«ææ‰€æœ‰ Markdown æ–‡ä»¶
    md_files = list(input_path.glob("*.md"))
    
    if not md_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {input_path} ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶")
        return {}
    
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {input_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_path}")
    print(f"ğŸ“„ æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
    print(f"ğŸ”„ è·³è¿‡å·²å­˜åœ¨: {'æ˜¯' if skip_existing else 'å¦'}\n")
    
    # åˆå§‹åŒ– SentenceSplitter
    splitter = SentenceSplitter(
        min_sentence_length=10,
        doi_mapping_file=doi_mapping_file,
        filter_references=True
    )
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_files': len(md_files),
        'processed': 0,
        'skipped': 0,
        'failed': 0,
        'total_sentences': 0,
        'errors': []
    }
    
    # æ‰¹é‡å¤„ç†
    print("ğŸš€ å¼€å§‹æ‰¹é‡åˆ‡åˆ†...\n")
    
    for idx, md_file in enumerate(tqdm(md_files, desc="åˆ‡åˆ†è¿›åº¦", unit="æ–‡ä»¶")):
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_file = output_path / f"{md_file.stem}_sentences.json"
            
            # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
            if skip_existing and output_file.exists():
                stats['skipped'] += 1
                continue
            
            # è¯»å– Markdown æ–‡ä»¶
            with open(md_file, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # åˆ‡åˆ†ä¸ºå¥å­
            source = md_file.stem.replace('_cleaned', '')
            sentences = splitter.split(text, source=source)
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            sentences_data = {
                'source': source,
                'total_sentences': len(sentences),
                'filtered_references': True,  # SentenceSplitter é»˜è®¤è¿‡æ»¤ REFERENCES
                'sentences': [sentence.to_dict() for sentence in sentences]
            }
            
            # ä¿å­˜ä¸º JSON
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(sentences_data, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°ç»Ÿè®¡
            stats['processed'] += 1
            stats['total_sentences'] += len(sentences)
            
            # æ¯å¤„ç† 100 ä¸ªæ–‡ä»¶æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (idx + 1) % 100 == 0:
                tqdm.write(f"âœ… å·²å¤„ç† {idx + 1}/{len(md_files)} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {stats['total_sentences']} ä¸ªå¥å­")
            
        except Exception as e:
            stats['failed'] += 1
            error_info = {
                'file': md_file.name,
                'error': str(e)
            }
            stats['errors'].append(error_info)
            tqdm.write(f"âŒ å¤„ç†å¤±è´¥: {md_file.name} - {str(e)}")
    
    # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š æ‰¹é‡åˆ‡åˆ†å®Œæˆï¼")
    print("="*80)
    print(f"ğŸ“„ æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"âœ… æˆåŠŸå¤„ç†: {stats['processed']} ä¸ªæ–‡ä»¶")
    print(f"â­ï¸  è·³è¿‡: {stats['skipped']} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤±è´¥: {stats['failed']} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ æ€»å¥å­æ•°: {stats['total_sentences']}")
    if stats['processed'] > 0:
        print(f"ğŸ“Š å¹³å‡æ¯æ–‡ä»¶: {stats['total_sentences'] / stats['processed']:.1f} ä¸ªå¥å­")
    print("="*80)
    print(f"\nğŸ’¾ JSON æ–‡ä»¶ä¿å­˜åœ¨: {output_path}")
    
    if stats['errors']:
        print(f"\nâš ï¸  é”™è¯¯è¯¦æƒ…:")
        for error in stats['errors'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
            print(f"  - {error['file']}: {error['error']}")
        if len(stats['errors']) > 10:
            print(f"  ... è¿˜æœ‰ {len(stats['errors']) - 10} ä¸ªé”™è¯¯")
    
    return stats


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ‰¹é‡åˆ‡åˆ† Markdown æ–‡ä»¶ä¸ºå¥å­")
    parser.add_argument(
        "--input-dir",
        default="qwen2.5B/output/cleaned",
        help="è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤: qwen2.5B/output/cleanedï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        default="rebuild_vector_db/sentences_data",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: rebuild_vector_db/sentences_dataï¼‰"
    )
    parser.add_argument(
        "--doi-mapping",
        default="/mnt/fast18/zhu/LiFeO4Agent/doi_to_pdf_mapping.json",
        help="DOI æ˜ å°„æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--no-skip-existing",
        action="store_true",
        help="ä¸è·³è¿‡å·²å­˜åœ¨çš„ JSON æ–‡ä»¶ï¼ˆé‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼‰"
    )
    
    args = parser.parse_args()
    
    batch_split_sentences(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        doi_mapping_file=args.doi_mapping,
        skip_existing=not args.no_skip_existing
    )
