#!/usr/bin/env python3
"""
å®Œæ•´å‘é‡æ•°æ®åº“æ„å»ºè„šæœ¬
ä» papers/ ç›®å½•çš„æ‰€æœ‰ PDF ç”Ÿæˆæ‘˜è¦å’Œ embeddingï¼Œå¹¶å¯¼å…¥åˆ° ChromaDB
"""
import json
import os
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import requests
import chromadb
from chromadb.config import Settings
import time
from threading import Semaphore

try:
    import fitz  # PyMuPDF
    PDF_AVAILABLE = True
except ImportError:
    print("âŒ PyMuPDF æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: conda install -n agent pymupdf")
    exit(1)

# ==================== é…ç½® ====================
PAPERS_DIR = Path("/Users/zhuyinghua/Desktop/agent/main/papers")
JSON_DIR = Path("/Users/zhuyinghua/Desktop/agent/main/json")
VECTOR_DB_PATH = "/Users/zhuyinghua/Desktop/agent/main/vector_database"

# BGE Embedding API
BGE_API_URL = "http://hf2d8696.natapp1.cc/v1/embeddings"

# LLM API (é˜¿é‡Œç™¾ç‚¼)
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "")
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
DASHSCOPE_MODEL = "deepseek-v3.1"

# å¹¶å‘é…ç½®
MAX_WORKERS = 1  # å®Œå…¨é¡ºåºå¤„ç†,é¿å…APIé™æµ
BATCH_SIZE = 100  # ChromaDB æ‰¹é‡æ’å…¥å¤§å°

# API é™æµæ§åˆ¶  
API_DELAY = 2.0  # æ¯ä¸ªè¯·æ±‚ä¹‹é—´å»¶è¿Ÿ 2 ç§’


# ==================== PDF æå– ====================
def extract_doi_from_pdf(pdf_path: Path) -> str:
    """ä» PDF æ–‡ä»¶åæå– DOI"""
    filename = pdf_path.stem
    # æ–‡ä»¶åæ ¼å¼: Author_Year_Journal_xxx.pdf
    # DOI åœ¨æ–‡ä»¶åä¸­ç¼–ç ï¼Œéœ€è¦åå‘è§£æ
    doi = filename.replace('_', '/')
    # ç®€åŒ–: ç›´æ¥ä»æ–‡ä»¶åæ„é€  DOI (å®é™…åº”è¯¥ä» PDF å…ƒæ•°æ®æå–)
    return f"10.xxxx/{filename}"  # å ä½ç¬¦


def extract_pdf_text(pdf_path: Path, max_pages: int = 3) -> str:
    """æå– PDF å‰å‡ é¡µæ–‡æœ¬(ç”¨äºæå–æ‘˜è¦å’ŒDOI)"""
    try:
        doc = fitz.open(str(pdf_path))
        text_parts = []
        
        # åªæå–å‰3é¡µ(é€šå¸¸åŒ…å«æ ‡é¢˜ã€æ‘˜è¦ã€DOI)
        for page_num in range(min(max_pages, doc.page_count)):
            page = doc[page_num]
            text = page.get_text()
            if text.strip():
                text_parts.append(text)
        
        doc.close()
        full_text = '\n'.join(text_parts)
        return full_text[:5000]  # é™åˆ¶åœ¨5000å­—ç¬¦
        
    except Exception as e:
        print(f"  âš ï¸ PDF æå–å¤±è´¥ {pdf_path.name}: {e}")
        return ""


def extract_abstract_from_text(text: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå– Abstract éƒ¨åˆ†,è¿”å›ç®€çŸ­æ‘˜è¦"""
    # å°è¯•æå– Abstract éƒ¨åˆ†
    patterns = [
        # è‹±æ–‡ Abstract
        r'(?:ABSTRACT|Abstract)\s*[:\n]\s*(.*?)(?:\n\s*\n|\n(?:INTRODUCTION|Introduction|Keywords|KEY\s*WORDS|1\.|Â©|\d+\.\s+Introduction))',
        # å¦ä¸€ç§æ ¼å¼
        r'(?:ABSTRACT|Abstract)\s*[:\n]\s*(.*?)(?=\n[A-Z][a-z]+:|\n\d+\.|\nÂ©)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            abstract = match.group(1).strip()
            # æ¸…ç†å¤šä½™ç©ºç™½å’Œæ¢è¡Œ
            abstract = re.sub(r'\s+', ' ', abstract)
            # æ¸…ç†å¯èƒ½çš„é¡µç ã€å¼•ç”¨ç­‰
            abstract = re.sub(r'\[\d+\]', '', abstract)
            
            if len(abstract) > 100:  # ç¡®ä¿æå–åˆ°æœ‰æ•ˆå†…å®¹
                # è¿”å›å‰500å­—ç¬¦(å’ŒåŸç‰ˆ JSON æ ¼å¼ä¸€è‡´)
                return abstract[:500].strip()
    
    # å¦‚æœæ²¡æ‰¾åˆ° Abstract,è¿”å›å‰300å­—ç¬¦ä½œä¸ºç®€çŸ­æ‘˜è¦
    clean_text = re.sub(r'\s+', ' ', text)
    return clean_text[:300].strip()


def extract_doi_from_text(text: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå– DOI"""
    patterns = [
        r'DOI[:\s]+(\d+\.\d+/[^\s\]]+)',
        r'doi[:\s]+(\d+\.\d+/[^\s\]]+)',
        r'https?://doi\.org/(\d+\.\d+/[^\s]+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            doi = match.group(1).strip()
            # æ¸…ç†æœ«å°¾çš„æ ‡ç‚¹ç¬¦å·
            doi = re.sub(r'[.,;]$', '', doi)
            return doi
    
    return None


# ==================== LLM æ‘˜è¦ç”Ÿæˆ ====================
# æ³¨é‡Šæ‰ LLM ç”Ÿæˆ,ç›´æ¥ä½¿ç”¨ PDF æå–çš„ Abstract
# def generate_summary_with_llm(pdf_text: str, doi: str) -> str:
#     """ä½¿ç”¨ LLM ç”Ÿæˆæ–‡çŒ®æ‘˜è¦"""
#     pass


# ==================== Embedding ç”Ÿæˆ ====================
def generate_embedding(text: str, retry_count=5) -> list:
    """è°ƒç”¨ BGE API ç”Ÿæˆ embedding (å¸¦é‡è¯•)"""
    
    for attempt in range(retry_count):
        try:
            # æ·»åŠ å»¶è¿Ÿé¿å…é™æµ
            time.sleep(API_DELAY)
            
            response = requests.post(
                BGE_API_URL,
                json={
                    "input": [text]
                },
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()["data"][0]["embedding"]
            elif response.status_code == 429:
                # é‡åˆ°é™æµ,æŒ‡æ•°é€€é¿
                wait_time = (2 ** attempt) * 5  # 5, 10, 20, 40, 80 ç§’
                print(f"  âš ï¸ API é™æµ (429), {wait_time}ç§’åé‡è¯• (ç¬¬{attempt+1}/{retry_count}æ¬¡)")
                time.sleep(wait_time)
                continue
            else:
                print(f"  âš ï¸ Embedding å¤±è´¥: {response.status_code}")
                if attempt < retry_count - 1:
                    time.sleep(3)
                    continue
                return None
                
        except Exception as e:
            print(f"  âš ï¸ Embedding é”™è¯¯: {e}")
            if attempt < retry_count - 1:
                time.sleep(3)
                continue
            return None
    
    return None


# ==================== å¤„ç†å•ä¸ª PDF ====================
def process_single_pdf(pdf_path: Path) -> dict:
    """å¤„ç†å•ä¸ª PDF: æå–æ–‡æœ¬ -> ç”Ÿæˆæ‘˜è¦ -> ç”Ÿæˆ embedding"""
    
    # æ£€æŸ¥ JSON æ˜¯å¦å·²å­˜åœ¨
    json_filename = pdf_path.stem + "_summary_embedding.json"
    json_path = JSON_DIR / json_filename
    
    if json_path.exists():
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return {
                "status": "exists",
                "data": data[0] if isinstance(data, list) else data,
                "pdf": pdf_path.name
            }
        except Exception as e:
            print(f"  âš ï¸ JSON è¯»å–å¤±è´¥ {json_filename}: {e}")
    
    # æå– PDF æ–‡æœ¬
    pdf_text = extract_pdf_text(pdf_path)
    if not pdf_text:
        return {"status": "error", "pdf": pdf_path.name, "error": "PDFæ–‡æœ¬æå–å¤±è´¥"}
    
    # æå– DOI
    doi = extract_doi_from_text(pdf_text)
    if not doi:
        # ä»æ–‡ä»¶åçŒœæµ‹
        doi = f"unknown/{pdf_path.stem}"
    
    # æå– Abstract ä½œä¸ºç®€çŸ­æ‘˜è¦(ä¸è°ƒç”¨ LLM)
    abstract = extract_abstract_from_text(pdf_text)
    
    # æ„é€ æ‘˜è¦æ–‡æœ¬(å’ŒåŸç‰ˆ JSON æ ¼å¼ä¸€è‡´)
    summary = f"[DOI: {doi}] {abstract}"
    
    # ç”Ÿæˆ embedding
    embedding = generate_embedding(summary)
    if not embedding:
        return {"status": "error", "pdf": pdf_path.name, "error": "Embeddingç”Ÿæˆå¤±è´¥"}
    
    # ä¿å­˜ JSON
    data = {
        "text": summary,
        "embedding": embedding,
        "metadata": {
            "source_file": pdf_path.name,
            "doi": doi
        }
    }
    
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump([data], f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"  âš ï¸ JSON ä¿å­˜å¤±è´¥: {e}")
    
    return {
        "status": "success",
        "data": data,
        "pdf": pdf_path.name
    }


# ==================== ä¸»æµç¨‹ ====================
def main():
    print("=" * 80)
    print("ğŸš€ å®Œæ•´å‘é‡æ•°æ®åº“æ„å»º")
    print("=" * 80)
    
    # æ£€æŸ¥ç›®å½•
    if not PAPERS_DIR.exists():
        print(f"âŒ papers ç›®å½•ä¸å­˜åœ¨: {PAPERS_DIR}")
        return
    
    JSON_DIR.mkdir(exist_ok=True)
    
    # è·å–æ‰€æœ‰ PDF
    pdf_files = sorted(PAPERS_DIR.glob("*.pdf"))
    print(f"ğŸ“„ æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
    
    if len(pdf_files) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        return
    
    # æ£€æŸ¥å·²æœ‰ JSON - å…¨éƒ¨å¿½ç•¥,é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡çŒ®
    existing_jsons = set()
    print(f"ğŸ“¦ å°†é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡çŒ®çš„å‘é‡æ•°æ®")
    
    # å¤„ç†æ‰€æœ‰ PDF
    pdfs_to_process = pdf_files
    print(f"ğŸ”¨ éœ€è¦å¤„ç† {len(pdfs_to_process)} ä¸ª PDF")
    
    if len(pdfs_to_process) == 0:
        print("âŒ æ²¡æœ‰ PDF éœ€è¦å¤„ç†")
        return
    else:
        print(f"\nå¼€å§‹å¤„ç† PDF (å¹¶å‘: {MAX_WORKERS})...")
        
        results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {executor.submit(process_single_pdf, pdf): pdf for pdf in pdfs_to_process}
            
            with tqdm(total=len(pdfs_to_process), desc="å¤„ç†è¿›åº¦") as pbar:
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
                    pbar.update(1)
                    
                    if result["status"] == "error":
                        tqdm.write(f"  âŒ {result['pdf']}: {result.get('error', 'Unknown')}")
        
        # ç»Ÿè®¡
        success_count = sum(1 for r in results if r["status"] == "success")
        exists_count = sum(1 for r in results if r["status"] == "exists")
        error_count = sum(1 for r in results if r["status"] == "error")
        
        print(f"\nå¤„ç†å®Œæˆ:")
        print(f"  âœ… æ–°ç”Ÿæˆ: {success_count}")
        print(f"  ğŸ“¦ å·²å­˜åœ¨: {exists_count}")
        print(f"  âŒ å¤±è´¥: {error_count}")
    
    # å¯¼å…¥åˆ° ChromaDB
    print(f"\n{'='*80}")
    print("ğŸ“Š å¯¼å…¥æ•°æ®åˆ° ChromaDB")
    print("=" * 80)
    
    # è¯»å–æ‰€æœ‰ JSON
    json_files = sorted(JSON_DIR.glob("*_summary_embedding.json"))
    print(f"ğŸ“„ æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ–‡ä»¶")
    
    # åˆå§‹åŒ– ChromaDB
    client = chromadb.PersistentClient(
        path=VECTOR_DB_PATH,
        settings=Settings(anonymized_telemetry=False)
    )
    
    # åˆ é™¤æ—§é›†åˆ(ä½¿ç”¨åŸç‰ˆé›†åˆå)
    try:
        client.delete_collection("lfp_papers")
        print("ğŸ—‘ï¸  å·²åˆ é™¤æ—§é›†åˆ: lfp_papers")
    except Exception:
        pass
    
    # åˆ›å»ºæ–°é›†åˆ(ä½¿ç”¨åŸç‰ˆçš„é›†åˆå)
    collection = client.create_collection(
        name="lfp_papers",
        metadata={"hnsw:space": "cosine"}
    )
    
    # æ‰¹é‡å¯¼å…¥
    documents = []
    embeddings = []
    metadatas = []
    ids = []
    total_imported = 0
    
    for i, json_file in enumerate(tqdm(json_files, desc="å¯¼å…¥è¿›åº¦")):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            item = data[0] if isinstance(data, list) else data
            text = item.get('text', '')
            embedding = item.get('embedding', [])
            metadata = item.get('metadata', {})
            
            if not text or not embedding:
                continue
            
            doc_id = f"{json_file.stem}_{i}"
            documents.append(text)
            embeddings.append(embedding)
            metadatas.append({
                **metadata,
                'source_file': json_file.name
            })
            ids.append(doc_id)
            
            # æ‰¹é‡æ’å…¥
            if len(documents) >= BATCH_SIZE:
                collection.add(
                    documents=documents,
                    embeddings=embeddings,
                    metadatas=metadatas,
                    ids=ids
                )
                total_imported += len(documents)
                documents = []
                embeddings = []
                metadatas = []
                ids = []
                
        except Exception as e:
            tqdm.write(f"  âš ï¸ å¤„ç†å¤±è´¥ {json_file.name}: {e}")
    
    # æ’å…¥å‰©ä½™æ•°æ®
    if documents:
        collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )
        total_imported += len(documents)
    
    final_count = collection.count()
    print(f"\nâœ… å¯¼å…¥å®Œæˆ!")
    print(f"   æœ€ç»ˆæ–‡æ¡£æ•°: {final_count}")
    print(f"   æ•°æ®åº“è·¯å¾„: {VECTOR_DB_PATH}")
    
    # ChromaDB PersistentClient ä¸éœ€è¦æ‰‹åŠ¨å…³é—­
    
    print("\n" + "=" * 80)
    print("ğŸ‰ å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ!")
    print("=" * 80)


if __name__ == '__main__':
    main()
