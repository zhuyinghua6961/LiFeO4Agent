"""
éªŒè¯è·¨æ•°æ®åº“å®šä½èƒ½åŠ›
ä½¿ç”¨ conda ç¯å¢ƒ py310 è¿è¡Œ: conda run -n py310 python verify_cross_db_location.py
"""
import sys
sys.path.append('./backend')

import chromadb
from chromadb.config import Settings
import json

print("="*80)
print("éªŒè¯è·¨æ•°æ®åº“å®šä½èƒ½åŠ›")
print("="*80)

# 1. åˆå§‹åŒ–æ®µè½çº§æ•°æ®åº“ (v3)
print("\nã€1ã€‘åˆå§‹åŒ–æ®µè½çº§æ•°æ®åº“")
print("-"*80)
paragraph_db_path = './vector_database_v3'
paragraph_collection_name = 'lfp_papers_v3'

print(f"ğŸ“‚ è·¯å¾„: {paragraph_db_path}")
print(f"ğŸ“¦ Collection: {paragraph_collection_name}")

client_paragraph = chromadb.PersistentClient(
    path=paragraph_db_path,
    settings=Settings(anonymized_telemetry=False)
)
paragraph_collection = client_paragraph.get_collection(paragraph_collection_name)
print(f"âœ… æ®µè½æ•°é‡: {paragraph_collection.count():,}")

# 2. åˆå§‹åŒ–å¥å­çº§æ•°æ®åº“
print("\nã€2ã€‘åˆå§‹åŒ–å¥å­çº§æ•°æ®åº“")
print("-"*80)
sentence_db_path = './vector_sentence'
sentence_collection_name = 'lfp_papers_sentences_v1'

print(f"ğŸ“‚ è·¯å¾„: {sentence_db_path}")
print(f"ğŸ“¦ Collection: {sentence_collection_name}")

client_sentence = chromadb.PersistentClient(
    path=sentence_db_path,
    settings=Settings(anonymized_telemetry=False)
)
sentence_collection = client_sentence.get_collection(sentence_collection_name)
print(f"âœ… å¥å­æ•°é‡: {sentence_collection.count():,}")

# 3. æµ‹è¯•æ®µè½çº§æ•°æ®åº“çš„å®šä½èƒ½åŠ›
print("\nã€3ã€‘æµ‹è¯•æ®µè½çº§æ•°æ®åº“å®šä½èƒ½åŠ›")
print("-"*80)

# è·å–ä¸€ä¸ªæœ‰æ•ˆDOI
result = paragraph_collection.get(limit=100, include=['metadatas'])
valid_doi = None
for meta in result['metadatas']:
    doi = meta.get('doi')
    if doi and doi != 'unknown_doi':
        valid_doi = doi
        break

if valid_doi:
    print(f"âœ… æµ‹è¯•DOI: {valid_doi}")
    
    # æŸ¥è¯¢è¯¥DOIçš„æ‰€æœ‰æ®µè½
    doi_paragraphs = paragraph_collection.get(
        where={"doi": valid_doi},
        limit=20,
        include=['metadatas', 'documents']
    )
    
    print(f"âœ… è¯¥DOIå…±æœ‰ {len(doi_paragraphs['ids'])} ä¸ªæ®µè½ï¼ˆæ˜¾ç¤ºå‰20ä¸ªï¼‰")
    
    # æ£€æŸ¥é¡µç 
    pages = [meta.get('page') for meta in doi_paragraphs['metadatas']]
    print(f"âœ… é¡µç èŒƒå›´: {min(pages)} - {max(pages)}")
    
    # æ£€æŸ¥chunkä¿¡æ¯
    chunk_info = doi_paragraphs['metadatas'][0]
    print(f"\nç¤ºä¾‹æ®µè½è¯¦æƒ…:")
    print(f"  - chunk_id: {chunk_info.get('chunk_id')}")
    print(f"  - é¡µç : {chunk_info.get('page')}")
    print(f"  - é¡µå†…æ®µè½ç´¢å¼•: {chunk_info.get('chunk_index_in_page')}")
    print(f"  - é¡µå†…æ€»æ®µè½æ•°: {chunk_info.get('total_chunks_in_page')}")
    print(f"  - å…¨å±€æ®µè½ç´¢å¼•: {chunk_info.get('chunk_index_global')}")
    print(f"  - æ–‡ä»¶å: {chunk_info.get('filename')}")
    print(f"  - å†…å®¹é•¿åº¦: {len(doi_paragraphs['documents'][0])} å­—ç¬¦")
    print(f"  - å†…å®¹é¢„è§ˆ: {doi_paragraphs['documents'][0][:150]}...")
    
    # æµ‹è¯•é€šè¿‡é¡µç å®šä½
    test_page = pages[0]
    page_paragraphs = paragraph_collection.get(
        where={"$and": [
            {"doi": valid_doi},
            {"page": test_page}
        ]},
        limit=10,
        include=['metadatas']
    )
    print(f"\nâœ… é€šè¿‡DOI+é¡µç ({test_page})å®šä½: æ‰¾åˆ° {len(page_paragraphs['ids'])} ä¸ªæ®µè½")
    
    # æ˜¾ç¤ºè¯¥é¡µçš„æ‰€æœ‰æ®µè½ç´¢å¼•
    page_chunk_indices = [meta.get('chunk_index_in_page') for meta in page_paragraphs['metadatas']]
    print(f"   é¡µå†…æ®µè½ç´¢å¼•: {sorted(page_chunk_indices)}")
else:
    print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆDOI")
    sys.exit(1)

# 4. æµ‹è¯•å¥å­çº§æ•°æ®åº“çš„å®šä½èƒ½åŠ›
print("\nã€4ã€‘æµ‹è¯•å¥å­çº§æ•°æ®åº“å®šä½èƒ½åŠ›")
print("-"*80)

# ä½¿ç”¨ç›¸åŒçš„DOI
doi_sentences = sentence_collection.get(
    where={"DOI": valid_doi},
    limit=20,
    include=['metadatas', 'documents']
)

if doi_sentences['ids']:
    print(f"âœ… è¯¥DOIå…±æœ‰å¥å­ï¼ˆæ˜¾ç¤ºå‰20ä¸ªï¼‰: {len(doi_sentences['ids'])} ä¸ª")
    
    # æ£€æŸ¥å¥å­ç´¢å¼•
    sentence_indices = [meta.get('sentence_index') for meta in doi_sentences['metadatas']]
    print(f"âœ… å¥å­ç´¢å¼•èŒƒå›´: {min(sentence_indices)} - {max(sentence_indices)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åº
    is_ordered = all(sentence_indices[i] <= sentence_indices[i+1] for i in range(len(sentence_indices)-1))
    print(f"âœ… å¥å­ç´¢å¼•æ˜¯å¦æœ‰åº: {'æ˜¯' if is_ordered else 'å¦'}")
    
    # ç¤ºä¾‹å¥å­
    sentence_meta = doi_sentences['metadatas'][0]
    print(f"\nç¤ºä¾‹å¥å­è¯¦æƒ…:")
    print(f"  - DOI: {sentence_meta.get('DOI')}")
    print(f"  - å¥å­ç´¢å¼•: {sentence_meta.get('sentence_index')}")
    print(f"  - åŒ…å«æ•°å€¼: {sentence_meta.get('has_number')}")
    print(f"  - åŒ…å«å•ä½: {sentence_meta.get('has_unit')}")
    print(f"  - å•è¯æ•°: {sentence_meta.get('word_count')}")
    print(f"  - å†…å®¹: {doi_sentences['documents'][0][:150]}...")
    
    # æµ‹è¯•é€šè¿‡å¥å­ç´¢å¼•å®šä½
    test_index = sentence_indices[0]
    index_sentences = sentence_collection.get(
        where={"$and": [
            {"DOI": valid_doi},
            {"sentence_index": test_index}
        ]},
        limit=5,
        include=['metadatas']
    )
    print(f"\nâœ… é€šè¿‡DOI+å¥å­ç´¢å¼•({test_index})å®šä½: æ‰¾åˆ° {len(index_sentences['ids'])} ä¸ªå¥å­")
else:
    print("âŒ åœ¨å¥å­çº§æ•°æ®åº“ä¸­æœªæ‰¾åˆ°è¯¥DOI")
    sys.exit(1)

# 5. æµ‹è¯•è·¨æ•°æ®åº“å®šä½ï¼ˆå…³é”®æµ‹è¯•ï¼‰
print("\nã€5ã€‘æµ‹è¯•è·¨æ•°æ®åº“å®šä½ï¼ˆå¥å­â†’æ®µè½â†’é¡µç ï¼‰")
print("-"*80)

# ä»å¥å­çº§æ•°æ®åº“è·å–ä¸€ä¸ªå¥å­
test_sentence = doi_sentences['documents'][0]
test_sentence_meta = doi_sentences['metadatas'][0]

print(f"å¥å­çº§æ•°æ®åº“:")
print(f"  - DOI: {test_sentence_meta['DOI']}")
print(f"  - å¥å­ç´¢å¼•: {test_sentence_meta['sentence_index']}")
print(f"  - å¥å­å†…å®¹: {test_sentence[:100]}...")

# åœ¨æ®µè½çº§æ•°æ®åº“ä¸­æŸ¥æ‰¾åŒ…å«è¯¥å¥å­çš„æ®µè½
found_paragraph = None
found_page = None

for i, para_text in enumerate(doi_paragraphs['documents']):
    # æ£€æŸ¥å¥å­çš„å‰50ä¸ªå­—ç¬¦æ˜¯å¦åœ¨æ®µè½ä¸­
    if test_sentence[:50] in para_text:
        found_paragraph = i
        found_page = doi_paragraphs['metadatas'][i].get('page')
        chunk_index = doi_paragraphs['metadatas'][i].get('chunk_index_in_page')
        print(f"\nâœ… åœ¨æ®µè½çº§æ•°æ®åº“ä¸­æ‰¾åˆ°åŒ¹é…!")
        print(f"   - æ®µè½ç´¢å¼•: {i}")
        print(f"   - é¡µç : {found_page}")
        print(f"   - é¡µå†…æ®µè½ç´¢å¼•: {chunk_index}")
        print(f"   - chunk_id: {doi_paragraphs['metadatas'][i].get('chunk_id')}")
        break

if found_paragraph is not None:
    print(f"\nâœ… è·¨æ•°æ®åº“å®šä½æˆåŠŸ!")
    print(f"   å¥å­ç´¢å¼• {test_sentence_meta['sentence_index']} â†’ é¡µç  {found_page}")
else:
    print(f"\nâš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯æ–‡æœ¬å¤„ç†å·®å¼‚ï¼‰")

# 6. æ€»ç»“
print("\n" + "="*80)
print("éªŒè¯ç»“æœæ€»ç»“")
print("="*80)

print(f"\nâœ… æ®µè½çº§æ•°æ®åº“ (vector_database_v3/lfp_papers_v3):")
print(f"   - æ”¯æŒé€šè¿‡DOIæŸ¥è¯¢æ®µè½")
print(f"   - æ”¯æŒé€šè¿‡DOI+é¡µç ç²¾ç¡®å®šä½")
print(f"   - åŒ…å«å®Œæ•´çš„é¡µç ä¿¡æ¯ (page)")
print(f"   - åŒ…å«æ®µè½ç´¢å¼•ä¿¡æ¯ (chunk_index_in_page, chunk_index_global)")
print(f"   - åŒ…å«ä¸Šä¸‹æ–‡é“¾æ¥ (prev_chunk_id, next_chunk_id)")

print(f"\nâœ… å¥å­çº§æ•°æ®åº“ (vector_sentence/lfp_papers_sentences_v1):")
print(f"   - æ”¯æŒé€šè¿‡DOIæŸ¥è¯¢å¥å­")
print(f"   - æ”¯æŒé€šè¿‡DOI+å¥å­ç´¢å¼•ç²¾ç¡®å®šä½")
print(f"   - åŒ…å«å¥å­ç´¢å¼•ä¿¡æ¯ (sentence_index)")
print(f"   - åŒ…å«æ•°å€¼/å•ä½æ ‡è®° (has_number, has_unit)")
print(f"   - å¥å­ç´¢å¼•æœ‰åºæ’åˆ—")

print(f"\nâœ… è·¨æ•°æ®åº“å®šä½:")
print(f"   - å¯ä»¥ä»å¥å­çº§æ•°æ®åº“å®šä½åˆ°æ®µè½çº§æ•°æ®åº“")
print(f"   - å¯ä»¥è·å–å‡†ç¡®çš„é¡µç ä¿¡æ¯")
print(f"   - å¯ä»¥è·å–æ®µè½åœ¨é¡µé¢ä¸­çš„ä½ç½®")

print(f"\nğŸ¯ ç»“è®º: æ•°æ®åº“å®Œå…¨æ”¯æŒå¼•ç”¨ä½ç½®å®šä½åŠŸèƒ½!")
print("="*80)
