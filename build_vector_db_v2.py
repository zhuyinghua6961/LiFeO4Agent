#!/usr/bin/env python3
"""
V2.0 å‘é‡æ•°æ®åº“æ„å»ºè„šæœ¬
é€’å½’åˆ‡ç‰‡ + é¡µé¢é”šç‚¹ç­–ç•¥ï¼Œæ”¯æŒåŸæ–‡é«˜äº®å’Œç²¾å‡†è·³è½¬

ç‰¹æ€§:
- å…¨ç¯‡ PDF å¤„ç†ï¼ˆéä»…å‰3é¡µï¼‰
- é€’å½’è¯­ä¹‰åˆ‡ç‰‡ (600å­—ç¬¦ï¼Œé‡å 100)
- å…ƒæ•°æ®ç»‘å®š DOI + é¡µç  + åŸæ–‡ç‰‡æ®µ
- æ”¯æŒåŒæ æ’ç‰ˆè¯†åˆ«
"""
import os
import re
import json
import time
import requests
import fitz  # PyMuPDF
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm
from uuid import uuid4
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- é…ç½®åŒº ---
# PDF æ–‡ä»¶å¤¹è·¯å¾„
PDF_DIR = "/Users/zhuyinghua/Desktop/agent/main/papers"
# DOI æ˜ å°„æ–‡ä»¶è·¯å¾„
DOI_MAPPING_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), "doi_to_pdf_mapping.json")
# ChromaDB æŒä¹…åŒ–è·¯å¾„
CHROMA_DB_PATH = os.path.dirname(__file__)
# æ–°çš„é›†åˆåç§°
COLLECTION_NAME = "lfp_papers_v2"
# BGE æœåŠ¡åœ°å€
BGE_API_URL = "http://localhost:8001/v1/embeddings"
# åˆ‡ç‰‡å‚æ•°
CHUNK_SIZE = 600
CHUNK_OVERLAP = 100
# æ‰¹å¤„ç†å¤§å°
BATCH_SIZE = 32


def get_embeddings(texts: list) -> list:
    """è°ƒç”¨ BGE æœåŠ¡è·å–å‘é‡"""
    if not texts:
        return []
    
    try:
        response = requests.post(
            BGE_API_URL,
            json={"input": texts},
            timeout=120
        )
        response.raise_for_status()
        data = response.json()["data"]
        return [item["embedding"] for item in data]
    except Exception as e:
        logger.error(f"Embedding API é”™è¯¯: {e}")
        # é”™è¯¯æ—¶è¿”å›é›¶å‘é‡ï¼Œé¿å…ç¨‹åºå´©æºƒ
        return [[0.0] * 1024 for _ in texts]


def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡æœ¬"""
    # ä¿®å¤è·¨è¡Œæ–­è¯
    text = text.replace("-\n", "").replace("\n", " ")
    # å‹ç¼©å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def process_single_pdf(filepath: str, filename: str, doi: str) -> list:
    """
    å¤„ç†å•ä¸ª PDF æ–‡ä»¶
    
    Args:
        filepath: PDF æ–‡ä»¶å®Œæ•´è·¯å¾„
        filename: æ–‡ä»¶å
        doi: DOI æ ‡è¯†ç¬¦
        
    Returns:
        åˆ‡ç‰‡åˆ—è¡¨
    """
    chunks = []
    
    try:
        doc = fitz.open(filepath)
        
        for page_index, page in enumerate(doc):
            # æå–æ–‡æœ¬ï¼Œsort=True è§£å†³åŒæ æ’ç‰ˆé—®é¢˜
            raw_text = page.get_text("text", sort=True)
            clean_text_str = clean_text(raw_text)
            
            # è·³è¿‡ç©ºç™½é¡µæˆ–å†…å®¹è¿‡å°‘çš„é¡µé¢
            if len(clean_text_str) < 50:
                continue
            
            # é€’å½’åˆ‡åˆ†
            text_chunks = text_splitter.split_text(clean_text_str)
            
            for chunk in text_chunks:
                if len(chunk) < 30:  # è·³è¿‡å¤ªçŸ­çš„ç¢ç‰‡
                    continue
                    
                record = {
                    "id": str(uuid4()),
                    "text": chunk,
                    "metadata": {
                        "doi": doi,
                        "filename": filename,
                        "page": page_index + 1,  # PDF é¡µç ä»1å¼€å§‹
                        "source_text": chunk[:300] if len(chunk) > 300 else chunk,
                        "type": "content"
                    }
                }
                chunks.append(record)
        
        doc.close()
        
    except Exception as e:
        logger.error(f"å¤„ç† PDF å¤±è´¥ {filename}: {e}")
    
    return chunks


def load_doi_mapping() -> dict:
    """åŠ è½½ DOI æ˜ å°„"""
    if os.path.exists(DOI_MAPPING_FILE):
        with open(DOI_MAPPING_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def main():
    """ä¸»æµç¨‹"""
    global text_splitter
    
    logger.info("=" * 60)
    logger.info("ğŸš€ V2.0 å‘é‡æ•°æ®åº“æ„å»ºç¨‹åºå¯åŠ¨")
    logger.info("=" * 60)
    
    # 1. æ£€æŸ¥ PDF ç›®å½•
    if not os.path.exists(PDF_DIR):
        logger.error(f"PDF ç›®å½•ä¸å­˜åœ¨: {PDF_DIR}")
        return
    
    pdf_files = [f for f in os.listdir(PDF_DIR) if f.endswith('.pdf')]
    logger.info(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
    
    if not pdf_files:
        logger.warning("æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
        return
    
    # 2. åŠ è½½ DOI æ˜ å°„
    file_to_doi = load_doi_mapping()
    logger.info(f"ğŸ“‹ åŠ è½½äº† {len(file_to_doi)} ä¸ª DOI æ˜ å°„")
    
    # 3. åˆå§‹åŒ–åˆ‡åˆ†å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    logger.info("âœ‚ï¸ åˆå§‹åŒ–é€’å½’åˆ‡åˆ†å™¨å®Œæˆ")
    
    # 4. åˆå§‹åŒ– ChromaDB
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # åˆ é™¤æ—§é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        client.delete_collection(COLLECTION_NAME)
        logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§é›†åˆ: {COLLECTION_NAME}")
    except:
        pass
    
    collection = client.create_collection(name=COLLECTION_NAME)
    logger.info(f"ğŸ“¦ åˆ›å»ºæ–°é›†åˆ: {COLLECTION_NAME}")
    
    # 5. æ‰¹é‡å¤„ç†
    batch_documents = []
    batch_metadatas = []
    batch_ids = []
    total_chunks = 0
    total_pdfs = 0
    failed_pdfs = []
    
    for filename in tqdm(pdf_files, desc="å¤„ç† PDF"):
        filepath = os.path.join(PDF_DIR, filename)
        doi = file_to_doi.get(filename, "unknown_doi")
        
        try:
            pdf_chunks = process_single_pdf(filepath, filename, doi)
            
            if pdf_chunks:
                for item in pdf_chunks:
                    batch_documents.append(item["text"])
                    batch_metadatas.append(item["metadata"])
                    batch_ids.append(item["id"])
                
                total_chunks += len(pdf_chunks)
                total_pdfs += 1
            else:
                failed_pdfs.append(filename)
                
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥ {filename}: {e}")
            failed_pdfs.append(filename)
            continue
        
        # æ‰¹æ¬¡å¤„ç†ï¼ˆè·å–å‘é‡å¹¶å†™å…¥æ•°æ®åº“ï¼‰
        if len(batch_documents) >= BATCH_SIZE:
            logger.info(f"   å¤„ç†æ‰¹æ¬¡: {len(batch_documents)} ä¸ªåˆ‡ç‰‡")
            
            # è·å–å‘é‡
            embeddings = get_embeddings(batch_documents)
            
            # å†™å…¥æ•°æ®åº“
            collection.add(
                embeddings=embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas,
                ids=batch_ids
            )
            
            # æ¸…ç©ºç¼“å†²åŒº
            batch_documents = []
            batch_metadatas = []
            batch_ids = []
    
    # å¤„ç†å‰©ä½™æ•°æ®
    if batch_documents:
        logger.info(f"   å¤„ç†æœ€åæ‰¹æ¬¡: {len(batch_documents)} ä¸ªåˆ‡ç‰‡")
        embeddings = get_embeddings(batch_documents)
        collection.add(
            embeddings=embeddings,
            documents=batch_documents,
            metadatas=batch_metadatas,
            ids=batch_ids
        )
    
    # 6. ç»Ÿè®¡ä¿¡æ¯
    final_count = collection.count()
    
    logger.info("=" * 60)
    logger.info("âœ… V2.0 å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ!")
    logger.info(f"   å¤„ç† PDF: {total_pdfs}/{len(pdf_files)}")
    logger.info(f"   ç”Ÿæˆåˆ‡ç‰‡: {total_chunks}")
    logger.info(f"   æ•°æ®åº“æ€»é‡: {final_count}")
    logger.info(f"   å¤±è´¥æ–‡ä»¶: {len(failed_pdfs)}")
    if failed_pdfs:
        logger.info(f"   å¤±è´¥åˆ—è¡¨: {failed_pdfs[:10]}...")
    logger.info("=" * 60)


if __name__ == "__main__":
    text_splitter = None
    main()
